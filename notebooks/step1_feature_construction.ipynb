{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## EMF Signal Processing Pipeline - Feature Construction\n",
    "\n",
    "**Signal Normalization:**\n",
    "- Per-subject z-score normalization: standardizes each signal channel individually per patient using mean and standard deviation\n",
    "- Theoretically shall eliminate sensor placement and session-specific baseline differences while preserving physiological signal patterns\n",
    "\n",
    "**Signal Windowing:**\n",
    "- 60-second overlapping windows with 15-second overlap\n",
    "- Maintains temporal context for frequency analysis while enabling sufficient training samples\n",
    "\n",
    "**Feature Extraction:**\n",
    "- **Time-domain features:** Mean, std, RMS, min, max, range, skewness, kurtosis, MAD, and Hjorth parameters (Activity, Mobility, Complexity)\n",
    "- **Frequency-domain features:** Bandpower analysis across 8 frequency bands including critical 0.01-0.02 Hz range, plus total power\n",
    "- **Wavelet features:** 6-level decomposition energies with frequency band descriptions for multi-resolution analysis\n",
    "- **Entropy measures:** Shannon entropy for signal complexity quantification\n",
    "- **Patient metadata:** Age, weight, height, sex, and calculated BMI from patient demographics\n",
    "\n",
    "**Feature Selection & Dimensionality Reduction:**\n",
    "- **PCA reduction is not applied** in this step to retain all features for comprehensive analysis\n",
    "- **All domain features retained** for comprehensive analysis\n",
    "\n",
    "**Target Construction:**\n",
    "- **Glucose regression:** 10-minute lag-corrected CGM values to account for sensor delay\n",
    "- **Glycemic state prediction:** 15-minute ahead glycemic state classification (hypoglycemic <70 mg/dL, normal 70-180 mg/dL, hyperglycemic >180 mg/dL)\n",
    "- **Hypoglycemia flag:** Binary flag (1) if glucose < 75 mg/dL within next 900 seconds (15 minutes)\n",
    "- **Hyperglycemia flag:** Binary flag (1) if glucose > 180 mg/dL within next 900 seconds (15 minutes)\n",
    "- **Metabolic state tracking:** Fasting, First Insulin, Ensure, Second Insulin phases based on experimental protocol events\n",
    "- Time-based nearest neighbor matching between feature windows and glucose measurements\n",
    "\n",
    "**Output:**\n",
    "- **Combined multi-patient dataset** with comprehensive feature set (all domain features)\n",
    "\n",
    "**Remarks:**\n",
    "- **Class Imbalance** - All the normoglycemic patients are marked as Normal (Negative) having a vast majority of the data, while the hypoglycemic and hyperglycemic patients are marked as T1DM Clamp (Positive) having a very small minority of the data. This will require special attention during model training.\n",
    "- **Dual Channels** - The signals of the dual channels are cross-correlated, so the features need to be chosen carefully to avoid redundancy.\n",
    "- **New Target Labels** - Added hypo/hyper flags provide binary classification targets for glucose anomaly detection within 15-minute windows."
   ],
   "id": "a13e406373e1a2c9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# General imports:\n",
    "\n",
    "# Disable warnings:\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Essential imports\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plotting enhancements\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Key Settings:\n",
    "Visualize_Signal = False # Signal visualization (only if required)\n",
    "\n",
    "# Physical constants and sensors specifications:\n",
    "SENSITIVITY = 50  # mV/nT\n",
    "MAGNETIC_NOISE = 3  # pT/âˆšHz @ 1 Hz\n",
    "MAX_AC_LINEARITY = 250  # nT (+/- 250 nT) - Equivalent to 21.78 V\n",
    "MAX_DC_LINEARITY = 60  # nT (+/- 60 nT)\n",
    "VOLTAGE_LIMIT = 15 # V (+/-15V)\n",
    "CONVERSION_FACTOR = 20  # nT per 1V\n",
    "SAMPLING_FREQUENCY = 5000  # Hz - expected from the experimental data\n",
    "SENSOR_SATURATION = 250  # nT - saturation threshold for the sensor\n",
    "\n",
    "# Subjects and their types\n",
    "Subject = {\"Normal\": \"Normal Subjects\",\"Clamp\": \"T1DM Clamp Subjects\", \"Additional\": \"Additional Subjects\"}\n",
    "\n",
    "# Path and directories\n",
    "base_dir = Path(\"../../../Data\")\n",
    "\n",
    "# Output directory for saving results\n",
    "output_dir = base_dir / \"ProcessedData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Directory for saving processed/downsampled signal files (parquet format)\n",
    "signals_dir = output_dir / \"Signal_Files\"\n",
    "os.makedirs(signals_dir, exist_ok=True)\n",
    "\n",
    "# Labels directory\n",
    "labels_dir = base_dir / \"RawData\"\n",
    "labels_filename = \"FilteredLabels.xlsx\"\n",
    "\n",
    "# Patients data file\n",
    "patients_file = \"patients.json\"\n",
    "\n",
    "# Patient Data\n",
    "with open(labels_dir / patients_file, 'r') as f:\n",
    "    # Load the JSON data\n",
    "    patients_data = json.load(f)\n",
    "\n",
    "# GMT zone correction for the sensor = GMT+2\n",
    "GMT = 2\n",
    "\n",
    "# Channel grouping\n",
    "signal_channels = {\n",
    "    'Head': ['Head_left', 'Head_right'],\n",
    "    'Hand': ['Hand1', 'Hand2'],\n",
    "    'Liver': ['Liver1', 'Liver2'],\n",
    "    'Background': ['Background1', 'Background2']\n",
    "}\n",
    "\n",
    "# Define glucose lag and glycemic prediction parameters\n",
    "GLUCOSE_LAG_MINUTES = 10  # Glucose lag in minutes\n",
    "GLYCEMIC_PREDICTION_MINUTES = 15  # Glycemic prediction in minutes\n",
    "WINDOW_SIZE_MINUTES = 1  # Window size in minutes\n",
    "WINDOW_OVERLAP_MINUTES = 0.25  # Overlap size in minutes\n"
   ],
   "id": "bbf39fb46470487d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the CGM/stick Glucose target values from the file, add metabolic states and detect events",
   "id": "b71cb50361711c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_metabolic_state_column(df_labels):\n",
    "    \"\"\"\n",
    "    Add a 'state' column to df_labels based on metabolic event timing.\n",
    "\n",
    "    States:\n",
    "    1. \"Fasting\" - From start until first insulin injection\n",
    "    2. \"First Insulin\" - From first insulin injection until ensure event\n",
    "    3. \"Ensure\" - From ensure event until second insulin injection\n",
    "    4. \"Second Insulin\" - From second insulin injection until end\n",
    "    \"\"\"\n",
    "    # Check if Events column exists\n",
    "    if \"Events\" not in df_labels.columns:\n",
    "        print(\"Warning: No Events column found - all entries will be labeled as 'Fasting'\")\n",
    "        return df_labels.with_columns(pl.lit(\"Fasting\").alias(\"state\"))\n",
    "\n",
    "    # Sort by time to ensure chronological order\n",
    "    df_sorted = df_labels.sort(\"time\")\n",
    "\n",
    "    # Extract insulin and ensure events\n",
    "    insulin_events = df_sorted.filter(pl.col('Events') == 'Insulin Injection')\n",
    "    ensure_events = df_sorted.filter(pl.col('Events') == 'Ensure')\n",
    "\n",
    "    # Get transition timestamps\n",
    "    first_insulin_time = insulin_events['time'].item(0) if len(insulin_events) > 0 else None\n",
    "    first_ensure_time = ensure_events['time'].item(0) if len(ensure_events) > 0 else None\n",
    "\n",
    "    # Find second insulin (first one after ensure)\n",
    "    second_insulin_time = None\n",
    "    if first_ensure_time is not None and len(insulin_events) > 1:\n",
    "        post_ensure_insulin = insulin_events.filter(pl.col('time') > first_ensure_time)\n",
    "        if len(post_ensure_insulin) > 0:\n",
    "            second_insulin_time = post_ensure_insulin['time'].item(0)\n",
    "\n",
    "    # Determine state for each timestamp\n",
    "    def get_state(timestamp):\n",
    "        if first_insulin_time is None or timestamp < first_insulin_time:\n",
    "            return \"Fasting\"\n",
    "        elif first_ensure_time is None or timestamp < first_ensure_time:\n",
    "            return \"First Insulin\"\n",
    "        elif second_insulin_time is None or timestamp < second_insulin_time:\n",
    "            return \"Ensure\"\n",
    "        else:\n",
    "            return \"Second Insulin\"\n",
    "\n",
    "    # Add state column using time from df_labels\n",
    "    states = [get_state(t) for t in df_labels['time']]\n",
    "    result_df = df_labels.with_columns(pl.Series(name=\"state\", values=states))\n",
    "\n",
    "    return result_df"
   ],
   "id": "38b56d293f080a29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def detect_all_transitions(df_labels):\n",
    "    \"\"\"\n",
    "    Detect both metabolic state transitions and glycemic state transitions.\n",
    "\n",
    "    Glycemic states:\n",
    "    - Hypoglycemia: < 70 mg/dL\n",
    "    - Normal: 70-180 mg/dL\n",
    "    - Hyperglycemia: > 180 mg/dL\n",
    "    \"\"\"\n",
    "    # Initialize transitions dictionary\n",
    "    transitions = {}\n",
    "\n",
    "    # Ensure data is sorted by time\n",
    "    df_sorted = df_labels.sort(\"time\")\n",
    "\n",
    "    # Extract metabolic state transitions (already computed)\n",
    "    metabolic_states = [\"Fasting\", \"First Insulin\", \"Ensure\", \"Second Insulin\"]\n",
    "\n",
    "    # Find the first occurrence of each state\n",
    "    for state in metabolic_states:\n",
    "        state_rows = df_sorted.filter(pl.col('state') == state)\n",
    "        if len(state_rows) > 0:\n",
    "            transitions[f\"Metabolic: {state}\"] = state_rows['time'].item(0)\n",
    "\n",
    "    # Filter out rows with null glucose values and add glycemic state\n",
    "    df_glycemic = df_sorted.filter(\n",
    "        pl.col('Glucose').is_not_null() &\n",
    "        ~pl.col('Glucose').is_nan()\n",
    "    ).with_columns([\n",
    "        pl.when(pl.col('Glucose') < 70)\n",
    "          .then(pl.lit(\"Hypoglycemia\"))\n",
    "          .when(pl.col('Glucose') <= 180)\n",
    "          .then(pl.lit(\"Normal\"))\n",
    "          .otherwise(pl.lit(\"Hyperglycemia\"))\n",
    "          .alias(\"glycemic_state\")\n",
    "    ])\n",
    "\n",
    "    # Record the initial glycemic state\n",
    "    if len(df_glycemic) > 0:\n",
    "        initial_state = df_glycemic['glycemic_state'].item(0)\n",
    "        initial_time = df_glycemic['time'].item(0)\n",
    "        transitions[f\"Initial Glycemic State: {initial_state}\"] = initial_time\n",
    "\n",
    "    # Detect transitions using a window-based approach\n",
    "    previous_state = None\n",
    "    for row in df_glycemic.iter_rows(named=True):\n",
    "        current_state = row['glycemic_state']\n",
    "        current_time = row['time']\n",
    "\n",
    "        if previous_state is not None and current_state != previous_state:\n",
    "            transition_name = f\"Glycemic: {previous_state} to {current_state}\"\n",
    "            transitions[transition_name] = current_time\n",
    "\n",
    "        previous_state = current_state\n",
    "\n",
    "    return transitions"
   ],
   "id": "60a304210dac27d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to load the downsampled signals",
   "id": "f65e24a16acc05fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the downsampled data from parquet file\n",
    "def load_downsampled_data(patient_name, signals_dir):\n",
    "    \"\"\"\n",
    "    Load downsampled data from parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - patient_name: name of the patient\n",
    "    - signals_dir: directory containing the parquet signal files\n",
    "\n",
    "    Returns:\n",
    "    - df_loaded: polars DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    # Create filename based on patient name\n",
    "    patient_safe_name = patient_name.replace(\" \", \"_\").replace(\"#\", \"\")\n",
    "    output_filename = f\"{patient_safe_name}_downsampled_25hz.parquet\"\n",
    "    output_path = signals_dir / output_filename\n",
    "\n",
    "    try:\n",
    "        if output_path.exists():\n",
    "            df_loaded = pl.read_parquet(output_path)\n",
    "            print(f\"Successfully loaded data from {output_filename}\")\n",
    "            print(f\"Loaded data shape: {df_loaded.shape}\")\n",
    "            return df_loaded\n",
    "        else:\n",
    "            print(f\"File not found: {output_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file: {e}\")\n",
    "        return None"
   ],
   "id": "b4b90baa5c193881",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to apply z-score normalization",
   "id": "f4545784ffbcfc55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def apply_zscore_normalization(df_signal, patient_name, global_scalers=None):\n",
    "    \"\"\"\n",
    "    Apply z-score (StandardScaler) normalization to all signal channels per channel,\n",
    "    followed by cross-session joint normalization.\n",
    "\n",
    "    Parameters:\n",
    "    - df_signal: polars DataFrame containing the signal data\n",
    "    - patient_name: name of the patient\n",
    "    - global_scalers: dictionary of pre-fitted StandardScaler objects for cross-session normalization\n",
    "\n",
    "    Returns:\n",
    "    - df_normalized: polars DataFrame with normalized signals\n",
    "    - channel_scalers: dictionary of fitted StandardScaler objects for this patient\n",
    "    \"\"\"\n",
    "    if df_signal is None:\n",
    "        print(\"No signal data provided for normalization\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Applying z-score normalization for {patient_name}\")\n",
    "\n",
    "    # Find the time column\n",
    "    time_col = None\n",
    "    for col in df_signal.columns:\n",
    "        if 'time' in col.lower():\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"Error: No time column found in the data\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert to pandas temporarily for easier processing\n",
    "    df_pandas = df_signal.to_pandas()\n",
    "\n",
    "    # Get all signal columns (exclude time column)\n",
    "    signal_columns = [col for col in df_pandas.columns if col != time_col]\n",
    "\n",
    "    print(f\"Processing {len(signal_columns)} signal channels\")\n",
    "\n",
    "    # Step 1: Per-channel z-score normalization\n",
    "    df_normalized = df_pandas.copy()\n",
    "    channel_scalers = {}\n",
    "\n",
    "    for col in signal_columns:\n",
    "        if df_pandas[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "            # Remove NaN values for fitting\n",
    "            valid_data = df_pandas[col].dropna()\n",
    "\n",
    "            if len(valid_data) == 0:\n",
    "                print(f\"Warning: No valid data for channel {col}\")\n",
    "                continue\n",
    "\n",
    "            # Fit StandardScaler on this channel\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            # Reshape for sklearn (needs 2D array)\n",
    "            data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "            scaler.fit(data_reshaped)\n",
    "\n",
    "            # Transform the entire column (including NaN values)\n",
    "            # Handle NaN values by only transforming non-NaN entries\n",
    "            transformed_data = df_pandas[col].copy()\n",
    "            non_nan_mask = ~df_pandas[col].isna()\n",
    "\n",
    "            if non_nan_mask.sum() > 0:\n",
    "                transformed_data[non_nan_mask] = scaler.transform(\n",
    "                    df_pandas[col][non_nan_mask].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "\n",
    "            df_normalized[col] = transformed_data\n",
    "            channel_scalers[col] = scaler\n",
    "\n",
    "            # Handle both scalar and array cases for mean_ and scale_\n",
    "            mean_val = scaler.mean_[0] if hasattr(scaler.mean_, '__len__') else scaler.mean_\n",
    "            scale_val = scaler.scale_[0] if hasattr(scaler.scale_, '__len__') else scaler.scale_\n",
    "            print(f\"Channel {col}: mean={mean_val:.4f}, std={scale_val:.4f}\")\n",
    "\n",
    "    # Step 2: Cross-session joint normalization (if global scalers provided)\n",
    "    if global_scalers is not None:\n",
    "        print(f\"Applying cross-session joint normalization\")\n",
    "\n",
    "        for col in signal_columns:\n",
    "            if col in global_scalers and col in df_normalized.columns:\n",
    "                # Apply global scaler to already per-channel normalized data\n",
    "                non_nan_mask = ~df_normalized[col].isna()\n",
    "\n",
    "                if non_nan_mask.sum() > 0:\n",
    "                    df_normalized.loc[non_nan_mask, col] = global_scalers[col].transform(\n",
    "                        df_normalized.loc[non_nan_mask, col].values.reshape(-1, 1)\n",
    "                    ).flatten()\n",
    "\n",
    "                print(f\"Applied cross-session normalization to {col}\")\n",
    "\n",
    "    # Convert back to polars\n",
    "    df_normalized = pl.from_pandas(df_normalized)\n",
    "\n",
    "    print(f\"Z-score normalization completed for {len(signal_columns)} channels\")\n",
    "    return df_normalized, channel_scalers\n",
    "\n",
    "def fit_global_scalers(all_patient_data, patients_data, signals_dir):\n",
    "    \"\"\"\n",
    "    Fit global StandardScaler objects across all patients for cross-session normalization.\n",
    "\n",
    "    Parameters:\n",
    "    - all_patient_data: dict of patient DataFrames (per-channel normalized)\n",
    "    - patients_data: dictionary containing patient information\n",
    "    - signals_dir: directory containing signal files\n",
    "\n",
    "    Returns:\n",
    "    - global_scalers: dictionary of fitted StandardScaler objects for cross-session normalization\n",
    "    \"\"\"\n",
    "    print(\"Fitting global scalers for cross-session normalization...\")\n",
    "\n",
    "    # Collect all channel data across patients\n",
    "    all_channel_data = {}\n",
    "\n",
    "    for patient_name in patients_data.keys():\n",
    "        print(f\"Loading data for global scaler fitting: {patient_name}\")\n",
    "\n",
    "        # Load signal data\n",
    "        df_loaded = load_downsampled_data(patient_name, signals_dir)\n",
    "        if df_loaded is None:\n",
    "            continue\n",
    "\n",
    "        # Apply per-channel normalization first\n",
    "        df_normalized, _ = apply_zscore_normalization(df_loaded, patient_name, global_scalers=None)\n",
    "        if df_normalized is None:\n",
    "            continue\n",
    "\n",
    "        # Convert to pandas for easier processing\n",
    "        df_pandas = df_normalized.to_pandas()\n",
    "\n",
    "        # Find time column\n",
    "        time_col = None\n",
    "        for col in df_pandas.columns:\n",
    "            if 'time' in col.lower():\n",
    "                time_col = col\n",
    "                break\n",
    "\n",
    "        if time_col is None:\n",
    "            continue\n",
    "\n",
    "        # Collect data for each channel\n",
    "        signal_columns = [col for col in df_pandas.columns if col != time_col]\n",
    "\n",
    "        for col in signal_columns:\n",
    "            if col not in all_channel_data:\n",
    "                all_channel_data[col] = []\n",
    "\n",
    "            # Add non-NaN values to the collection\n",
    "            valid_data = df_pandas[col].dropna()\n",
    "            if len(valid_data) > 0:\n",
    "                all_channel_data[col].extend(valid_data.values)\n",
    "\n",
    "    # Fit global scalers\n",
    "    global_scalers = {}\n",
    "\n",
    "    for col, data in all_channel_data.items():\n",
    "        if len(data) > 0:\n",
    "            # Fit StandardScaler on combined data from all patients\n",
    "            scaler = StandardScaler()\n",
    "            data_array = np.array(data).reshape(-1, 1)\n",
    "            scaler.fit(data_array)\n",
    "            global_scalers[col] = scaler\n",
    "\n",
    "            print(f\"Global scaler for {col}: mean={scaler.mean_:.4f}, std={scaler.scale_:.4f}\")\n",
    "\n",
    "    print(f\"Fitted global scalers for {len(global_scalers)} channels\")\n",
    "    return global_scalers"
   ],
   "id": "cf1a3428d8cc2593",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to plot the time series data",
   "id": "c8177014786ba72a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_timeseries_by_channel(df, signal_channels, patient_name, time_window_hours=None, y_label='Signal (nT)'):\n",
    "    \"\"\"\n",
    "    Plot time series for each channel group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with time series data\n",
    "    - signal_channels: dictionary of channel groups\n",
    "    - patient_name: name of the patient for plot title\n",
    "    - time_window_hours: optional time window in hours to limit the plot\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas for easier plotting\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Find the time column (case-insensitive)\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"No time column found for plotting\")\n",
    "        return\n",
    "\n",
    "    # Limit time window if specified\n",
    "    if time_window_hours is not None:\n",
    "        start_time = df_pandas[time_col].min()\n",
    "        end_time = start_time + pd.Timedelta(hours=time_window_hours)\n",
    "        df_pandas = df_pandas[df_pandas[time_col] <= end_time]\n",
    "        print(f\"Plotting first {time_window_hours} hours of data\")\n",
    "\n",
    "    # Create subplots for each channel group\n",
    "    n_groups = len(signal_channels)\n",
    "    fig, axes = plt.subplots(n_groups, 1, figsize=(15, 4*n_groups))\n",
    "\n",
    "    if n_groups == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Plot each channel group\n",
    "    for i, (group_name, channels) in enumerate(signal_channels.items()):\n",
    "        ax = axes[i]\n",
    "\n",
    "        for channel in channels:\n",
    "            if channel in df_pandas.columns:\n",
    "                ax.plot(df_pandas[time_col], df_pandas[channel], label=channel, alpha=0.8)\n",
    "            else:\n",
    "                print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "        ax.set_title(f'{group_name} Channels - {patient_name}')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Format x-axis for better readability\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        ax.xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_channels_overlay(df, signal_channels, patient_name, time_window_hours=None, y_label='Signal (nT)'):\n",
    "    \"\"\"\n",
    "    Plot all channels overlaid on a single plot.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with time series data\n",
    "    - signal_channels: dictionary of channel groups\n",
    "    - patient_name: name of the patient for plot title\n",
    "    - time_window_hours: optional time window in hours to limit the plot\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas for easier plotting\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Find the time column (case-insensitive)\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"No time column found for plotting\")\n",
    "        return\n",
    "\n",
    "    # Limit time window if specified\n",
    "    if time_window_hours is not None:\n",
    "        start_time = df_pandas[time_col].min()\n",
    "        end_time = start_time + pd.Timedelta(hours=time_window_hours)\n",
    "        df_pandas = df_pandas[df_pandas[time_col] <= end_time]\n",
    "        print(f\"Plotting first {time_window_hours} hours of data\")\n",
    "\n",
    "    # Create single plot with all channels\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Define colors for each group\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "\n",
    "    for i, (group_name, channels) in enumerate(signal_channels.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        for j, channel in enumerate(channels):\n",
    "            if channel in df_pandas.columns:\n",
    "                # Use different line styles for channels within the same group\n",
    "                linestyle = '-' if j == 0 else '--'\n",
    "                plt.plot(df_pandas[time_col], df_pandas[channel],\n",
    "                        label=f'{group_name}: {channel}',\n",
    "                        color=color, linestyle=linestyle, alpha=0.8)\n",
    "            else:\n",
    "                print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "    plt.title(f'All Channels - {patient_name}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Format x-axis for better readability\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "    plt.setp(plt.gca().xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "dca54778de88fea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the functions of Feature Extraction",
   "id": "706d5d7dce1caf3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "import pywt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_hjorth_parameters(signal):\n",
    "    \"\"\"\n",
    "    Calculate Hjorth parameters (Activity, Mobility, Complexity)\n",
    "    \"\"\"\n",
    "    # First derivative\n",
    "    diff1 = np.diff(signal)\n",
    "    # Second derivative\n",
    "    diff2 = np.diff(diff1)\n",
    "\n",
    "    # Variance calculations\n",
    "    var_signal = np.var(signal)\n",
    "    var_diff1 = np.var(diff1)\n",
    "    var_diff2 = np.var(diff2)\n",
    "\n",
    "    # Hjorth parameters\n",
    "    activity = var_signal\n",
    "    mobility = np.sqrt(var_diff1 / var_signal) if var_signal > 0 else 0\n",
    "    complexity = np.sqrt(var_diff2 / var_diff1) / mobility if var_diff1 > 0 and mobility > 0 else 0\n",
    "\n",
    "    return activity, mobility, complexity\n",
    "\n",
    "def calculate_bandpower(signal, fs, freq_range):\n",
    "    \"\"\"\n",
    "    Calculate power in a specific frequency band\n",
    "    \"\"\"\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(len(signal), 1024))\n",
    "\n",
    "    # Find frequency indices\n",
    "    freq_mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])\n",
    "\n",
    "    if np.sum(freq_mask) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate power in band\n",
    "    bandpower = np.trapezoid(psd[freq_mask], freqs[freq_mask])\n",
    "    return bandpower\n",
    "\n",
    "def calculate_entropy_measures(signal):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy only - removed approximate entropy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Shannon entropy\n",
    "        hist, _ = np.histogram(signal, bins=50)\n",
    "        hist = hist[hist > 0]  # Remove zero bins\n",
    "        shannon_entropy = entropy(hist)\n",
    "\n",
    "        return shannon_entropy\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Entropy calculation failed: {e}\")\n",
    "        return 0\n",
    "\n",
    "def calculate_shannon_entropy(signal):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy\n",
    "    \"\"\"\n",
    "    return calculate_entropy_measures(signal)\n",
    "\n",
    "def calculate_bandpower_efficient(signal, fs, freq_ranges):\n",
    "    \"\"\"\n",
    "    Calculate power in multiple frequency bands with better low-frequency resolution\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For better low-frequency resolution, use longer segments\n",
    "        # Aim for frequency resolution of ~0.005 Hz to capture 0.01-0.02 Hz band\n",
    "        target_freq_resolution = 0.005\n",
    "        ideal_nperseg = int(fs / target_freq_resolution)  # ~5000 samples\n",
    "\n",
    "        # Use the full signal length if possible, but cap at reasonable limit\n",
    "        nperseg = min(len(signal), ideal_nperseg, 8192)\n",
    "\n",
    "        # Ensure nperseg is not too small\n",
    "        nperseg = max(nperseg, 512)\n",
    "\n",
    "        freqs, psd = welch(signal, fs=fs, nperseg=nperseg, noverlap=nperseg//2)\n",
    "\n",
    "        # Calculate all band powers in one pass\n",
    "        bandpowers = {}\n",
    "        for band_name, freq_range in freq_ranges.items():\n",
    "            freq_mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])\n",
    "\n",
    "            if np.sum(freq_mask) == 0:\n",
    "                # If no frequencies in range, interpolate or use nearest\n",
    "                print(f\"Warning: No frequencies found for band {band_name} ({freq_range})\")\n",
    "                bandpowers[band_name] = 0\n",
    "            else:\n",
    "                bandpowers[band_name] = np.trapezoid(psd[freq_mask], freqs[freq_mask])\n",
    "\n",
    "        return bandpowers\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Bandpower calculation failed: {e}\")\n",
    "        return {band_name: 0 for band_name in freq_ranges.keys()}\n",
    "\n",
    "def calculate_wavelet_energies(signal, fs=25, wavelet='db4', levels=6):\n",
    "    \"\"\"\n",
    "    Calculate wavelet energies with frequency band descriptions - optimized version\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Limit signal length for faster computation\n",
    "        max_samples = 5000  # Limit to ~3 minutes at 25 Hz\n",
    "        if len(signal) > max_samples:\n",
    "            signal = signal[:max_samples]\n",
    "\n",
    "        # Decompose signal using wavelet\n",
    "        coeffs = pywt.wavedec(signal, wavelet, level=levels)\n",
    "\n",
    "        # Calculate energy in each subband with frequency descriptions\n",
    "        energies = []\n",
    "\n",
    "        # Approximate frequency bands based on sampling rate and decomposition levels\n",
    "        # For fs=25Hz, Nyquist = 12.5Hz\n",
    "        nyquist = fs / 2\n",
    "\n",
    "        for i, coeff in enumerate(coeffs):\n",
    "            energy = np.sum(np.square(coeff))\n",
    "\n",
    "            # Map wavelet levels to approximate frequency bands\n",
    "            if i == 0:  # Approximation coefficients (lowest frequencies)\n",
    "                freq_desc = 'very_low_0_0.1'\n",
    "            elif i == 1:  # Detail level 6\n",
    "                freq_desc = 'low_0.1_0.2'\n",
    "            elif i == 2:  # Detail level 5\n",
    "                freq_desc = 'low_0.2_0.4'\n",
    "            elif i == 3:  # Detail level 4\n",
    "                freq_desc = 'mid_0.4_0.8'\n",
    "            elif i == 4:  # Detail level 3\n",
    "                freq_desc = 'mid_0.8_1.6'\n",
    "            elif i == 5:  # Detail level 2\n",
    "                freq_desc = 'high_1.6_3.1'\n",
    "            elif i == 6:  # Detail level 1\n",
    "                freq_desc = 'high_3.1_6.25'\n",
    "            else:\n",
    "                freq_desc = f'level_{i}'\n",
    "\n",
    "            energies.append((energy, freq_desc))\n",
    "\n",
    "        return energies\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Wavelet calculation failed: {e}\")\n",
    "        return [(0, f'level_{i}') for i in range(levels + 1)]\n",
    "\n",
    "def extract_features_from_window(window_data, patient_info, fs=25):\n",
    "    \"\"\"\n",
    "    Extract comprehensive features from a single window - optimized version with cleaned features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Find time column\n",
    "    time_col = None\n",
    "    for col in window_data.columns:\n",
    "        if 'time' in col.lower():\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    # Get signal columns (exclude time and background channels)\n",
    "    signal_cols = [col for col in window_data.columns\n",
    "                   if col != time_col and 'background' not in col.lower()]\n",
    "\n",
    "    # Add patient metadata\n",
    "    features['age'] = patient_info.get('age', 0)\n",
    "    features['weight'] = patient_info.get('weight', 0)\n",
    "    features['height'] = patient_info.get('height', 0)\n",
    "    features['sex'] = 1 if patient_info.get('sex', '').lower() == 'male' else 0\n",
    "\n",
    "    # Calculate BMI\n",
    "    if features['weight'] > 0 and features['height'] > 0:\n",
    "        features['bmi'] = features['weight'] / ((features['height'] / 100) ** 2)\n",
    "    else:\n",
    "        features['bmi'] = 0\n",
    "\n",
    "    # Define frequency ranges for efficient computation\n",
    "    freq_ranges = {\n",
    "        '0_05_0_1': [0.05, 0.1],\n",
    "        '0_1_0_5': [0.1, 0.5],\n",
    "        '0_5_2': [0.5, 2],\n",
    "        '2_5': [2, 5],\n",
    "        '5_10': [5, 10],\n",
    "        'total': [0.01, fs/2]\n",
    "    }\n",
    "\n",
    "    # Process each channel\n",
    "    for channel in signal_cols:\n",
    "        if channel in window_data.columns:\n",
    "            signal = window_data[channel].values\n",
    "\n",
    "            # Skip if signal is empty or all NaN\n",
    "            if len(signal) == 0 or np.all(np.isnan(signal)):\n",
    "                continue\n",
    "\n",
    "            # Remove NaN values\n",
    "            signal = signal[~np.isnan(signal)]\n",
    "\n",
    "            if len(signal) == 0:\n",
    "                continue\n",
    "\n",
    "            # Downsample if signal is too long (for faster processing)\n",
    "            if len(signal) > 10000:  # More than ~6 minutes at 25 Hz\n",
    "                step = len(signal) // 10000\n",
    "                signal = signal[::step]\n",
    "\n",
    "            try:\n",
    "                # Time-domain statistics (fast) - removed variance as it's duplicate of Hjorth activity\n",
    "                features[f'{channel}_mean'] = np.mean(signal)\n",
    "                features[f'{channel}_std'] = np.std(signal)\n",
    "                features[f'{channel}_rms'] = np.sqrt(np.mean(np.square(signal)))\n",
    "                features[f'{channel}_min'] = np.min(signal)\n",
    "                features[f'{channel}_max'] = np.max(signal)\n",
    "                features[f'{channel}_range'] = np.max(signal) - np.min(signal)\n",
    "                features[f'{channel}_skewness'] = stats.skew(signal)\n",
    "                features[f'{channel}_kurtosis'] = stats.kurtosis(signal)\n",
    "                features[f'{channel}_mad'] = np.mean(np.abs(signal - np.mean(signal)))\n",
    "\n",
    "                # Hjorth parameters (Activity = variance, so we keep this instead of separate variance)\n",
    "                activity, mobility, complexity = calculate_hjorth_parameters(signal)\n",
    "                features[f'{channel}_hjorth_activity'] = activity  # This is the variance\n",
    "                features[f'{channel}_hjorth_mobility'] = mobility\n",
    "                features[f'{channel}_hjorth_complexity'] = complexity\n",
    "\n",
    "                # Frequency-domain features (efficient batch computation)\n",
    "                bandpowers = calculate_bandpower_efficient(signal, fs, freq_ranges)\n",
    "                for band_name, power in bandpowers.items():\n",
    "                    features[f'{channel}_bandpower_{band_name}'] = power\n",
    "\n",
    "                # Wavelet energies with frequency band descriptions\n",
    "                wavelet_energies = calculate_wavelet_energies(signal, fs)\n",
    "                for i, (energy, freq_desc) in enumerate(wavelet_energies):\n",
    "                    features[f'{channel}_wavelet_energy_{freq_desc}'] = energy\n",
    "\n",
    "                # Entropy measures (only Shannon entropy - removed approximate entropy)\n",
    "                shannon_ent = calculate_shannon_entropy(signal)\n",
    "                features[f'{channel}_shannon_entropy'] = shannon_ent\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Feature extraction failed for channel {channel}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return features\n",
    "\n",
    "def create_windows(df_signal, window_size_min=1, overlap_min=0.25, fs=25):\n",
    "    \"\"\"\n",
    "    Create overlapping windows from the signal - optimized version\n",
    "    \"\"\"\n",
    "    if df_signal is None:\n",
    "        return []\n",
    "\n",
    "    # Convert to pandas for easier processing\n",
    "    df_pandas = df_signal.to_pandas()\n",
    "\n",
    "    # Find time column\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if 'time' in col.lower():\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"Error: No time column found\")\n",
    "        return []\n",
    "\n",
    "    # Calculate window parameters\n",
    "    window_size_samples = int(window_size_min * 60 * fs)  # Convert to samples\n",
    "    overlap_samples = int(overlap_min * 60 * fs)\n",
    "    step_samples = window_size_samples - overlap_samples\n",
    "\n",
    "    print(f\"Window size: {window_size_min} min ({window_size_samples} samples)\")\n",
    "    print(f\"Overlap: {overlap_min} min ({overlap_samples} samples)\")\n",
    "    print(f\"Step size: {step_samples} samples\")\n",
    "\n",
    "    windows = []\n",
    "    total_samples = len(df_pandas)\n",
    "\n",
    "\n",
    "    # Create windows\n",
    "    start_idx = 0\n",
    "\n",
    "    while start_idx + window_size_samples <= total_samples:\n",
    "        end_idx = start_idx + window_size_samples\n",
    "\n",
    "        window_data = df_pandas.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "        window_info = {\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'start_time': window_data[time_col].iloc[0],\n",
    "            'end_time': window_data[time_col].iloc[-1],\n",
    "            'data': window_data\n",
    "        }\n",
    "\n",
    "        windows.append(window_info)\n",
    "        start_idx += step_samples\n",
    "\n",
    "    print(f\"Created {len(windows)} windows from {total_samples} samples\")\n",
    "    return windows\n",
    "\n",
    "def construct_features_for_patient(df_normalized, patient_name, patients_data,\n",
    "                                 window_size_min=1, overlap_min=0.25, fs=25):\n",
    "    \"\"\"\n",
    "    Construct features for all windows of a patient\n",
    "\n",
    "    Parameters:\n",
    "    - df_normalized: polars DataFrame with normalized signal data\n",
    "    - patient_name: name of the patient\n",
    "    - patients_data: dictionary with patient information\n",
    "    - window_size_min: window size in minutes\n",
    "    - overlap_min: overlap size in minutes\n",
    "    - fs: sampling frequency\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with features for all windows\n",
    "    \"\"\"\n",
    "    print(f\"\\nConstructing features for patient: {patient_name}\")\n",
    "\n",
    "    # Create windows\n",
    "    windows = create_windows(df_normalized, window_size_min, overlap_min, fs)\n",
    "\n",
    "    if not windows:\n",
    "        print(\"No windows created\")\n",
    "        return None\n",
    "\n",
    "    # Get patient info\n",
    "    patient_info = patients_data[patient_name]\n",
    "\n",
    "    # Extract features from each window\n",
    "    all_features = []\n",
    "\n",
    "    for i, window in enumerate(tqdm(windows, desc=\"Extracting features\")):\n",
    "        window_features = extract_features_from_window(window['data'], patient_info, fs)\n",
    "\n",
    "        # Add window metadata\n",
    "        window_features['patient'] = patient_name\n",
    "        window_features['window_idx'] = i\n",
    "        window_features['start_time'] = window['start_time']\n",
    "        window_features['end_time'] = window['end_time']\n",
    "\n",
    "        all_features.append(window_features)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame(all_features)\n",
    "\n",
    "    print(f\"Extracted {len(features_df)} windows with {len(features_df.columns)} features each\")\n",
    "\n",
    "    return features_df"
   ],
   "id": "66e72768188fe746",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to construct the final dataset with features and targets",
   "id": "43c62c3106476467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def construct_final_dataset(features_df, labels_df, patient_name, glucose_lag_minutes=10, glycemic_prediction_minutes=15):\n",
    "    \"\"\"\n",
    "    Construct the final dataset with features and targets.\n",
    "\n",
    "    Parameters:\n",
    "    - features_df: pandas DataFrame with extracted features\n",
    "    - labels_df: polars DataFrame with labels and events\n",
    "    - patient_name: name of the patient\n",
    "    - glucose_lag_minutes: minutes to lag glucose target (default 10 min)\n",
    "    - glycemic_prediction_minutes: minutes ahead to predict glycemic state (default 15 min)\n",
    "\n",
    "    Returns:\n",
    "    - final_df: pandas DataFrame with features and corresponding targets\n",
    "    \"\"\"\n",
    "    # Convert polars DataFrame to pandas if needed\n",
    "    if hasattr(labels_df, 'to_pandas'):\n",
    "        labels_pandas = labels_df.to_pandas()\n",
    "    else:\n",
    "        labels_pandas = labels_df\n",
    "\n",
    "    # Add patient column to labels if not present\n",
    "    if 'patient' not in labels_pandas.columns:\n",
    "        labels_pandas['patient'] = patient_name\n",
    "\n",
    "    print(f\"Features DataFrame shape: {features_df.shape}\")\n",
    "    print(f\"Labels DataFrame shape: {labels_pandas.shape}\")\n",
    "    print(f\"Features columns: {list(features_df.columns)}\")\n",
    "    print(f\"Labels columns: {list(labels_pandas.columns)}\")\n",
    "\n",
    "    # Sort labels by time for proper lagging\n",
    "    labels_pandas = labels_pandas.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    # Create lagged glucose target to correct for CGM lag\n",
    "    # CGM reading at time T represents the true glucose from 10 minutes ago\n",
    "    # So to get the \"true\" glucose at time T, we need the CGM reading from T+10 minutes\n",
    "    lag_offset = pd.Timedelta(minutes=glucose_lag_minutes)\n",
    "\n",
    "    # Create prediction offset for glycemic state prediction\n",
    "    prediction_offset = pd.Timedelta(minutes=glycemic_prediction_minutes)\n",
    "\n",
    "    # Create target_lagged_glucose by shifting the glucose values backwards to account for lag\n",
    "    if 'Glucose' in labels_pandas.columns:\n",
    "        # For each original time point, find the glucose value from 10 minutes later (lag correction)\n",
    "        target_lagged_glucose = []\n",
    "\n",
    "        # For glycemic state prediction, find glucose values 15 minutes ahead\n",
    "        future_glucose_values = []\n",
    "        future_glycemic_states = []\n",
    "\n",
    "        for idx, row in labels_pandas.iterrows():\n",
    "            current_time = row['time']\n",
    "\n",
    "            # 1. Lag-corrected glucose (10 minutes later)\n",
    "            future_time_lag = current_time + lag_offset\n",
    "            time_diffs_lag = np.abs((labels_pandas['time'] - future_time_lag).dt.total_seconds())\n",
    "            closest_idx_lag = time_diffs_lag.idxmin()\n",
    "\n",
    "            if time_diffs_lag[closest_idx_lag] <= 15 * 60:  # 15 minutes tolerance\n",
    "                target_lagged_glucose.append(labels_pandas.loc[closest_idx_lag, 'Glucose'])\n",
    "            else:\n",
    "                target_lagged_glucose.append(row['Glucose'])\n",
    "\n",
    "            # 2. Future glucose for glycemic state prediction (15 minutes ahead)\n",
    "            future_time_pred = current_time + prediction_offset\n",
    "            time_diffs_pred = np.abs((labels_pandas['time'] - future_time_pred).dt.total_seconds())\n",
    "            closest_idx_pred = time_diffs_pred.idxmin()\n",
    "\n",
    "            if time_diffs_pred[closest_idx_pred] <= 15 * 60:  # 15 minutes tolerance\n",
    "                future_glucose = labels_pandas.loc[closest_idx_pred, 'Glucose']\n",
    "                future_glucose_values.append(future_glucose)\n",
    "\n",
    "                # Classify future glucose into glycemic states\n",
    "                if pd.isna(future_glucose):\n",
    "                    future_glycemic_states.append(None)\n",
    "                elif future_glucose < 70:\n",
    "                    future_glycemic_states.append('Hypoglycemia')\n",
    "                elif future_glucose > 180:\n",
    "                    future_glycemic_states.append('Hyperglycemia')\n",
    "                else:\n",
    "                    future_glycemic_states.append('Normal')\n",
    "            else:\n",
    "                future_glucose_values.append(None)\n",
    "                future_glycemic_states.append(None)\n",
    "\n",
    "    # Create a time-based merge using nearest timestamp matching\n",
    "    final_features = []\n",
    "\n",
    "    for idx, row in features_df.iterrows():\n",
    "        window_start = row['start_time']\n",
    "        window_end = row['end_time']\n",
    "\n",
    "        # Find labels that fall within this window\n",
    "        window_labels = labels_pandas[\n",
    "            (labels_pandas['time'] >= window_start) &\n",
    "            (labels_pandas['time'] <= window_end)\n",
    "        ]\n",
    "\n",
    "        # If no labels in window, find the closest label before window end\n",
    "        if len(window_labels) == 0:\n",
    "            before_window = labels_pandas[labels_pandas['time'] <= window_end]\n",
    "            if len(before_window) > 0:\n",
    "                # Get the most recent label before window end\n",
    "                closest_label = before_window.loc[before_window['time'].idxmax()]\n",
    "                target_glucose = closest_label['Glucose'] if 'Glucose' in closest_label else None\n",
    "                metabolic_state = closest_label['state'] if 'state' in closest_label else None\n",
    "                target_lagged_glucose = closest_label['target_lagged_glucose'] if 'target_lagged_glucose' in closest_label else None\n",
    "                future_glucose = closest_label[f'future_glucose_{glycemic_prediction_minutes}min'] if f'future_glucose_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "                future_glycemic_state = closest_label[f'future_glycemic_state_{glycemic_prediction_minutes}min'] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "            else:\n",
    "                target_glucose = None\n",
    "                metabolic_state = None\n",
    "                target_lagged_glucose = None\n",
    "                future_glucose = None\n",
    "                future_glycemic_state = None\n",
    "        else:\n",
    "            # Use the first label in the window\n",
    "            target_glucose = window_labels['Glucose'].iloc[0] if 'Glucose' in window_labels.columns else None\n",
    "            metabolic_state = window_labels['state'].iloc[0] if 'state' in window_labels.columns else None\n",
    "            target_lagged_glucose = window_labels['target_lagged_glucose'].iloc[0] if 'target_lagged_glucose' in window_labels.columns else None\n",
    "            future_glucose = window_labels[f'future_glucose_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glucose_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "            future_glycemic_state = window_labels[f'future_glycemic_state_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "\n",
    "        # Create feature row with targets\n",
    "        feature_row = row.to_dict()\n",
    "        feature_row['CGM'] = target_glucose\n",
    "        feature_row[f'lagged_CGM_{glucose_lag_minutes}min'] = target_lagged_glucose\n",
    "        feature_row['metabolic_state'] = metabolic_state\n",
    "        feature_row[f'future_CGM_{glycemic_prediction_minutes}min'] = future_glucose\n",
    "        feature_row[f'glycemic_state_{glycemic_prediction_minutes}min'] = future_glycemic_state\n",
    "\n",
    "        final_features.append(feature_row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_df = pd.DataFrame(final_features)\n",
    "\n",
    "    # Reorder columns to place the glucose columns together\n",
    "    glucose_cols = ['CGM', f'lagged_CGM_{glucose_lag_minutes}min', f'future_CGM_{glycemic_prediction_minutes}min']\n",
    "    target_cols = ['metabolic_state', f'glycemic_state_{glycemic_prediction_minutes}min']\n",
    "    other_cols = [col for col in final_df.columns if col not in glucose_cols + target_cols]\n",
    "\n",
    "    # Place glucose columns after the basic window info but before other features\n",
    "    window_info_cols = ['patient', 'window_idx', 'start_time', 'end_time']\n",
    "    remaining_cols = [col for col in other_cols if col not in window_info_cols]\n",
    "\n",
    "    # Reorder: window info, glucose columns, target columns, then other features\n",
    "    new_column_order = window_info_cols + glucose_cols + target_cols + remaining_cols\n",
    "    final_df = final_df[new_column_order]\n",
    "\n",
    "    print(f\"Final dataset shape: {final_df.shape}\")\n",
    "    print(f\"CGM values available: {final_df['CGM'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Lagged CGM values available: {final_df[f'lagged_CGM_{glucose_lag_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future CGM values available: {final_df[f'future_CGM_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future glycemic state values available: {final_df[f'glycemic_state_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def construct_final_dataset_with_flags(features_df, labels_df, patient_name, glucose_lag_minutes=10, glycemic_prediction_minutes=15):\n",
    "    \"\"\"\n",
    "    Construct the final dataset with features and targets including hypo/hyper flags.\n",
    "\n",
    "    Parameters:\n",
    "    - features_df: pandas DataFrame with extracted features\n",
    "    - labels_df: polars DataFrame with labels and events\n",
    "    - patient_name: name of the patient\n",
    "    - glucose_lag_minutes: minutes to lag glucose target (default 10 min)\n",
    "    - glycemic_prediction_minutes: minutes ahead to predict glycemic state (default 15 min)\n",
    "\n",
    "    Returns:\n",
    "    - final_df: pandas DataFrame with features and corresponding targets including flags\n",
    "    \"\"\"\n",
    "    # Convert polars DataFrame to pandas if needed\n",
    "    if hasattr(labels_df, 'to_pandas'):\n",
    "        labels_pandas = labels_df.to_pandas()\n",
    "    else:\n",
    "        labels_pandas = labels_df\n",
    "\n",
    "    # Add patient column to labels if not present\n",
    "    if 'patient' not in labels_pandas.columns:\n",
    "        labels_pandas['patient'] = patient_name\n",
    "\n",
    "    print(f\"Features DataFrame shape: {features_df.shape}\")\n",
    "    print(f\"Labels DataFrame shape: {labels_pandas.shape}\")\n",
    "\n",
    "    # Sort labels by time for proper lagging\n",
    "    labels_pandas = labels_pandas.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    # Create lagged glucose target to correct for CGM lag\n",
    "    lag_offset = pd.Timedelta(minutes=glucose_lag_minutes)\n",
    "    prediction_offset = pd.Timedelta(minutes=glycemic_prediction_minutes)\n",
    "\n",
    "    # Create target_lagged_glucose by shifting the glucose values backwards to account for lag\n",
    "    if 'Glucose' in labels_pandas.columns:\n",
    "        # For each original time point, find the glucose value from 10 minutes later (lag correction)\n",
    "        target_lagged_glucose = []\n",
    "        future_glucose_values = []\n",
    "        future_glycemic_states = []\n",
    "        hypo_flags = []\n",
    "        hyper_flags = []\n",
    "\n",
    "        for idx, row in labels_pandas.iterrows():\n",
    "            current_time = row['time']\n",
    "\n",
    "            # 1. Lag-corrected glucose (10 minutes later)\n",
    "            future_time_lag = current_time + lag_offset\n",
    "            time_diffs_lag = np.abs((labels_pandas['time'] - future_time_lag).dt.total_seconds())\n",
    "            closest_idx_lag = time_diffs_lag.idxmin()\n",
    "\n",
    "            if time_diffs_lag[closest_idx_lag] <= 15 * 60:  # 15 minutes tolerance\n",
    "                target_lagged_glucose.append(labels_pandas.loc[closest_idx_lag, 'Glucose'])\n",
    "            else:\n",
    "                target_lagged_glucose.append(row['Glucose'])\n",
    "\n",
    "            # 2. Future glucose for glycemic state prediction (15 minutes ahead)\n",
    "            future_time_pred = current_time + prediction_offset\n",
    "            time_diffs_pred = np.abs((labels_pandas['time'] - future_time_pred).dt.total_seconds())\n",
    "            closest_idx_pred = time_diffs_pred.idxmin()\n",
    "\n",
    "            if time_diffs_pred[closest_idx_pred] <= 15 * 60:  # 15 minutes tolerance\n",
    "                future_glucose = labels_pandas.loc[closest_idx_pred, 'Glucose']\n",
    "                future_glucose_values.append(future_glucose)\n",
    "\n",
    "                # Classify future glucose into glycemic states\n",
    "                if pd.isna(future_glucose):\n",
    "                    future_glycemic_states.append(None)\n",
    "                elif future_glucose < 70:\n",
    "                    future_glycemic_states.append('Hypoglycemia')\n",
    "                elif future_glucose > 180:\n",
    "                    future_glycemic_states.append('Hyperglycemia')\n",
    "                else:\n",
    "                    future_glycemic_states.append('Normal')\n",
    "            else:\n",
    "                future_glucose_values.append(None)\n",
    "                future_glycemic_states.append(None)\n",
    "\n",
    "            # 3. Hypo/Hyper flags: check for glucose < 75 or > 180 within next 900 seconds (15 minutes)\n",
    "            future_time_flag = current_time + pd.Timedelta(seconds=900)  # 900 seconds = 15 minutes\n",
    "\n",
    "            # Find all glucose measurements within the next 900 seconds\n",
    "            future_window = labels_pandas[\n",
    "                (labels_pandas['time'] > current_time) &\n",
    "                (labels_pandas['time'] <= future_time_flag)\n",
    "            ]\n",
    "\n",
    "            hypo_flag = 0\n",
    "            hyper_flag = 0\n",
    "\n",
    "            if len(future_window) > 0:\n",
    "                valid_glucose = future_window['Glucose'].dropna()\n",
    "                if len(valid_glucose) > 0:\n",
    "                    # Check for hypoglycemia (< 75 mg/dL)\n",
    "                    if (valid_glucose < 75).any():\n",
    "                        hypo_flag = 1\n",
    "                    # Check for hyperglycemia (> 180 mg/dL)\n",
    "                    if (valid_glucose > 180).any():\n",
    "                        hyper_flag = 1\n",
    "\n",
    "            hypo_flags.append(hypo_flag)\n",
    "            hyper_flags.append(hyper_flag)\n",
    "\n",
    "        labels_pandas['target_lagged_glucose'] = target_lagged_glucose\n",
    "        labels_pandas[f'future_glucose_{glycemic_prediction_minutes}min'] = future_glucose_values\n",
    "        labels_pandas[f'future_glycemic_state_{glycemic_prediction_minutes}min'] = future_glycemic_states\n",
    "        labels_pandas['hypo_flag'] = hypo_flags\n",
    "        labels_pandas['hyper_flag'] = hyper_flags\n",
    "    else:\n",
    "        labels_pandas['target_lagged_glucose'] = None\n",
    "        labels_pandas[f'future_glucose_{glycemic_prediction_minutes}min'] = None\n",
    "        labels_pandas[f'future_glycemic_state_{glycemic_prediction_minutes}min'] = None\n",
    "        labels_pandas['hypo_flag'] = 0\n",
    "        labels_pandas['hyper_flag'] = 0\n",
    "\n",
    "    print(f\"Applied {glucose_lag_minutes}-minute lag correction to glucose targets\")\n",
    "    print(f\"Created {glycemic_prediction_minutes}-minute ahead glycemic state predictions\")\n",
    "    print(f\"Created hypo/hyper flags for glucose events within 900 seconds\")\n",
    "\n",
    "    # Create a time-based merge using nearest timestamp matching\n",
    "    final_features = []\n",
    "\n",
    "    for idx, row in features_df.iterrows():\n",
    "        window_start = row['start_time']\n",
    "        window_end = row['end_time']\n",
    "\n",
    "        # Find labels that fall within this window\n",
    "        window_labels = labels_pandas[\n",
    "            (labels_pandas['time'] >= window_start) &\n",
    "            (labels_pandas['time'] <= window_end)\n",
    "        ]\n",
    "\n",
    "        # If no labels in window, find the closest label before window end\n",
    "        if len(window_labels) == 0:\n",
    "            before_window = labels_pandas[labels_pandas['time'] <= window_end]\n",
    "            if len(before_window) > 0:\n",
    "                # Get the most recent label before window end\n",
    "                closest_label = before_window.loc[before_window['time'].idxmax()]\n",
    "                target_glucose = closest_label['Glucose'] if 'Glucose' in closest_label else None\n",
    "                metabolic_state = closest_label['state'] if 'state' in closest_label else None\n",
    "                target_lagged_glucose = closest_label['target_lagged_glucose'] if 'target_lagged_glucose' in closest_label else None\n",
    "                future_glucose = closest_label[f'future_glucose_{glycemic_prediction_minutes}min'] if f'future_glucose_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "                future_glycemic_state = closest_label[f'future_glycemic_state_{glycemic_prediction_minutes}min'] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "                hypo_flag = closest_label['hypo_flag'] if 'hypo_flag' in closest_label else 0\n",
    "                hyper_flag = closest_label['hyper_flag'] if 'hyper_flag' in closest_label else 0\n",
    "            else:\n",
    "                target_glucose = None\n",
    "                metabolic_state = None\n",
    "                target_lagged_glucose = None\n",
    "                future_glucose = None\n",
    "                future_glycemic_state = None\n",
    "                hypo_flag = 0\n",
    "                hyper_flag = 0\n",
    "        else:\n",
    "            # Use the first label in the window\n",
    "            target_glucose = window_labels['Glucose'].iloc[0] if 'Glucose' in window_labels.columns else None\n",
    "            metabolic_state = window_labels['state'].iloc[0] if 'state' in window_labels.columns else None\n",
    "            target_lagged_glucose = window_labels['target_lagged_glucose'].iloc[0] if 'target_lagged_glucose' in window_labels.columns else None\n",
    "            future_glucose = window_labels[f'future_glucose_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glucose_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "            future_glycemic_state = window_labels[f'future_glycemic_state_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "            hypo_flag = window_labels['hypo_flag'].iloc[0] if 'hypo_flag' in window_labels.columns else 0\n",
    "            hyper_flag = window_labels['hyper_flag'].iloc[0] if 'hyper_flag' in window_labels.columns else 0\n",
    "\n",
    "        # Create feature row with targets\n",
    "        feature_row = row.to_dict()\n",
    "        feature_row['CGM'] = target_glucose\n",
    "        feature_row[f'lagged_CGM_{glucose_lag_minutes}min'] = target_lagged_glucose\n",
    "        feature_row['metabolic_state'] = metabolic_state\n",
    "        feature_row[f'future_CGM_{glycemic_prediction_minutes}min'] = future_glucose\n",
    "        feature_row[f'glycemic_state_{glycemic_prediction_minutes}min'] = future_glycemic_state\n",
    "        feature_row['hypo_flag'] = hypo_flag\n",
    "        feature_row['hyper_flag'] = hyper_flag\n",
    "\n",
    "        final_features.append(feature_row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_df = pd.DataFrame(final_features)\n",
    "\n",
    "    # Reorder columns to place the glucose columns together\n",
    "    glucose_cols = ['CGM', f'lagged_CGM_{glucose_lag_minutes}min', f'future_CGM_{glycemic_prediction_minutes}min']\n",
    "    target_cols = ['metabolic_state', f'glycemic_state_{glycemic_prediction_minutes}min', 'hypo_flag', 'hyper_flag']\n",
    "    other_cols = [col for col in final_df.columns if col not in glucose_cols + target_cols]\n",
    "\n",
    "    # Place glucose columns after the basic window info but before other features\n",
    "    window_info_cols = ['patient', 'window_idx', 'start_time', 'end_time']\n",
    "    remaining_cols = [col for col in other_cols if col not in window_info_cols]\n",
    "\n",
    "    # Reorder: window info, glucose columns, target columns, then other features\n",
    "    new_column_order = window_info_cols + glucose_cols + target_cols + remaining_cols\n",
    "    final_df = final_df[new_column_order]\n",
    "\n",
    "    print(f\"Final dataset shape: {final_df.shape}\")\n",
    "    print(f\"CGM values available: {final_df['CGM'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Lagged CGM values available: {final_df[f'lagged_CGM_{glucose_lag_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future CGM values available: {final_df[f'future_CGM_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future glycemic state values available: {final_df[f'glycemic_state_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Hypo flags: {final_df['hypo_flag'].sum()}/{len(final_df)} ({final_df['hypo_flag'].sum()/len(final_df)*100:.1f}%)\")\n",
    "    print(f\"Hyper flags: {final_df['hyper_flag'].sum()}/{len(final_df)} ({final_df['hyper_flag'].sum()/len(final_df)*100:.1f}%)\")\n",
    "\n",
    "    return final_df"
   ],
   "id": "5940c13e929ec292",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process all patients to generate combined features dataframe",
   "id": "68e8b89a633b9e16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display patients data to understand the structure\n",
    "print(\"Available patients in patients_data:\")\n",
    "for patient_name in patients_data.keys():\n",
    "    print(f\"- {patient_name}\")"
   ],
   "id": "97f83e09ce92cbc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to process all patients",
   "id": "6cfcce6dc0ce8d27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process all patients to generate combined features dataframe\n",
    "def process_all_patients(patients_data, signals_dir, labels_dir, labels_filename, output_dir):\n",
    "    \"\"\"\n",
    "    Process all Normal and T1D patients to generate combined features dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - patients_data: dictionary containing patient information\n",
    "    - signals_dir: directory containing signal files\n",
    "    - labels_dir: directory containing label files\n",
    "    - labels_filename: name of the labels Excel file\n",
    "    - output_dir: directory to save results\n",
    "\n",
    "    Returns:\n",
    "    - combined_df: pandas DataFrame with features from all patients\n",
    "    \"\"\"\n",
    "    all_final_datasets = []\n",
    "    processing_summary = []\n",
    "\n",
    "    # Define glucose lag and glycemic prediction parameters\n",
    "    glucose_lag_minutes = GLUCOSE_LAG_MINUTES\n",
    "    glycemic_prediction_minutes = GLYCEMIC_PREDICTION_MINUTES\n",
    "    window_size = WINDOW_SIZE_MINUTES\n",
    "    window_overlap = WINDOW_OVERLAP_MINUTES\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROCESSING ALL PATIENTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for patient_name in patients_data.keys():\n",
    "        print(f\"\\n{'='*20} Processing: {patient_name} {'='*20}\")\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load downsampled signal data\n",
    "            print(f\"Step 1: Loading downsampled data for {patient_name}...\")\n",
    "            df_loaded = load_downsampled_data(patient_name, signals_dir)\n",
    "\n",
    "            if df_loaded is None:\n",
    "                print(f\"No signal data found for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - No signal data',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Step 2: Apply StandardScaler normalization\n",
    "            print(f\"Step 2: Applying StandardScaler normalization for {patient_name}...\")\n",
    "            df_normalized, channel_scalers = apply_zscore_normalization(df_loaded, patient_name)\n",
    "\n",
    "            if df_normalized is None:\n",
    "                print(f\"Normalization failed for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - Normalization error',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Step 3: Construct features\n",
    "            print(f\"Step 3: Constructing features for {patient_name}...\")\n",
    "            features_df = construct_features_for_patient(\n",
    "                df_normalized,\n",
    "                patient_name,\n",
    "                patients_data,\n",
    "                window_size_min=window_size,\n",
    "                overlap_min=window_overlap,\n",
    "                fs=25\n",
    "            )\n",
    "\n",
    "            if features_df is None:\n",
    "                print(f\"Feature construction failed for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - Feature construction error',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Step 4: Load labels (skip for Normal patients who don't have labels)\n",
    "            df_labels = None\n",
    "            if \"Normal\" not in patient_name:\n",
    "                print(f\"Step 4: Loading labels for {patient_name}...\")\n",
    "                try:\n",
    "                    df_labels = pl.read_excel(\n",
    "                        labels_dir / labels_filename,\n",
    "                        sheet_name=patient_name,\n",
    "                        columns=[0, 1, 2, 3],\n",
    "                        schema_overrides={\n",
    "                            \"time\": pl.Datetime,\n",
    "                            \"Glucose\": pl.Float64,\n",
    "                            \"Events\": pl.Utf8,\n",
    "                            \"Remarks\": pl.Utf8\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    if len(df_labels.columns) >= 4:\n",
    "                        df_labels = df_labels.select(df_labels.columns[:4])\n",
    "\n",
    "                    # Add metabolic state column\n",
    "                    df_labels = add_metabolic_state_column(df_labels)\n",
    "                    print(f\"Loaded {df_labels.shape[0]} labels for {patient_name}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"âš Could not load labels for {patient_name}: {e}\")\n",
    "                    df_labels = None\n",
    "            else:\n",
    "                print(f\"â„¹Skipping labels for {patient_name} (Normal patient)\")\n",
    "\n",
    "            # Step 5: Construct final dataset with new target labels\n",
    "            print(f\"Step 5: Constructing final dataset for {patient_name}...\")\n",
    "            if df_labels is not None:\n",
    "                final_df = construct_final_dataset_with_flags(\n",
    "                    features_df, df_labels, patient_name,\n",
    "                    glucose_lag_minutes, glycemic_prediction_minutes\n",
    "                )\n",
    "            else:\n",
    "                # For Normal patients without labels, create a simplified final dataset\n",
    "                final_df = features_df.copy()\n",
    "                final_df['CGM'] = None\n",
    "                final_df[f'lagged_CGM_{glucose_lag_minutes}min'] = None\n",
    "                final_df['metabolic_state'] = 'Normal' if 'Normal' in patient_name else None\n",
    "                final_df[f'future_CGM_{glycemic_prediction_minutes}min'] = None\n",
    "                final_df[f'glycemic_state_{glycemic_prediction_minutes}min'] = None\n",
    "                final_df['hypo_flag'] = 0  # Normal patients don't have hypoglycemia\n",
    "                final_df['hyper_flag'] = 0  # Normal patients don't have hyperglycemia\n",
    "\n",
    "                # Reorder columns to match the expected structure\n",
    "                glucose_cols = ['CGM', f'lagged_CGM_{glucose_lag_minutes}min', f'future_CGM_{glycemic_prediction_minutes}min']\n",
    "                target_cols = ['metabolic_state', f'glycemic_state_{glycemic_prediction_minutes}min', 'hypo_flag', 'hyper_flag']\n",
    "                other_cols = [col for col in final_df.columns if col not in glucose_cols + target_cols]\n",
    "\n",
    "                window_info_cols = ['patient', 'window_idx', 'start_time', 'end_time']\n",
    "                remaining_cols = [col for col in other_cols if col not in window_info_cols]\n",
    "\n",
    "                new_column_order = window_info_cols + glucose_cols + target_cols + remaining_cols\n",
    "                final_df = final_df[new_column_order]\n",
    "\n",
    "            if final_df is not None:\n",
    "                all_final_datasets.append(final_df)\n",
    "                print(f\"Successfully processed {patient_name}\")\n",
    "                print(f\"   - Windows: {len(final_df)}\")\n",
    "                print(f\"   - Features: {len(final_df.columns)}\")\n",
    "\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Success',\n",
    "                    'features_count': len(final_df.columns),\n",
    "                    'windows_count': len(final_df)\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Final dataset construction failed for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - Final dataset construction error',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {patient_name}: {str(e)}\")\n",
    "            processing_summary.append({\n",
    "                'patient': patient_name,\n",
    "                'status': f'Failed - {str(e)}',\n",
    "                'features_count': 0,\n",
    "                'windows_count': 0\n",
    "            })\n",
    "\n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "\n",
    "    # Combine all datasets\n",
    "    if all_final_datasets:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"COMBINING ALL DATASETS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        combined_df = pd.concat(all_final_datasets, ignore_index=True)\n",
    "\n",
    "        print(f\"Combined dataset created successfully!\")\n",
    "        print(f\"   - Total patients processed: {len(all_final_datasets)}\")\n",
    "        print(f\"   - Total windows: {len(combined_df)}\")\n",
    "        print(f\"   - Total features: {len(combined_df.columns)}\")\n",
    "\n",
    "        # Save combined dataset (all features)\n",
    "        combined_output_file = output_dir / \"combined_features_and_targets_all_features.csv\"\n",
    "        combined_df.to_csv(combined_output_file, index=False)\n",
    "        print(f\"   - Full dataset saved to: {combined_output_file}\")\n",
    "\n",
    "        # Display summary statistics\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PROCESSING SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        summary_df = pd.DataFrame(processing_summary)\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        # Dataset statistics\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Patient distribution\n",
    "        print(f\"\\nPatient distribution:\")\n",
    "        print(combined_df['patient'].value_counts())\n",
    "\n",
    "        # Target statistics (for patients with labels)\n",
    "        if 'CGM' in combined_df.columns:\n",
    "            cgm_available = combined_df['CGM'].notna().sum()\n",
    "            print(f\"\\nCGM targets available: {cgm_available}/{len(combined_df)} ({cgm_available/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "        if 'metabolic_state' in combined_df.columns:\n",
    "            print(f\"\\nMetabolic state distribution:\")\n",
    "            print(combined_df['metabolic_state'].value_counts())\n",
    "\n",
    "        if f'glycemic_state_{glycemic_prediction_minutes}min' in combined_df.columns:\n",
    "            glycemic_available = combined_df[f'glycemic_state_{glycemic_prediction_minutes}min'].notna().sum()\n",
    "            print(f\"\\nGlycemic state targets available: {glycemic_available}/{len(combined_df)} ({glycemic_available/len(combined_df)*100:.1f}%)\")\n",
    "            if glycemic_available > 0:\n",
    "                print(f\"Glycemic state distribution:\")\n",
    "                print(combined_df[f'glycemic_state_{glycemic_prediction_minutes}min'].value_counts())\n",
    "\n",
    "        # New flag statistics\n",
    "        if 'hypo_flag' in combined_df.columns:\n",
    "            hypo_flags = combined_df['hypo_flag'].sum()\n",
    "            print(f\"\\nHypoglycemia flags (within 900s): {hypo_flags}/{len(combined_df)} ({hypo_flags/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "        if 'hyper_flag' in combined_df.columns:\n",
    "            hyper_flags = combined_df['hyper_flag'].sum()\n",
    "            print(f\"Hyperglycemia flags (within 900s): {hyper_flags}/{len(combined_df)} ({hyper_flags/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"âŒ No datasets were successfully processed!\")\n",
    "        return None"
   ],
   "id": "9eef3e2ba86340c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Execute the feature construction pipeline for all patients",
   "id": "8aeedbbdb79af7e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Execute the complete feature construction pipeline\n",
    "print(\"Starting the complete feature construction pipeline...\")\n",
    "\n",
    "# Process all patients and create the combined features dataset\n",
    "combined_features_df = process_all_patients(\n",
    "    patients_data=patients_data,\n",
    "    signals_dir=signals_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    labels_filename=labels_filename,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "if combined_features_df is not None:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FEATURE CONSTRUCTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  Shape: {combined_features_df.shape}\")\n",
    "    print(f\"  Patients: {combined_features_df['patient'].nunique()}\")\n",
    "    print(f\"  Windows: {len(combined_features_df)}\")\n",
    "    print(f\"  Features: {len(combined_features_df.columns)}\")\n",
    "\n",
    "    # Display first few rows info\n",
    "    print(f\"\\nColumn overview:\")\n",
    "    print(f\"  - Metadata columns: {['patient', 'window_idx', 'start_time', 'end_time']}\")\n",
    "    print(f\"  - Target columns: {[col for col in combined_features_df.columns if 'CGM' in col or 'state' in col or 'flag' in col]}\")\n",
    "    print(f\"  - Feature columns: {len([col for col in combined_features_df.columns if col not in ['patient', 'window_idx', 'start_time', 'end_time'] and 'CGM' not in col and 'state' not in col and 'flag' not in col])}\")\n",
    "\n",
    "    print(f\"\\nCombined features dataset is now available as 'combined_features_df'\")\n",
    "    print(f\"Feature construction pipeline completed successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Feature construction pipeline failed!\")\n",
    "    print(\"Please check the error messages above and ensure:\")\n",
    "    print(\"  - Signal files are available in the signals_dir\")\n",
    "    print(\"  - Patient data is correctly loaded\")\n",
    "    print(\"  - Directory permissions are correct\")\n"
   ],
   "id": "8103e7631ff033ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gc.collect()\n",
   "id": "1c8681d04bcf6394",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
