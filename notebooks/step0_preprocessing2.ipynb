{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefa4f2a707ae5a3",
   "metadata": {},
   "source": [
    "# Magnetic Signal Data Preprocessing Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive preprocessing pipeline for magnetic field data collected from Type 1 Diabetes patients and normal subjects. The pipeline processes raw TDMS files containing voltage measurements from magnetic sensors positioned at various anatomical locations (head, hand, liver) and converts them into clean, downsampled signals suitable for analysis.\n",
    "\n",
    "## Scientific Context\n",
    "The preprocessing supports research into magnetic field signatures generated by ATP synthase activity in mitochondria. The fundamental hypothesis is that glucose metabolism affects ATP production, which generates detectable magnetic fields that vary with diabetic state and metabolic activity.\n",
    "\n",
    "## Data Sources\n",
    "- **Normal Subjects**: Baseline magnetic field measurements from healthy individuals\n",
    "- **T1DM Clamp Subjects**: Magnetic field data from Type 1 Diabetes patients during insulin clamp procedures\n",
    "- **Sensor Configuration**: Dual-channel magnetic sensors positioned at:\n",
    "  - Head (left/right channels)\n",
    "  - Hand (dual channels)\n",
    "  - Liver (dual channels)\n",
    "  - Background (dual channels for noise reference)\n",
    "\n",
    "## Processing Pipeline\n",
    "\n",
    "### 1. Raw Data Loading\n",
    "- Reads TDMS files containing voltage measurements from magnetic sensors\n",
    "- Applies GMT+2 timezone correction for sensor timestamps\n",
    "- Handles multiple patient datasets with different subdirectories\n",
    "\n",
    "### 2. Unit Conversion\n",
    "- Converts raw voltage signals to magnetic field strength (nanoTesla)\n",
    "- Uses calibrated conversion factor: 20 nT per 1V\n",
    "- Maintains temporal alignment across all channels\n",
    "\n",
    "### 3. Anti-Aliasing Filtering\n",
    "- Applies 6th-order Butterworth band-pass filter with 0.05-10Hz bandpass\n",
    "- Removes high-frequency noise and prevents aliasing artifacts\n",
    "- Uses zero-phase filtering (filtfilt) to preserve signal timing\n",
    "\n",
    "### 4. Saturation Handling\n",
    "- Detects and removes saturated samples (>±250 nT threshold)\n",
    "- Excludes saturated values from averaging calculations\n",
    "- Tracks saturation statistics for quality assessment\n",
    "\n",
    "### 5. Downsampling\n",
    "- Reduces sampling rate from 5000Hz to 25Hz using averaging\n",
    "- Maintains signal quality while reducing computational requirements\n",
    "- Preserves temporal resolution sufficient for metabolic analysis\n",
    "\n",
    "### 6. Data Storage\n",
    "- Saves processed signals in efficient Parquet format\n",
    "- Updates patient metadata with processed file locations\n",
    "- Enables fast loading for subsequent analysis steps\n",
    "\n",
    "### Signal Processing\n",
    "- **Anti-aliasing**: 10 Hz cutoff, 6th-order Butterworth\n",
    "- **Noise Mitigation**: 50 Hz powerline and harmonics awareness (not used in this step)\n",
    "- **Quality Control**: Saturation detection and removal\n",
    "\n",
    "## Visualization Features\n",
    "- Time series plots by channel groups\n",
    "- Channel correlation analysis (within and between locations)\n",
    "- Signal quality assessment with saturation flagging\n",
    "- Cross-correlation analysis for sensor pair validation\n",
    "\n",
    "## Output Files\n",
    "- **Processed Signals**: `{PatientName}_downsampled_25hz.parquet`\n",
    "- **Updated Metadata**: `patients.json` with processing status\n",
    "- **Quality Metrics**: Saturation statistics and signal characteristics\n",
    "\n",
    "## Usage Notes\n",
    "- Set `Visualize_Signal = True` to enable plotting during processing\n",
    "- Processed files are stored in `ProcessedData/Signal_Files/` directory\n",
    "- Analysis can resume from processed files without reprocessing raw data\n",
    "- Memory usage is optimized through incremental processing and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-09T17:01:27.003618Z",
     "start_time": "2025-08-09T17:01:22.930817Z"
    }
   },
   "source": [
    "# General imports:\n",
    "\n",
    "# Disable warnings:\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Essential imports\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import read_tdms\n",
    "\n",
    "# Add signal processing imports for antialiasing filter\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Plotting enhancements\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "bd4554e54e461122",
   "metadata": {},
   "source": [
    "#### Base variables and constants:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c118a917591d5e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:01:27.037665Z",
     "start_time": "2025-08-09T17:01:27.019777Z"
    }
   },
   "source": [
    "# Key Settings:\n",
    "Visualize_Signal = False # Signal visualization (only if required)\n",
    "\n",
    "# Physical constants and sensors specifications:\n",
    "SENSITIVITY = 50  # mV/nT\n",
    "MAGNETIC_NOISE = 3  # pT/√Hz @ 1 Hz\n",
    "MAX_AC_LINEARITY = 250  # nT (+/- 250 nT) - Equivalent to 21.78 V\n",
    "MAX_DC_LINEARITY = 60  # nT (+/- 60 nT)\n",
    "VOLTAGE_LIMIT = 15 # V (+/-15V)\n",
    "CONVERSION_FACTOR = 20  # nT per 1V\n",
    "SAMPLING_FREQUENCY = 5000  # Hz - expected from the experimental data\n",
    "SENSOR_SATURATION = 250  # nT - saturation threshold for the sensor\n",
    "\n",
    "# Subjects and their types\n",
    "Subject = {\"Normal\": \"Normal Subjects\",\"Clamp\": \"T1DM Clamp Subjects\", \"Additional\": \"Additional Subjects\"}\n",
    "\n",
    "# Path and directories\n",
    "base_dir = Path(\"../../../Data\")\n",
    "\n",
    "# Output directory for saving results\n",
    "output_dir = base_dir / \"ProcessedData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Directory for saving processed/downsampled signal files (parquet format)\n",
    "signals_dir = output_dir / \"Signal_Files\"\n",
    "os.makedirs(signals_dir, exist_ok=True)\n",
    "\n",
    "# Labels directory\n",
    "labels_dir = base_dir / \"RawData\"\n",
    "labels_filename = \"FilteredLabels.xlsx\"\n",
    "\n",
    "# Patients data file\n",
    "patients_file = \"patients.json\"\n",
    "\n",
    "# Patient Data\n",
    "with open(labels_dir / patients_file, 'r') as f:\n",
    "    # Load the JSON data\n",
    "    patients_data = json.load(f)\n",
    "\n",
    "# GMT zone correction for the sensor = GMT+2\n",
    "GMT = 2\n",
    "\n",
    "# Key frequencies from background noise analysis\n",
    "POWER_LINE_FREQ = 50  # Hz\n",
    "POWERLINE_HARMONICS = [POWER_LINE_FREQ*i for i in range(1, 4)]  # 50, 100, 150 Hz.\n",
    "\n",
    "# Filter parameters:\n",
    "HIGHCUT_FREQ = 10  # Hz low-pass filter cutoff frequency\n",
    "LOWCUT_FREQ = 0.05 # Hz high-pass filter cutoff frequency\n",
    "FILTER_ORDER = 6 # for steeper roll-off\n",
    "\n",
    "# Channel grouping\n",
    "signal_channels = {\n",
    "    'Head': ['Head_left', 'Head_right'],\n",
    "    'Hand': ['Hand1', 'Hand2'],\n",
    "    'Liver': ['Liver1', 'Liver2'],\n",
    "    'Background': ['Background1', 'Background2']\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "aafcb0a30f000119",
   "metadata": {},
   "source": [
    "#### Define voltage to nanoTesla conversion function"
   ]
  },
  {
   "cell_type": "code",
   "id": "114ca08fa4927850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:01:27.062626Z",
     "start_time": "2025-08-09T17:01:27.050394Z"
    }
   },
   "source": [
    "def convert_voltage_to_nanotesla(df, signal_channels, conversion_factor):\n",
    "    \"\"\"\n",
    "    Convert raw voltage signals to nanoTesla (nT).\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with voltage signal data\n",
    "    - signal_channels: dictionary of channel groups\n",
    "    - conversion_factor: conversion factor in nT per Volt (default: 20 nT/V)\n",
    "\n",
    "    Returns:\n",
    "    - df_converted: polars DataFrame with signals converted to nT\n",
    "    \"\"\"\n",
    "    # Get all signal channel names (exclude time column)\n",
    "    signal_column_names = []\n",
    "    for channel_group in signal_channels.values():\n",
    "        signal_column_names.extend(channel_group)\n",
    "\n",
    "    # Apply conversion to each signal channel\n",
    "    converted_data = {}\n",
    "\n",
    "    # Keep the time column unchanged\n",
    "    time_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is not None:\n",
    "        converted_data[time_col] = df[time_col].to_numpy()\n",
    "        print(f\"Time column '{time_col}' preserved during conversion\")\n",
    "    else:\n",
    "        print(\"Warning: No time column found in input data\")\n",
    "\n",
    "    print(f\"Converting voltage signals to nanoTesla using factor: {conversion_factor} nT/V\")\n",
    "\n",
    "    for channel in tqdm(signal_column_names, desc=\"Converting channels\"):\n",
    "        if channel in df.columns:\n",
    "            # Extract voltage signal data\n",
    "            voltage_signal = df[channel].to_numpy()\n",
    "\n",
    "            # Convert to nanoTesla: nT = V × conversion_factor\n",
    "            nanotesla_signal = voltage_signal * conversion_factor\n",
    "\n",
    "            # Store converted signal\n",
    "            converted_data[channel] = nanotesla_signal\n",
    "        else:\n",
    "            print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "    # Convert back to polars DataFrame\n",
    "    df_converted = pl.DataFrame(converted_data)\n",
    "\n",
    "    print(f\"Voltage to nanoTesla conversion completed. Converted {len(signal_column_names)} channels.\")\n",
    "    print(f\"Signal values are now in nanoTesla (nT) units.\")\n",
    "\n",
    "    return df_converted"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "9d1e8859ac3fdcf7",
   "metadata": {},
   "source": [
    "#### Define the anti-aliasing filter function"
   ]
  },
  {
   "cell_type": "code",
   "id": "116d448b50dd23e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:01:27.089013Z",
     "start_time": "2025-08-09T17:01:27.075395Z"
    }
   },
   "source": [
    "def apply_antialiasing_filter(df, lowcut_freq=0.05, highcut_freq=10, sampling_freq=5000, filter_order=6):\n",
    "    \"\"\"\n",
    "    Apply low-pass and high-pass filters separately with improved stability.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with signal data\n",
    "    - lowcut_freq: low cutoff frequency in Hz (default: 0.05Hz)\n",
    "    - highcut_freq: high cutoff frequency in Hz (default: 10Hz)\n",
    "    - sampling_freq: sampling frequency in Hz (default: 5000Hz)\n",
    "    - filter_order: filter order (default: 6)\n",
    "\n",
    "    Returns:\n",
    "    - df_filtered: polars DataFrame with filtered signals\n",
    "    \"\"\"\n",
    "    # Calculate normalized cutoff frequencies\n",
    "    nyq = 0.5 * sampling_freq\n",
    "    high_normal = highcut_freq / nyq\n",
    "\n",
    "    # Design Butterworth low-pass filter\n",
    "    b_low, a_low = butter(filter_order, high_normal, btype='low', analog=False)\n",
    "\n",
    "    # Design high-pass filter with improved stability\n",
    "    apply_highpass = lowcut_freq > 0\n",
    "    if apply_highpass:\n",
    "        low_normal = lowcut_freq / nyq\n",
    "        print(f\"High-pass normalized frequency: {low_normal:.6f}\")\n",
    "\n",
    "        # Use lower filter order for very low frequencies to improve stability\n",
    "        hp_filter_order = min(filter_order, 4) if low_normal < 0.001 else filter_order\n",
    "\n",
    "        # Design high-pass filter with SOS (Second-Order Sections) for better numerical stability\n",
    "        sos_high = butter(hp_filter_order, low_normal, btype='high', analog=False, output='sos')\n",
    "        print(f\"Using filter order {hp_filter_order} for high-pass filter\")\n",
    "\n",
    "    # Get all signal channel names (exclude time column)\n",
    "    signal_column_names = []\n",
    "    for channel_group in signal_channels.values():\n",
    "        signal_column_names.extend(channel_group)\n",
    "\n",
    "    # Apply filter to each signal channel\n",
    "    filtered_data = {}\n",
    "\n",
    "    # Keep the time column unchanged\n",
    "    time_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is not None:\n",
    "        filtered_data[time_col] = df[time_col].to_numpy()\n",
    "        print(f\"Time column '{time_col}' preserved in filtered data\")\n",
    "    else:\n",
    "        print(\"Warning: No time column found in input data\")\n",
    "\n",
    "    filter_description = f\"low-pass ({highcut_freq}Hz)\"\n",
    "    if apply_highpass:\n",
    "        filter_description = f\"high-pass ({lowcut_freq}Hz) + {filter_description}\"\n",
    "\n",
    "    print(f\"Applying {filter_description} filters to channels...\")\n",
    "\n",
    "    for channel in tqdm(signal_column_names, desc=\"Filtering channels\"):\n",
    "        if channel in df.columns:\n",
    "            # Extract signal data\n",
    "            signal = df[channel].to_numpy()\n",
    "\n",
    "            # First apply low-pass filter\n",
    "            filtered_signal = filtfilt(b_low, a_low, signal)\n",
    "\n",
    "            # Then apply high-pass filter if needed using SOS format\n",
    "            if apply_highpass:\n",
    "                from scipy.signal import sosfiltfilt\n",
    "                filtered_signal = sosfiltfilt(sos_high, filtered_signal)\n",
    "\n",
    "            # Check for NaN values and report\n",
    "            if np.any(np.isnan(filtered_signal)):\n",
    "                print(f\"Warning: NaN values detected in {channel} after filtering\")\n",
    "                # Option: replace NaN with interpolated values or skip this channel\n",
    "                nan_count = np.sum(np.isnan(filtered_signal))\n",
    "                print(f\"  {nan_count} NaN values out of {len(filtered_signal)} samples\")\n",
    "\n",
    "                # Simple NaN handling: replace with median of non-NaN values\n",
    "                if nan_count < len(filtered_signal) * 0.1:  # Less than 10% NaN\n",
    "                    median_val = np.nanmedian(filtered_signal)\n",
    "                    filtered_signal = np.where(np.isnan(filtered_signal), median_val, filtered_signal)\n",
    "                    print(f\"  Replaced NaN values with median: {median_val:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  Too many NaN values ({nan_count}/{len(filtered_signal)}), skipping channel\")\n",
    "                    continue\n",
    "\n",
    "            # Store filtered signal\n",
    "            filtered_data[channel] = filtered_signal\n",
    "        else:\n",
    "            print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "    # Convert back to polars DataFrame\n",
    "    df_filtered = pl.DataFrame(filtered_data)\n",
    "\n",
    "    print(f\"Filtering completed successfully. Processed {len(signal_column_names)} channels.\")\n",
    "    return df_filtered"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "fb41488d25d27d23",
   "metadata": {},
   "source": [
    "#### Downsample the data using averaging"
   ]
  },
  {
   "cell_type": "code",
   "id": "2bb40bb367208085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:01:27.122275Z",
     "start_time": "2025-08-09T17:01:27.101332Z"
    }
   },
   "source": [
    "def downsample_data(df, original_fs=5000, target_fs=25, fix_saturated=True):\n",
    "    \"\"\"\n",
    "    Downsample the (filtered) data using averaging.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with filtered signal data\n",
    "    - original_fs: original sampling frequency in Hz (default: 5000Hz)\n",
    "    - target_fs: target sampling frequency in Hz (default: 25Hz)\n",
    "    - fix_saturated: bool, if True removes saturated values before averaging (default: True)\n",
    "\n",
    "    Returns:\n",
    "    - df_downsampled: polars DataFrame with downsampled signals\n",
    "    \"\"\"\n",
    "    # Calculate downsampling factor\n",
    "    downsample_factor = original_fs // target_fs\n",
    "    print(f\"Downsampling from {original_fs}Hz to {target_fs}Hz (factor: {downsample_factor})\")\n",
    "\n",
    "    if fix_saturated:\n",
    "        print(\"Saturation removal enabled - will exclude saturated values from averaging\")\n",
    "\n",
    "    # Get signal column names (exclude time column)\n",
    "    signal_column_names = []\n",
    "    for channel_group in signal_channels.values():\n",
    "        signal_column_names.extend(channel_group)\n",
    "\n",
    "    # Calculate number of complete windows\n",
    "    n_samples = df.shape[0]\n",
    "    n_windows = n_samples // downsample_factor\n",
    "    print(f\"Processing {n_samples} samples into {n_windows} downsampled points\")\n",
    "\n",
    "    # Initialize dictionary for downsampled data\n",
    "    downsampled_data = {}\n",
    "\n",
    "    # Downsample time column if present - check for both 'time' and 'Time'\n",
    "    time_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is not None:\n",
    "        time_data = df[time_col].to_numpy()\n",
    "        # Take every nth sample for time (or average if needed)\n",
    "        downsampled_time = time_data[::downsample_factor][:n_windows]\n",
    "        downsampled_data[time_col] = downsampled_time\n",
    "        print(f\"Time column '{time_col}' downsampled\")\n",
    "    else:\n",
    "        print(\"Warning: No time column found for downsampling\")\n",
    "\n",
    "    # Define saturation thresholds based on physical constants\n",
    "    # Use the predefined sensor saturation threshold\n",
    "    saturation_threshold_nt = SENSOR_SATURATION  # Saturation threshold for the sensor\n",
    "    print(f\"Saturation threshold: ±{saturation_threshold_nt} nT\")\n",
    "\n",
    "    # Track saturation statistics\n",
    "    total_saturated_samples = 0\n",
    "    saturated_windows = 0\n",
    "\n",
    "    # Downsample each signal channel using averaging\n",
    "    print(\"Downsampling channels using averaging...\")\n",
    "    for channel in tqdm(signal_column_names, desc=\"Downsampling channels\"):\n",
    "        if channel in df.columns:\n",
    "            # Extract signal data\n",
    "            signal = df[channel].to_numpy()\n",
    "\n",
    "            # Reshape for averaging (trim to complete windows)\n",
    "            signal_windowed = signal[:n_windows * downsample_factor].reshape(n_windows, downsample_factor)\n",
    "\n",
    "            if fix_saturated:\n",
    "                # Apply saturation filtering before averaging\n",
    "                downsampled_signal = []\n",
    "                saturation_flags = []  # Track which windows had saturation issues\n",
    "                channel_saturated_samples = 0\n",
    "                channel_saturated_windows = 0\n",
    "\n",
    "                for window in signal_windowed:\n",
    "                    # Identify saturated samples (beyond threshold)\n",
    "                    # Use > instead of >= to avoid edge case issues\n",
    "                    saturated_mask = np.abs(window) > saturation_threshold_nt\n",
    "                    saturated_count = np.sum(saturated_mask)\n",
    "\n",
    "                    if saturated_count > 0:\n",
    "                        channel_saturated_samples += saturated_count\n",
    "                        channel_saturated_windows += 1\n",
    "\n",
    "                        # Remove saturated values from averaging\n",
    "                        valid_samples = window[~saturated_mask]\n",
    "\n",
    "                        if len(valid_samples) > 0:\n",
    "                            # Average only non-saturated samples\n",
    "                            window_avg = np.mean(valid_samples)\n",
    "                            saturation_flags.append(1)  # Partially saturated\n",
    "                        else:\n",
    "                            # If all samples are saturated, mark as invalid\n",
    "                            # Use NaN to indicate unreliable data\n",
    "                            window_avg = np.nan\n",
    "                            saturation_flags.append(2)  # Fully saturated (unreliable)\n",
    "\n",
    "                        downsampled_signal.append(window_avg)\n",
    "                    else:\n",
    "                        # No saturation, use normal averaging\n",
    "                        downsampled_signal.append(np.mean(window))\n",
    "                        saturation_flags.append(0)  # No saturation\n",
    "\n",
    "                downsampled_signal = np.array(downsampled_signal)\n",
    "\n",
    "                # Note: Saturation flags are tracked for statistics but not stored in output data\n",
    "\n",
    "                # Update statistics\n",
    "                total_saturated_samples += channel_saturated_samples\n",
    "                saturated_windows += channel_saturated_windows\n",
    "\n",
    "                if channel_saturated_samples > 0:\n",
    "                    saturation_percentage = (channel_saturated_samples / (n_windows * downsample_factor)) * 100\n",
    "                    print(f\"  {channel}: {channel_saturated_samples} saturated samples ({saturation_percentage:.2f}%) in {channel_saturated_windows} windows\")\n",
    "            else:\n",
    "                # Standard averaging without saturation handling\n",
    "                downsampled_signal = np.mean(signal_windowed, axis=1)\n",
    "\n",
    "            # Store downsampled signal\n",
    "            downsampled_data[channel] = downsampled_signal\n",
    "        else:\n",
    "            print(f\"Warning: Channel '{channel}' not found in filtered data\")\n",
    "\n",
    "    # Print saturation summary\n",
    "    if fix_saturated and total_saturated_samples > 0:\n",
    "        total_samples = n_windows * downsample_factor * len(signal_column_names)\n",
    "        overall_saturation_percentage = (total_saturated_samples / total_samples) * 100\n",
    "        print(f\"\\nSaturation Summary:\")\n",
    "        print(f\"  Total saturated samples: {total_saturated_samples}\")\n",
    "        print(f\"  Total windows with saturation: {saturated_windows}\")\n",
    "        print(f\"  Overall saturation rate: {overall_saturation_percentage:.2f}%\")\n",
    "\n",
    "    # Convert to polars DataFrame\n",
    "    df_downsampled = pl.DataFrame(downsampled_data)\n",
    "\n",
    "    print(f\"Downsampling completed. New shape: {df_downsampled.shape}\")\n",
    "    print(f\"Effective sampling rate: {target_fs}Hz\")\n",
    "\n",
    "    return df_downsampled"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "d56e73c656592d22",
   "metadata": {},
   "source": [
    "#### Main pipeline:\n",
    "- Load the TDMS file and extract the data\n",
    "- Apply the anti-aliasing low-pass filter (10hz cutoff)\n",
    "- Downsample the data to 25Hz using averaging\n",
    "- Save the downsampled data to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7d8934c4da37698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.097675Z",
     "start_time": "2025-08-09T17:01:27.139415Z"
    }
   },
   "source": [
    "\n",
    "# Process all Normal, Additional and Insulin Clamp patients\n",
    "patients_to_process = [patient_name for patient_name in patients_data.keys()\n",
    "                      if \"Normal\" in patient_name or \"Clamp\" or \"Additional\" in patient_name]\n",
    "\n",
    "print(f\"Found {len(patients_to_process)} patients to process:\")\n",
    "for p in patients_to_process:\n",
    "    print(f\"  - {p}\")\n",
    "\n",
    "# Process each patient\n",
    "for current_patient in patients_to_process:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING PATIENT: {current_patient}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        # Get patient-specific data\n",
    "        tdms_file = patients_data[current_patient][\"tdms_file\"]\n",
    "        sub_dir = patients_data[current_patient][\"sub_dir\"]\n",
    "\n",
    "        # Determine the correct path based on patient type\n",
    "        if \"Clamp\" in current_patient:\n",
    "            path = base_dir / \"RawData\" / Subject[\"Clamp\"] / sub_dir / tdms_file\n",
    "        elif \"Normal\" in current_patient:\n",
    "            path = base_dir / \"RawData\" / Subject[\"Normal\"] / sub_dir / tdms_file\n",
    "        elif \"Additional\" in current_patient:\n",
    "            path = base_dir / \"RawData\" / Subject[\"Additional\"] / sub_dir / tdms_file\n",
    "        else:\n",
    "            print(f\"Unknown patient type for {current_patient}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing file: {path}\")\n",
    "\n",
    "        # Read the data from tdms file into polars dataframe\n",
    "        try:\n",
    "            df = read_tdms.read_tdms_file_to_dataframe(str(path), GMT)\n",
    "            print(f\"Successfully loaded data with {df.shape[0]} samples and {df.shape[1]} channels\")\n",
    "            print(\"Raw data is in Volts - converting to nanoTesla (nT)...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading TDMS file for {current_patient}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert raw voltage signals to nanoTesla (nT)\n",
    "        if df is not None:\n",
    "            df_converted = convert_voltage_to_nanotesla(df, signal_channels, CONVERSION_FACTOR)\n",
    "            print(f\"Converted data shape: {df_converted.shape}\")\n",
    "        else:\n",
    "            print(f\"No data loaded to convert for {current_patient}\")\n",
    "            continue\n",
    "\n",
    "        # Apply the anti-aliasing filter to all signal channels\n",
    "        if df_converted is not None:\n",
    "            print(\"Applying band-pass filter...\")\n",
    "            df_filtered = apply_antialiasing_filter(df_converted, lowcut_freq=LOWCUT_FREQ, highcut_freq=HIGHCUT_FREQ, sampling_freq=SAMPLING_FREQUENCY, filter_order=FILTER_ORDER)\n",
    "            print(f\"Filtered data shape: {df_filtered.shape}\")\n",
    "        else:\n",
    "            print(f\"No converted data available for filtering for {current_patient}\")\n",
    "            continue\n",
    "\n",
    "        # Apply downsampling to the filtered data\n",
    "        if df_filtered is not None:\n",
    "            print(\"Downsampling filtered data to 25Hz...\")\n",
    "            df_downsampled = downsample_data(df_filtered, original_fs=SAMPLING_FREQUENCY, target_fs=25)\n",
    "            print(f\"Downsampled data shape: {df_downsampled.shape}\")\n",
    "        else:\n",
    "            print(f\"No filtered data available for downsampling for {current_patient}\")\n",
    "            continue\n",
    "\n",
    "        # Save downsampled data to parquet file\n",
    "        if df_downsampled is not None:\n",
    "            print(\"Saving downsampled data to parquet file...\")\n",
    "            # Create output filename based on patient name\n",
    "            patient_safe_name = current_patient.replace(\" \", \"_\").replace(\"#\", \"\")\n",
    "            output_filename = f\"{patient_safe_name}_downsampled_25hz.parquet\"\n",
    "            output_path = signals_dir / output_filename\n",
    "\n",
    "            print(f\"Saving downsampled data to: {output_path}\")\n",
    "\n",
    "            try:\n",
    "                # Save to parquet format\n",
    "                df_downsampled.write_parquet(output_path)\n",
    "                print(f\"Successfully saved downsampled data to {output_filename}\")\n",
    "                print(f\"File size: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "                # Update patients_data with signal_file field\n",
    "                patients_data[current_patient]['signal_file'] = output_filename\n",
    "                print(f\"Updated patients_data with signal_file: {output_filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving parquet file for {current_patient}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"No downsampled data available to save for {current_patient}\")\n",
    "            continue\n",
    "\n",
    "        # Clean up memory\n",
    "        del df, df_converted, df_filtered, df_downsampled\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Completed processing {current_patient}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing patient {current_patient}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save updated patients_data back to JSON file\n",
    "try:\n",
    "    patients_json_path = labels_dir / patients_file\n",
    "    with open(patients_json_path, 'w') as f:\n",
    "        json.dump(patients_data, f, indent=2)\n",
    "    print(f\"\\nSaved updated patients.json to: {patients_json_path}\")\n",
    "    print(f\"Processing completed for {len(patients_to_process)} patients\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving updated patients.json: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 patients to process:\n",
      "  - Additional #1\n",
      "  - Additional #2\n",
      "  - Additional #3\n",
      "\n",
      "============================================================\n",
      "PROCESSING PATIENT: Additional #1\n",
      "============================================================\n",
      "Processing file: ..\\..\\..\\Data\\RawData\\Additional Subjects\\Base\\ML base_1.tdms\n",
      "Reading ..\\..\\..\\Data\\RawData\\Additional Subjects\\Base\\ML base_1.tdms...\n",
      "Adjusted 'Time' column by GMT+2 hours\n",
      "Successfully processed ML base_1.tdms\n",
      "Successfully loaded data with 155000 samples and 9 channels\n",
      "Raw data is in Volts - converting to nanoTesla (nT)...\n",
      "Time column 'Time' preserved during conversion\n",
      "Converting voltage signals to nanoTesla using factor: 20 nT/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting channels: 100%|██████████| 8/8 [00:00<00:00, 1133.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage to nanoTesla conversion completed. Converted 8 channels.\n",
      "Signal values are now in nanoTesla (nT) units.\n",
      "Converted data shape: (155000, 9)\n",
      "Applying band-pass filter...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:00<00:00, 59.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape: (155000, 9)\n",
      "Downsampling filtered data to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 155000 samples into 775 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  50%|█████     | 4/8 [00:00<00:00, 39.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Liver1: 9417 saturated samples (6.08%) in 56 windows\n",
      "  Liver2: 9708 saturated samples (6.26%) in 56 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:00<00:00, 38.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 19125\n",
      "  Total windows with saturation: 112\n",
      "  Overall saturation rate: 1.54%\n",
      "Downsampling completed. New shape: (775, 9)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape: (775, 9)\n",
      "Saving downsampled data to parquet file...\n",
      "Saving downsampled data to: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\Additional_1_downsampled_25hz.parquet\n",
      "Successfully saved downsampled data to Additional_1_downsampled_25hz.parquet\n",
      "File size: 0.05 MB\n",
      "Updated patients_data with signal_file: Additional_1_downsampled_25hz.parquet\n",
      "Completed processing Additional #1\n",
      "\n",
      "============================================================\n",
      "PROCESSING PATIENT: Additional #2\n",
      "============================================================\n",
      "Processing file: ..\\..\\..\\Data\\RawData\\Additional Subjects\\GL Insulin recording\\GL insulin_1.tdms\n",
      "Reading ..\\..\\..\\Data\\RawData\\Additional Subjects\\GL Insulin recording\\GL insulin_1.tdms...\n",
      "Adjusted 'Time' column by GMT+2 hours\n",
      "Successfully processed GL insulin_1.tdms\n",
      "Successfully loaded data with 10017500 samples and 9 channels\n",
      "Raw data is in Volts - converting to nanoTesla (nT)...\n",
      "Time column 'Time' preserved during conversion\n",
      "Converting voltage signals to nanoTesla using factor: 20 nT/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting channels: 100%|██████████| 8/8 [00:00<00:00, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage to nanoTesla conversion completed. Converted 8 channels.\n",
      "Signal values are now in nanoTesla (nT) units.\n",
      "Converted data shape: (10017500, 9)\n",
      "Applying band-pass filter...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:04<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape: (10017500, 9)\n",
      "Downsampling filtered data to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 10017500 samples into 50087 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  12%|█▎        | 1/8 [00:01<00:13,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Head_left: 8555 saturated samples (0.09%) in 48 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  25%|██▌       | 2/8 [00:03<00:10,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Head_right: 8551 saturated samples (0.09%) in 47 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:05<00:09,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 8550 saturated samples (0.09%) in 48 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  50%|█████     | 4/8 [00:07<00:07,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 8549 saturated samples (0.09%) in 48 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  62%|██████▎   | 5/8 [00:08<00:05,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Liver1: 48449 saturated samples (0.48%) in 277 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  75%|███████▌  | 6/8 [00:10<00:03,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Liver2: 48624 saturated samples (0.49%) in 268 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:13<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Background2: 8557 saturated samples (0.09%) in 48 windows\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 139835\n",
      "  Total windows with saturation: 784\n",
      "  Overall saturation rate: 0.17%\n",
      "Downsampling completed. New shape: (50087, 9)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape: (50087, 9)\n",
      "Saving downsampled data to parquet file...\n",
      "Saving downsampled data to: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\Additional_2_downsampled_25hz.parquet\n",
      "Successfully saved downsampled data to Additional_2_downsampled_25hz.parquet\n",
      "File size: 3.06 MB\n",
      "Updated patients_data with signal_file: Additional_2_downsampled_25hz.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing Additional #2\n",
      "\n",
      "============================================================\n",
      "PROCESSING PATIENT: Additional #3\n",
      "============================================================\n",
      "Processing file: ..\\..\\..\\Data\\RawData\\Additional Subjects\\June 8 2025 EMF Insulin mimic #1\\morris_home_2.tdms\n",
      "Reading ..\\..\\..\\Data\\RawData\\Additional Subjects\\June 8 2025 EMF Insulin mimic #1\\morris_home_2.tdms...\n",
      "Adjusted 'Time' column by GMT+2 hours\n",
      "Successfully processed morris_home_2.tdms\n",
      "Successfully loaded data with 6750000 samples and 9 channels\n",
      "Raw data is in Volts - converting to nanoTesla (nT)...\n",
      "Time column 'Time' preserved during conversion\n",
      "Converting voltage signals to nanoTesla using factor: 20 nT/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting channels: 100%|██████████| 8/8 [00:00<00:00, 47.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage to nanoTesla conversion completed. Converted 8 channels.\n",
      "Signal values are now in nanoTesla (nT) units.\n",
      "Converted data shape: (6750000, 9)\n",
      "Applying band-pass filter...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:03<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape: (6750000, 9)\n",
      "Downsampling filtered data to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 6750000 samples into 33750 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:03<00:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 37891 saturated samples (0.56%) in 199 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  50%|█████     | 4/8 [00:04<00:04,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 35550 saturated samples (0.53%) in 194 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  62%|██████▎   | 5/8 [00:05<00:03,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Liver1: 38923 saturated samples (0.58%) in 232 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  75%|███████▌  | 6/8 [00:06<00:02,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Liver2: 41324 saturated samples (0.61%) in 247 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:09<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 153688\n",
      "  Total windows with saturation: 872\n",
      "  Overall saturation rate: 0.28%\n",
      "Downsampling completed. New shape: (33750, 9)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape: (33750, 9)\n",
      "Saving downsampled data to parquet file...\n",
      "Saving downsampled data to: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\Additional_3_downsampled_25hz.parquet\n",
      "Successfully saved downsampled data to Additional_3_downsampled_25hz.parquet\n",
      "File size: 2.06 MB\n",
      "Updated patients_data with signal_file: Additional_3_downsampled_25hz.parquet\n",
      "Completed processing Additional #3\n",
      "\n",
      "Saved updated patients.json to: ..\\..\\..\\Data\\RawData\\patients.json\n",
      "Processing completed for 3 patients\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "45dec9c1674864d0",
   "metadata": {},
   "source": [
    "#### One may start the analysis from here, by loading the downsampled data from parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254b9f2000ea240",
   "metadata": {},
   "source": [
    "#### Load the downsampled data from parquet file"
   ]
  },
  {
   "cell_type": "code",
   "id": "185fbcf2e31c8e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.272648Z",
     "start_time": "2025-08-09T17:02:10.264028Z"
    }
   },
   "source": [
    "# Load the downsampled data from parquet file\n",
    "def load_downsampled_data(patient_name, signals_dir):\n",
    "    \"\"\"\n",
    "    Load downsampled data from parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - patient_name: name of the patient\n",
    "    - signals_dir: directory containing the parquet signal files\n",
    "\n",
    "    Returns:\n",
    "    - df_loaded: polars DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    # Create filename based on patient name\n",
    "    patient_safe_name = patient_name.replace(\" \", \"_\").replace(\"#\", \"\")\n",
    "    output_filename = f\"{patient_safe_name}_downsampled_25hz.parquet\"\n",
    "    output_path = signals_dir / output_filename\n",
    "\n",
    "    try:\n",
    "        if output_path.exists():\n",
    "            df_loaded = pl.read_parquet(output_path)\n",
    "            print(f\"Successfully loaded data from {output_filename}\")\n",
    "            print(f\"Loaded data shape: {df_loaded.shape}\")\n",
    "            return df_loaded\n",
    "        else:\n",
    "            print(f\"File not found: {output_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file: {e}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "a88128ce4526b1e8",
   "metadata": {},
   "source": [
    "#### Define plotting functions for time series visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "2299f2422b8ca3f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.301223Z",
     "start_time": "2025-08-09T17:02:10.283783Z"
    }
   },
   "source": [
    "def plot_timeseries_by_channel(df, signal_channels, patient_name, time_window_hours=None, y_label='Signal (nT)'):\n",
    "    \"\"\"\n",
    "    Plot time series for each channel group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with time series data\n",
    "    - signal_channels: dictionary of channel groups\n",
    "    - patient_name: name of the patient for plot title\n",
    "    - time_window_hours: optional time window in hours to limit the plot\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas for easier plotting\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Find the time column (case-insensitive)\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"No time column found for plotting\")\n",
    "        return\n",
    "\n",
    "    # Limit time window if specified\n",
    "    if time_window_hours is not None:\n",
    "        start_time = df_pandas[time_col].min()\n",
    "        end_time = start_time + pd.Timedelta(hours=time_window_hours)\n",
    "        df_pandas = df_pandas[df_pandas[time_col] <= end_time]\n",
    "        print(f\"Plotting first {time_window_hours} hours of data\")\n",
    "\n",
    "    # Create subplots for each channel group\n",
    "    n_groups = len(signal_channels)\n",
    "    fig, axes = plt.subplots(n_groups, 1, figsize=(15, 4*n_groups))\n",
    "\n",
    "    if n_groups == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Plot each channel group\n",
    "    for i, (group_name, channels) in enumerate(signal_channels.items()):\n",
    "        ax = axes[i]\n",
    "\n",
    "        for channel in channels:\n",
    "            if channel in df_pandas.columns:\n",
    "                ax.plot(df_pandas[time_col], df_pandas[channel], label=channel, alpha=0.8)\n",
    "            else:\n",
    "                print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "        ax.set_title(f'{group_name} Channels - {patient_name}')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Format x-axis for better readability\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "        ax.xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_channels_overlay(df, signal_channels, patient_name, time_window_hours=None, y_label='Signal (nT)'):\n",
    "    \"\"\"\n",
    "    Plot all channels overlaid on a single plot.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with time series data\n",
    "    - signal_channels: dictionary of channel groups\n",
    "    - patient_name: name of the patient for plot title\n",
    "    - time_window_hours: optional time window in hours to limit the plot\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas for easier plotting\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Find the time column (case-insensitive)\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"No time column found for plotting\")\n",
    "        return\n",
    "\n",
    "    # Limit time window if specified\n",
    "    if time_window_hours is not None:\n",
    "        start_time = df_pandas[time_col].min()\n",
    "        end_time = start_time + pd.Timedelta(hours=time_window_hours)\n",
    "        df_pandas = df_pandas[df_pandas[time_col] <= end_time]\n",
    "        print(f\"Plotting first {time_window_hours} hours of data\")\n",
    "\n",
    "    # Create single plot with all channels\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Define colors for each group\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "\n",
    "    for i, (group_name, channels) in enumerate(signal_channels.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        for j, channel in enumerate(channels):\n",
    "            if channel in df_pandas.columns:\n",
    "                # Use different line styles for channels within the same group\n",
    "                linestyle = '-' if j == 0 else '--'\n",
    "                plt.plot(df_pandas[time_col], df_pandas[channel],\n",
    "                        label=f'{group_name}: {channel}',\n",
    "                        color=color, linestyle=linestyle, alpha=0.8)\n",
    "            else:\n",
    "                print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "    plt.title(f'All Channels - {patient_name}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Format x-axis for better readability\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "    plt.setp(plt.gca().xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "aad925004e927aa8",
   "metadata": {},
   "source": [
    "#### Load and visualize the downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "id": "64b8733ae3accf99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.315221Z",
     "start_time": "2025-08-09T17:02:10.310825Z"
    }
   },
   "source": [
    "# Example patient name for Insulin Clamp subjects:\n",
    "patient = \"Insulin Clamp #1\"  # Example patient name for Insulin Clamp subjects\n",
    "# Patient = \"Normal #1\"  # Example patient name for Normal subjects"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "53c59a7311ac9e5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.401568Z",
     "start_time": "2025-08-09T17:02:10.325283Z"
    }
   },
   "source": [
    "# Load the data\n",
    "df_loaded = load_downsampled_data(patient, signals_dir)\n",
    "\n",
    "if df_loaded is not None:\n",
    "    print(f\"Available columns: {df_loaded.columns}\")\n",
    "\n",
    "    # Check if time column exists and handle accordingly\n",
    "    time_col = None\n",
    "    for col in df_loaded.columns:\n",
    "        if 'time' in col.lower():\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is not None:\n",
    "        print(f\"Data time range: {df_loaded[time_col].min()} to {df_loaded[time_col].max()}\")\n",
    "        print(f\"Duration: {(df_loaded[time_col].max() - df_loaded[time_col].min()).total_seconds()/3600:.2f} hours\")\n",
    "    else:\n",
    "        print(\"No time column found in the data\")\n",
    "        print(\"Data shape:\", df_loaded.shape)\n",
    "        print(\"First few rows:\")\n",
    "        print(df_loaded.head())\n",
    "\n",
    "# Create visualizations if data is available\n",
    "if df_loaded is not None and Visualize_Signal:\n",
    "    print(\"Creating time series visualizations...\")\n",
    "\n",
    "    # Plot by channel groups (separate subplots)\n",
    "    plot_timeseries_by_channel(df_loaded, signal_channels, patient)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from Insulin_Clamp_1_downsampled_25hz.parquet\n",
      "Loaded data shape: (370600, 9)\n",
      "Available columns: ['Time', 'Head_left', 'Head_right', 'Hand1', 'Hand2', 'Liver1', 'Liver2', 'Background1', 'Background2']\n",
      "Data time range: 2025-01-23 08:36:51.884062 to 2025-01-23 12:43:55.844062\n",
      "Duration: 4.12 hours\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "30fa8711338bf191",
   "metadata": {},
   "source": [
    "#### Plot correlation between sub-channels (Head_left vs Head_right)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a199d3d574c2c15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.467455Z",
     "start_time": "2025-08-09T17:02:10.427998Z"
    }
   },
   "source": [
    "def plot_channel_correlation(df, signal_channels, patient_name, time_window_hours=None):\n",
    "    \"\"\"\n",
    "    Plot correlation between sub-channels within each group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with time series data\n",
    "    - signal_channels: dictionary of channel groups\n",
    "    - patient_name: name of the patient for plot title\n",
    "    - time_window_hours: optional time window in hours to limit the plot\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas for easier plotting\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Find the time column (case-insensitive)\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"No time column found for plotting\")\n",
    "        return\n",
    "\n",
    "    # Limit time window if specified\n",
    "    if time_window_hours is not None:\n",
    "        start_time = df_pandas[time_col].min()\n",
    "        end_time = start_time + pd.Timedelta(hours=time_window_hours)\n",
    "        df_pandas = df_pandas[df_pandas[time_col] <= end_time]\n",
    "        print(f\"Plotting first {time_window_hours} hours of data\")\n",
    "\n",
    "    # Create subplots for each channel group that has multiple channels\n",
    "    groups_with_pairs = {name: channels for name, channels in signal_channels.items() if len(channels) >= 2}\n",
    "\n",
    "    if not groups_with_pairs:\n",
    "        print(\"No channel groups with multiple channels found for correlation plotting\")\n",
    "        return\n",
    "\n",
    "    n_groups = len(groups_with_pairs)\n",
    "    fig, axes = plt.subplots(n_groups, 2, figsize=(15, 4*n_groups))\n",
    "\n",
    "    if n_groups == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (group_name, channels) in enumerate(groups_with_pairs.items()):\n",
    "        # Take first two channels for correlation\n",
    "        ch1, ch2 = channels[0], channels[1]\n",
    "\n",
    "        if ch1 in df_pandas.columns and ch2 in df_pandas.columns:\n",
    "            # Get data and clean it\n",
    "            x_data = df_pandas[ch1].values\n",
    "            y_data = df_pandas[ch2].values\n",
    "\n",
    "            # Remove NaN and infinite values\n",
    "            valid_mask = np.isfinite(x_data) & np.isfinite(y_data)\n",
    "            x_clean = x_data[valid_mask]\n",
    "            y_clean = y_data[valid_mask]\n",
    "\n",
    "            if len(x_clean) == 0:\n",
    "                print(f\"Warning: No valid data points for {group_name} channels after cleaning\")\n",
    "                continue\n",
    "\n",
    "            # Time series correlation plot\n",
    "            ax1 = axes[i, 0]\n",
    "            ax1.plot(df_pandas[time_col], df_pandas[ch1], label=ch1, alpha=0.8, color='blue')\n",
    "            ax1.plot(df_pandas[time_col], df_pandas[ch2], label=ch2, alpha=0.8, color='red')\n",
    "            ax1.set_title(f'{group_name} Channels - {patient_name}')\n",
    "            ax1.set_xlabel('Time')\n",
    "            ax1.set_ylabel('Signal (nT)')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "            ax1.xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "            plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "            # Scatter plot for correlation\n",
    "            ax2 = axes[i, 1]\n",
    "            ax2.scatter(x_clean, y_clean, alpha=0.5, s=1)\n",
    "            ax2.set_xlabel(f'{ch1} (nT)')\n",
    "            ax2.set_ylabel(f'{ch2} (nT)')\n",
    "\n",
    "            # Calculate correlation coefficient (handle case with insufficient data)\n",
    "            if len(x_clean) > 1 and np.std(x_clean) > 0 and np.std(y_clean) > 0:\n",
    "                correlation = np.corrcoef(x_clean, y_clean)[0, 1]\n",
    "                ax2.set_title(f'{group_name} Correlation: r = {correlation:.3f}')\n",
    "\n",
    "                # Add trend line with error handling\n",
    "                try:\n",
    "                    if np.std(x_clean) > 1e-10:  # Check for sufficient variance\n",
    "                        z = np.polyfit(x_clean, y_clean, 1)\n",
    "                        p = np.poly1d(z)\n",
    "                        x_trend = np.linspace(np.min(x_clean), np.max(x_clean), 100)\n",
    "                        ax2.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "                except (np.linalg.LinAlgError, np.RankWarning):\n",
    "                    print(f\"Warning: Could not fit trend line for {group_name} channels\")\n",
    "            else:\n",
    "                ax2.set_title(f'{group_name} - Insufficient data for correlation')\n",
    "\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: Channels '{ch1}' or '{ch2}' not found in data\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_channels_detailed_correlation(df, channel1, channel2, patient_name, time_window_hours=None):\n",
    "    \"\"\"\n",
    "    Detailed correlation analysis between any two channels.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with time series data\n",
    "    - channel1: name of first channel\n",
    "    - channel2: name of second channel\n",
    "    - patient_name: name of the patient for plot title\n",
    "    - time_window_hours: optional time window in hours to limit the plot\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available for plotting\")\n",
    "        return\n",
    "\n",
    "    # Convert to pandas for easier plotting\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Find the time column (case-insensitive)\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"No time column found for plotting\")\n",
    "        return\n",
    "\n",
    "    # Check if both channels exist\n",
    "    if channel1 not in df_pandas.columns:\n",
    "        print(f\"Channel '{channel1}' not found in data. Available channels: {list(df_pandas.columns)}\")\n",
    "        return\n",
    "\n",
    "    if channel2 not in df_pandas.columns:\n",
    "        print(f\"Channel '{channel2}' not found in data. Available channels: {list(df_pandas.columns)}\")\n",
    "        return\n",
    "\n",
    "    # Limit time window if specified\n",
    "    if time_window_hours is not None:\n",
    "        start_time = df_pandas[time_col].min()\n",
    "        end_time = start_time + pd.Timedelta(hours=time_window_hours)\n",
    "        df_pandas = df_pandas[df_pandas[time_col] <= end_time]\n",
    "        print(f\"Plotting first {time_window_hours} hours of data\")\n",
    "\n",
    "    # Clean the data\n",
    "    x_data = df_pandas[channel1].values\n",
    "    y_data = df_pandas[channel2].values\n",
    "\n",
    "    # Remove NaN and infinite values\n",
    "    valid_mask = np.isfinite(x_data) & np.isfinite(y_data)\n",
    "\n",
    "    if np.sum(valid_mask) == 0:\n",
    "        print(f\"Error: No valid data points found for {channel1} and {channel2}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using {np.sum(valid_mask)} valid data points out of {len(x_data)} total points\")\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "    # 1. Time series overlay\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    ax1.plot(df_pandas[time_col], df_pandas[channel1], label=channel1, alpha=0.8, color='blue')\n",
    "    ax1.plot(df_pandas[time_col], df_pandas[channel2], label=channel2, alpha=0.8, color='red')\n",
    "    ax1.set_title(f'{channel1} vs {channel2} Time Series - {patient_name}')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Signal (nT)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # 2. Scatter plot with correlation\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    x_clean = x_data[valid_mask]\n",
    "    y_clean = y_data[valid_mask]\n",
    "\n",
    "    ax2.scatter(x_clean, y_clean, alpha=0.5, s=1)\n",
    "    ax2.set_xlabel(f'{channel1} (nT)')\n",
    "    ax2.set_ylabel(f'{channel2} (nT)')\n",
    "\n",
    "    # Calculate correlation with error handling\n",
    "    correlation = np.nan\n",
    "    if len(x_clean) > 1 and np.std(x_clean) > 0 and np.std(y_clean) > 0:\n",
    "        correlation = np.corrcoef(x_clean, y_clean)[0, 1]\n",
    "\n",
    "        # Add trend line with error handling\n",
    "        try:\n",
    "            if np.std(x_clean) > 1e-10:\n",
    "                z = np.polyfit(x_clean, y_clean, 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_trend = np.linspace(np.min(x_clean), np.max(x_clean), 100)\n",
    "                ax2.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "        except (np.linalg.LinAlgError, np.RankWarning):\n",
    "            print(f\"Warning: Could not fit trend line for {channel1} vs {channel2}\")\n",
    "\n",
    "    ax2.set_title(f'{channel1} vs {channel2} Correlation: r = {correlation:.3f}')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Difference signal\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    diff_signal = df_pandas[channel1] - df_pandas[channel2]\n",
    "    valid_diff = diff_signal.dropna()\n",
    "\n",
    "    ax3.plot(df_pandas[time_col], diff_signal, color='green', alpha=0.8)\n",
    "    ax3.set_title(f'Difference Signal ({channel1} - {channel2})')\n",
    "    ax3.set_xlabel('Time')\n",
    "    ax3.set_ylabel('Difference (nT)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # 4. Rolling correlation\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    window_size = max(10, len(df_pandas) // 50)  # Ensure minimum window size\n",
    "    try:\n",
    "        rolling_corr = df_pandas[channel1].rolling(window=window_size).corr(df_pandas[channel2])\n",
    "        ax4.plot(df_pandas[time_col], rolling_corr, color='purple', alpha=0.8)\n",
    "        ax4.set_title(f'Rolling Correlation (window={window_size})')\n",
    "    except Exception as e:\n",
    "        ax4.text(0.5, 0.5, f'Rolling correlation failed:\\n{str(e)}',\n",
    "                transform=ax4.transAxes, ha='center', va='center')\n",
    "        ax4.set_title('Rolling Correlation - Failed')\n",
    "\n",
    "    ax4.set_xlabel('Time')\n",
    "    ax4.set_ylabel('Correlation')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # 5. Histogram of differences\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    if len(valid_diff) > 0:\n",
    "        ax5.hist(valid_diff, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "        mean_diff = np.mean(valid_diff)\n",
    "        std_diff = np.std(valid_diff)\n",
    "\n",
    "        ax5.axvline(mean_diff, color='red', linestyle='--', label=f'Mean: {mean_diff:.3f}')\n",
    "        ax5.axvline(mean_diff + std_diff, color='orange', linestyle='--', alpha=0.7, label=f'+1σ: {mean_diff + std_diff:.3f}')\n",
    "        ax5.axvline(mean_diff - std_diff, color='orange', linestyle='--', alpha=0.7, label=f'-1σ: {mean_diff - std_diff:.3f}')\n",
    "        ax5.legend()\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'No valid difference data', transform=ax5.transAxes, ha='center', va='center')\n",
    "\n",
    "    ax5.set_title('Distribution of Differences')\n",
    "    ax5.set_xlabel('Difference (nT)')\n",
    "    ax5.set_ylabel('Frequency')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Cross-correlation\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    try:\n",
    "        # Downsample for cross-correlation to avoid memory issues\n",
    "        downsample_factor = max(1, len(x_clean) // 1000)\n",
    "        x1 = x_clean[::downsample_factor]\n",
    "        x2 = y_clean[::downsample_factor]\n",
    "\n",
    "        if len(x1) > 10 and np.std(x1) > 0 and np.std(x2) > 0:\n",
    "            cross_corr = np.correlate(x1 - np.mean(x1), x2 - np.mean(x2), mode='full')\n",
    "            cross_corr = cross_corr / (np.std(x1) * np.std(x2) * len(x1))\n",
    "\n",
    "            lags = np.arange(-len(x1) + 1, len(x1))\n",
    "            ax6.plot(lags, cross_corr, color='brown', alpha=0.8)\n",
    "            ax6.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "            ax6.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "            max_corr_idx = np.argmax(np.abs(cross_corr))\n",
    "            max_corr_lag = lags[max_corr_idx]\n",
    "            max_corr_val = cross_corr[max_corr_idx]\n",
    "        else:\n",
    "            ax6.text(0.5, 0.5, 'Insufficient data for cross-correlation',\n",
    "                    transform=ax6.transAxes, ha='center', va='center')\n",
    "            max_corr_lag = 0\n",
    "            max_corr_val = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        ax6.text(0.5, 0.5, f'Cross-correlation failed:\\n{str(e)}',\n",
    "                transform=ax6.transAxes, ha='center', va='center')\n",
    "        max_corr_lag = 0\n",
    "        max_corr_val = 0\n",
    "\n",
    "    ax6.set_title('Cross-Correlation')\n",
    "    ax6.set_xlabel('Lag')\n",
    "    ax6.set_ylabel('Cross-Correlation')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"CHANNEL CORRELATION ANALYSIS - {patient_name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Channels analyzed: {channel1} vs {channel2}\")\n",
    "    print(f\"Valid data points: {np.sum(valid_mask)}/{len(x_data)} ({100*np.sum(valid_mask)/len(x_data):.1f}%)\")\n",
    "    print(f\"Correlation coefficient: {correlation:.4f}\")\n",
    "    if len(valid_diff) > 0:\n",
    "        print(f\"Mean difference: {np.mean(valid_diff):.4f} nT\")\n",
    "        print(f\"Std deviation of difference: {np.std(valid_diff):.4f} nT\")\n",
    "    try:\n",
    "        print(f\"Max cross-correlation: {max_corr_val:.4f} at lag {max_corr_lag}\")\n",
    "    except:\n",
    "        print(\"Cross-correlation analysis failed\")\n",
    "    print(\"=\"*60)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "ff319ded8ee39dd3",
   "metadata": {},
   "source": [
    "#### Run the analysis and visualizations for channel correlations"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd367a98053850a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.483294Z",
     "start_time": "2025-08-09T17:02:10.477575Z"
    }
   },
   "source": [
    "if df_loaded is None:\n",
    "    print(\"No data loaded. Please check the previous steps.\")\n",
    "elif Visualize_Signal:\n",
    "    print(\"Data loaded successfully. Proceeding with visualizations...\")\n",
    "\n",
    "    # Plot correlation between sub-channels\n",
    "    plot_channel_correlation(df_loaded, signal_channels, patient)\n",
    "\n",
    "    # Detailed correlation analysis for specific channels\n",
    "    # Head channels\n",
    "    plot_channels_detailed_correlation(df_loaded, \"Head_left\", \"Head_right\", patient)\n",
    "\n",
    "    # Hand channels\n",
    "    plot_channels_detailed_correlation(df_loaded, \"Hand1\", \"Hand2\", patient)\n",
    "\n",
    "    # Cross-location comparison\n",
    "    plot_channels_detailed_correlation(df_loaded, \"Liver1\", \"Liver2\", patient)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "a9ec255e6fd82de2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T17:02:10.581066Z",
     "start_time": "2025-08-09T17:02:10.491886Z"
    }
   },
   "source": [
    "gc.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project-venv)",
   "language": "python",
   "name": "project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
