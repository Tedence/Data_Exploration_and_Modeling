{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Biomarkers Notebook (LDA for state transition)",
   "id": "dd13200e6e14c19b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Frequency Band Energies - Long List\n",
    "\n",
    "#### Individual Sub-bands\n",
    "\n",
    "| Band         | Frequency Range | Metabolic Significance                                                      |\n",
    "|--------------|-----------------|-----------------------------------------------------------------------------|\n",
    "| **Delta1-1** | 0.1-0.3 Hz      | -> Very slow metabolic oscillations; cellular homeostasis *Delta1* Sub-Band |\n",
    "| **Delta1-2** | 0.3-0.6 Hz      | -> Very slow metabolic oscillations; cellular homeostasis *Delta1* Sub-Band |\n",
    "| **Delta1-3** | 0.6-1 Hz        | -> Very slow metabolic oscillations; cellular homeostasis *Delta1* Sub-Band |\n",
    "| **Delta2**   | 1-2 Hz          | Slow metabolic transitions; baseline cellular processes                     |\n",
    "| **Delta3**   | 2-3 Hz          | Low-frequency metabolic activity; energy storage processes                  |\n",
    "| **Delta4**   | 3-4 Hz          | Transitional metabolic rhythms                                              |\n",
    "| **Theta1**   | 4-5 Hz          | Slow rhythmic activity; early mitochondrial responses                       |\n",
    "| **Theta2**   | 5-6 Hz          | Low-medium frequency metabolic patterns                                     |\n",
    "| **Theta3**   | 6-7 Hz          | Medium rhythmic metabolic activity                                          |\n",
    "| **Theta4**   | 7-8 Hz          | Upper theta band; transition to faster metabolic activity                   |\n",
    "| **Alpha1**   | 8-10 Hz         | Lower alpha; medium-speed enzymatic cycles                                  |\n",
    "| **Alpha2**   | 10-13 Hz        | Upper alpha; faster organized metabolic patterns                            |\n",
    "| **Beta1**    | 13-20 Hz        | Lower beta; increased metabolic demand responses                            |\n",
    "| **Beta2**    | 20-25 Hz        | Mid-beta; rapid metabolic signaling                                         |\n",
    "| **Beta3**    | 25-30 Hz        | Upper beta; high-frequency metabolic regulation                             |\n",
    "| **Gamma1**   | 30-35 Hz        | Lower gamma; very fast synchronized cellular activity                       |\n",
    "| **Gamma2**   | 35-40 Hz        | Mid-gamma; rapid energy utilization patterns                                |\n",
    "| **Gamma3**   | 40-50 Hz        | High gamma; ultrafast metabolic oscillations                                |\n",
    "| **Gamma4**   | 50-60 Hz        | Very high gamma; extreme metabolic activity                                 |\n",
    "| **EMG_High** | 50-100 Hz       | High EMG; extreme metabolic signaling                                       |"
   ],
   "id": "34f8f67cfce1f88e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Objective\n",
    "\n",
    "We wish to investigate the presence of potential biomarkers in EMF signal, features that could be indicative of specific physiological events in diabetic patients.\n",
    "- Insulin administration\n",
    "- Ensure intake (glucose supplement)\n",
    "- Hypoglycemic episodes\n",
    "- Hyperglycemic episodes\n",
    "\n",
    "##### Scope of Analysis:\n",
    "- The study will be conducted on a per-patient basis (Patients #1 to #10) due to anticipated individual variability in signal response patterns.\n",
    "- Event types will be grouped by their expected signal dynamics:\n",
    "\n",
    "**Ensure/Insulin:** Expected to induce relatively short-term, rapid signal changes, suitable for anomaly detection or transient feature tracking.\n",
    "**Hypo/Hyper:** Expected to relate to longer-term changes, warranting analysis over broader time windows.\n",
    "\n",
    "##### Methodology:\n",
    "Event Alignment: We will analyze the temporal evolution of features around annotated events (Insulin, Ensure) and CGM-defined thresholds (Hypo/Hyper).\n",
    "\n",
    "##### Statistical Comparison:\n",
    "\n",
    "- We will compare multi-domain feature distributions and summary statistics (mean, variance, entropy, etc.) before vs. after events.\n",
    "- We will use appropriate paired statistical tests depending on feature behavior and sample size.\n",
    "\n",
    "##### Visualizations (TBD):\n",
    "\n",
    "- Event-centered Feature trajectories aligned to events.\n",
    "- Before-vs-After BoxPlots\n",
    "- Statistical Tables: p-values, effect sizes, CI, etc...\n",
    "- Embedding Projections: Event-related feature clustering in low-dimension space (LDA)\n",
    "### Objective\n",
    "\n",
    "We wish to investigate the presence of potential biomarkers in EMF signal, features that could be indicative of specific physiological events in diabetic patients.\n",
    "- Insulin administration\n",
    "- Ensure intake (glucose supplement)\n",
    "- Hypoglycemic episodes\n",
    "- Hyperglycemic episodes\n",
    "\n",
    "##### Scope of Analysis:\n",
    "- The study will be conducted on a per-patient basis (Patients #1 to #10) due to anticipated individual variability in signal response patterns.\n",
    "- Event types will be grouped by their expected signal dynamics:\n",
    "\n",
    "**Ensure/Insulin:** Expected to induce relatively short-term, rapid signal changes, suitable for anomaly detection or transient feature tracking.\n",
    "**Hypo/Hyper:** Expected to relate to longer-term changes, warranting analysis over broader time windows.\n",
    "\n",
    "##### Methodology:\n",
    "Event Alignment: We will analyze the temporal evolution of features around annotated events (Insulin, Ensure) and CGM-defined thresholds (Hypo/Hyper).\n",
    "\n",
    "##### Statistical Comparison:\n",
    "\n",
    "- We will compare multi-domain feature distributions and summary statistics (mean, variance, entropy, etc.) before vs. after events.\n",
    "- We will use appropriate paired statistical tests depending on feature behavior and sample size.\n",
    "\n",
    "##### Visualizations (TBD):\n",
    "\n",
    "- Event-centered Feature trajectories aligned to events.\n",
    "- Before-vs-After BoxPlots\n",
    "- Statistical Tables: p-values, effect sizes, CI, etc...\n",
    "- Embedding Projections: Event-related feature clustering in low-dimension space (LDA)\n",
    "\n",
    "##### Expected Outcome:\n",
    "- Identification of event-specific biomarkers from EMF signals, at least at the patient-level\n",
    "- Hopefully - insights into patient-specific vs. common patterns of physiological signal response.\n"
   ],
   "id": "8dd62c2ec017f36a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Additional task (subtask) for method robustness validation\n",
    "1. Perform the before-and-after analysis on different time-scale (original 60min/20min -> reduced windows of 30-15-5min for glycemic, 10-5-2.5min for metabolic transitions)\n",
    "2. Aside to the Wilcoxon signed-rank test, paired t-test, Cohen’s d (Effect size), CI, Mean difference and LDA, consider additional evaluation metrics, more suitable to reveal features transient dynamics (Detect/Count outlier responses? Anomaly Detection? time-weighted Cliff’s delta?)"
   ],
   "id": "355c52d1e3d64b85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Possible approach\n",
    "\n",
    "##### Data Preparation:\n",
    "- Input: Precomputed filtered and sub-sampled to 250Hz EMF signal per patient and event logs (Insulin, Ensure, Hypo, Hyper).\n",
    "- Synchronization: Align signal time series with event timestamps (±20 minutes for Ensure/Insulin, ±60+ minutes for Hypo/Hyper).\n",
    "\n",
    "##### Labeling:\n",
    "\n",
    "- Before and After window chunking for each event.\n",
    "- Optionally annotate baseline windows (far from events) for additional contrast (TBD if enough signal timing).\n",
    "\n",
    "##### Feature Aggregation\n",
    "Extract Features\n",
    "For each feature set (e.g., relative spectral band power, entropy, Hjorth parameters, wavelet energy, etc):\n",
    "- Compute summary statistics in the corresponding \"before\" and \"after\" event windows:\n",
    "- Normalize features per patient (e.g., z-score within patient scale) to make corresponding trends comparable.\n",
    "\n",
    "##### Statistical Analysis\n",
    "- Paired tests (Wilcoxon signed-rank or t-test):\n",
    "- Compare pre vs post-event windows.\n",
    "- Effect size estimation (Cohen’s d) to assess strength of signal change.\n",
    "- Bootstrap CI to check robustness of observed effects.\n",
    "\n",
    "##### Time-Resolved Feature Evolution dynamics\n",
    "- Compute rolling feature averages (e.g., 60s moving window) aligned to events.\n",
    "- Visualize feature dynamics as event-centered heatmaps:\n",
    "- Rows = events per patient\n",
    "- Columns = time before/after event, for different events\n",
    "- Cluster rows to reveal subtypes or repeatability patterns.\n",
    "\n",
    "##### Dimensionality Reduction\n",
    "Use LDA on feature vector groups to:\n",
    "- Visualize event-related feature distributions\n",
    "- Visualize box-plots for \"before\" and \"after\" subgroups\n",
    "- Detect outlier responses (TBD)\n",
    "- Highlight features contributing most to separability and visualize with bar-plots\n",
    "\n",
    "##### Clustering Analysis (TBD on all patient data)\n",
    "Clustering analysis across all 10 patients:\n",
    "We cut-up the data for each of the 10 patients into time segments around the events of interest (insulin, ensure, hypo, hyper) and see if some of the time segments from different patients corresponding to a particular event type (e.g. ensure) cluster together.\n",
    "Introducing \"patient\" meta-dimension and do pseudo-2D clustering?\n",
    "\n",
    "##### Personalization Layer\n",
    "- Train per-patient models to classify event occurrence from features.\n",
    "- Extract model coefficients or feature importance scores to define personalized biomarker sets.\n",
    "- Compare across patients for recurring features or sensor locations.\n",
    "\n",
    "##### Deliverables & Visualization\n",
    "- Feature Trajectories: Time-evolving and patient-aligned\n",
    "- Before-vs-After Box-Plots: Key features per event type\n",
    "- Statistical Tables: p-values, effect sizes, CI\n",
    "- Embedding Projections: Event clustering in LDA space with appropriate distributions \"before\" and \"after\" the event"
   ],
   "id": "3f463a649cbc12e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### General imports",
   "id": "c5122b49edcdf758"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# General imports:\n",
    "\n",
    "# Disable warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from scipy.signal import welch, hilbert\n",
    "\n",
    "\n",
    "from scipy.stats import wilcoxon, ttest_rel, skew, kurtosis, t\n",
    "\n",
    "import pywt  # For wavelet\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # For LDA\n",
    "from sklearn.metrics import classification_report, confusion_matrix  # For evaluation\n",
    "from sklearn.preprocessing import RobustScaler  # For robust feature scaling\n",
    "from datetime import datetime\n",
    "\n",
    "# Plotting enhancements\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ],
   "id": "ad0b889802bc7aa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Base variables and physical constants:",
   "id": "8e1b605ef13ba109"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Key Settings:\n",
    "\n",
    "# Patient Signal filename\n",
    "patient = \"Insulin Clamp #10\"\n",
    "#patient = \"Normal #7\"\n",
    "\n",
    "# Metabolic and Glycemic extraction windows:\n",
    "metabolic_window = 20 # min\n",
    "glycemic_window = 60 # min\n",
    "\n",
    "# Allow or Prevent the potential event overlapping:\n",
    "overlap_events = True\n",
    "\n",
    "# Physical constants and sensors specifications:\n",
    "SENSITIVITY = 50  # mV/nT\n",
    "MAGNETIC_NOISE = 3  # pT/√Hz @ 1 Hz\n",
    "MAX_AC_LINEARITY = 250  # nT (+/- 250 nT) - Equivalent to 21.78 V\n",
    "MAX_DC_LINEARITY = 60  # nT (+/- 60 nT)\n",
    "VOLTAGE_LIMIT = 15 # V (+/-15V)\n",
    "CONVERSION_FACTOR = 20  # nT per 1V\n",
    "\n",
    "# Path and directories\n",
    "base_dir = Path(\"../../../Data\")\n",
    "\n",
    "# Output directory for saving results\n",
    "output_dir = base_dir / \"ProcessedData\"\n",
    "signals_dir = output_dir / \"Processed Signals\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Labels directory\n",
    "labels_dir = base_dir / \"RawData\"\n",
    "labels_filename = \"FilteredLabels.xlsx\"\n"
   ],
   "id": "9762c437c8a51a63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature extraction constants:\n",
    "\n",
    "# Define constants\n",
    "SAMPLING_RATE = 250  # Hz for downsampled signal\n",
    "top_k = 10 # Default top-k features\n",
    "\n",
    "# Potentially will introduce specific window sizes for glycemic/metabolic events\n",
    "WINDOW_SIZE_MIN = 5  # minutes\n",
    "WINDOW_SIZE_SAMPLES = SAMPLING_RATE * 60 * WINDOW_SIZE_MIN\n",
    "OVERLAP = 50  # [%] Overlap between windows (might be changed later)\n",
    "\n",
    "# Will be changed according to the power spectrum analysis\n",
    "FREQUENCY_BANDS = {\n",
    "    # Individual sub-bands\n",
    "    #'delta1_1': (0.1, 0.3), # resolution issues\n",
    "    #'delta1_2': (0.3, 0.6), # resolution issues\n",
    "    'delta1_3': (0.6, 1.0),\n",
    "    'delta2': (1.0, 2.0),\n",
    "    'delta3': (2.0, 3.0),\n",
    "    'delta4': (3.0, 4.0),\n",
    "    'theta1': (4.0, 5.0),\n",
    "    'theta2': (5.0, 6.0),\n",
    "    'theta3': (6.0, 7.0),\n",
    "    'theta4': (7.0, 8.0),\n",
    "    'alpha1': (8.0, 10.0),\n",
    "    'alpha2': (10.0, 13.0),\n",
    "    'beta1': (13.0, 20.0),\n",
    "    'beta2': (20.0, 25.0),\n",
    "    'beta3': (25.0, 30.0),\n",
    "    'gamma1': (30.0, 35.0),\n",
    "    'gamma2': (35.0, 40.0)\n",
    "}"
   ],
   "id": "fcbb9ea50715874c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the CGM/stick Glucose target values from the file, add metabolic states and detect events",
   "id": "7134c2607ce0cd29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_metabolic_state_column(df_labels):\n",
    "    \"\"\"\n",
    "    Add a 'state' column to df_labels based on metabolic event timing.\n",
    "\n",
    "    States:\n",
    "    1. \"Fasting\" - From start until first insulin injection\n",
    "    2. \"First Insulin\" - From first insulin injection until ensure event\n",
    "    3. \"Ensure\" - From ensure event until second insulin injection\n",
    "    4. \"Second Insulin\" - From second insulin injection until end\n",
    "    \"\"\"\n",
    "    # Check if Events column exists\n",
    "    if \"Events\" not in df_labels.columns:\n",
    "        print(\"Warning: No Events column found - all entries will be labeled as 'Fasting'\")\n",
    "        df_labels['state'] = \"Fasting\"\n",
    "        return df_labels\n",
    "\n",
    "    # Sort by time to ensure chronological order\n",
    "    df_sorted = df_labels.sort_values(\"time\")\n",
    "\n",
    "    # Extract insulin and ensure events\n",
    "    insulin_events = df_sorted[df_sorted['Events'] == 'Insulin Injection']\n",
    "    ensure_events = df_sorted[df_sorted['Events'] == 'Ensure']\n",
    "\n",
    "    # Get transition timestamps\n",
    "    first_insulin_time = insulin_events['time'].iloc[0] if len(insulin_events) > 0 else None\n",
    "    first_ensure_time = ensure_events['time'].iloc[0] if len(ensure_events) > 0 else None\n",
    "\n",
    "    # Find second insulin (first one after ensure)\n",
    "    second_insulin_time = None\n",
    "    if first_ensure_time is not None and len(insulin_events) > 1:\n",
    "        post_ensure_insulin = insulin_events[insulin_events['time'] > first_ensure_time]\n",
    "        if len(post_ensure_insulin) > 0:\n",
    "            second_insulin_time = post_ensure_insulin['time'].iloc[0]\n",
    "\n",
    "    # Determine state for each timestamp\n",
    "    def get_state(timestamp):\n",
    "        if first_insulin_time is None or timestamp < first_insulin_time:\n",
    "            return \"Fasting\"\n",
    "        elif first_ensure_time is None or timestamp < first_ensure_time:\n",
    "            return \"First Insulin\"\n",
    "        elif second_insulin_time is None or timestamp < second_insulin_time:\n",
    "            return \"Ensure\"\n",
    "        else:\n",
    "            return \"Second Insulin\"\n",
    "\n",
    "    # Add state column using time from df_labels\n",
    "    df_labels['state'] = df_labels['time'].apply(get_state)\n",
    "\n",
    "    return df_labels"
   ],
   "id": "c12e53be6c351b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def detect_all_transitions(df_labels):\n",
    "    \"\"\"\n",
    "    Detect both metabolic state transitions and glycemic state transitions.\n",
    "\n",
    "    Glycemic states:\n",
    "    - Hypoglycemia: < 70 mg/dL\n",
    "    - Normal: 70-180 mg/dL\n",
    "    - Hyperglycemia: > 180 mg/dL\n",
    "    \"\"\"\n",
    "    # Initialize transitions dictionary\n",
    "    transitions = {}\n",
    "\n",
    "    # Ensure data is sorted by time\n",
    "    df_sorted = df_labels.sort_values(\"time\")\n",
    "\n",
    "    # Extract metabolic state transitions (already computed)\n",
    "    metabolic_states = [\"Fasting\", \"First Insulin\", \"Ensure\", \"Second Insulin\"]\n",
    "\n",
    "    # Find the first occurrence of each state\n",
    "    for state in metabolic_states:\n",
    "        state_rows = df_sorted[df_sorted['state'] == state]\n",
    "        if len(state_rows) > 0:\n",
    "            transitions[f\"Metabolic: {state}\"] = state_rows['time'].iloc[0]\n",
    "\n",
    "    # Filter out rows with null glucose values\n",
    "    df_glycemic = df_sorted.dropna(subset=['Glucose'])\n",
    "\n",
    "    # Add glycemic_state column\n",
    "    conditions = [\n",
    "        df_glycemic['Glucose'] < 70,\n",
    "        df_glycemic['Glucose'] <= 180\n",
    "    ]\n",
    "    choices = ['Hypoglycemia', 'Normal']\n",
    "    df_glycemic['glycemic_state'] = np.select(conditions, choices, default='Hyperglycemia')\n",
    "\n",
    "    # Record the initial glycemic state\n",
    "    if len(df_glycemic) > 0:\n",
    "        initial_state = df_glycemic['glycemic_state'].iloc[0]\n",
    "        initial_time = df_glycemic['time'].iloc[0]\n",
    "        transitions[f\"Initial Glycemic State: {initial_state}\"] = initial_time\n",
    "\n",
    "    # Detect transitions using a window-based approach\n",
    "    previous_state = None\n",
    "    for _, row in df_glycemic.iterrows():\n",
    "        current_state = row['glycemic_state']\n",
    "        current_time = row['time']\n",
    "\n",
    "        if previous_state is not None and current_state != previous_state:\n",
    "            transition_name = f\"Glycemic: {previous_state} to {current_state}\"\n",
    "            transitions[transition_name] = current_time\n",
    "\n",
    "        previous_state = current_state\n",
    "\n",
    "    return transitions"
   ],
   "id": "6c2f19b397d4db6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run the label loading and preprocessing",
   "id": "768b3f0f10729933"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load labels from file\n",
    "print(f\"Looking for tab: {patient}\")\n",
    "\n",
    "# Full path to Excel file\n",
    "labels_path = labels_dir / labels_filename\n",
    "print(f\"Reading labels from: {labels_path}\")\n",
    "\n",
    "# Read Excel file with pandas\n",
    "try:\n",
    "    # Read Excel file with pandas\n",
    "    df_labels = pd.read_excel(\n",
    "        labels_path,\n",
    "        sheet_name=patient,\n",
    "        usecols=[0, 1, 2, 3],  # Use usecols instead of columns\n",
    "        dtype={\n",
    "            \"Glucose\": float,\n",
    "            \"Events\": str,\n",
    "            \"Remarks\": str\n",
    "        },\n",
    "        parse_dates=[\"time\"]\n",
    "    )\n",
    "\n",
    "    # Keep only the first 4 columns (pandas way)\n",
    "    if len(df_labels.columns) > 4:\n",
    "        df_labels = df_labels.iloc[:, :4]\n",
    "\n",
    "    # Adding metabolic state column\n",
    "    df_labels = add_metabolic_state_column(df_labels)\n",
    "\n",
    "    print(f\"Successfully loaded {df_labels.shape[0]} labels with {df_labels.shape[1]} columns\")\n",
    "\n",
    "except Exception as e:\n",
    "    df_labels = None\n",
    "    print(f\"Error loading Excel file: {e}\")"
   ],
   "id": "10e109771f8560a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the preprocessed (filtered and cleaned) signal from parquet file",
   "id": "956983435ab7e346"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the data from tdms file into pandas dataframe:\n",
    "file_path = Path(signals_dir / str(patient+\"_processed_signals.parquet\"))\n",
    "# Load to dataframe\n",
    "\n",
    "try:\n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "\n",
    "    # Load the parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(f\"Successfully loaded dataframe from {file_path} with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading parquet file: {e}\")\n"
   ],
   "id": "ea092c1b67fd3a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Detect all transitions in the data\n",
    "if df_labels is not None:\n",
    "    transitions = detect_all_transitions(df_labels)\n",
    "\n",
    "    # Print all transitions in chronological order\n",
    "    print(\"All transitions in chronological order:\")\n",
    "    for name, time in sorted(transitions.items(), key=lambda x: x[1]):\n",
    "        print(f\"{time}: {name}\")"
   ],
   "id": "450a4287cb25f298",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Data Preparation**\n",
    "- Align signal time series with event timestamps (By default: ±20 minutes for Ensure/Insulin, ±30+ minutes for Hypo/Hyper transition)\n",
    "- Cut the signal windows around the events of interest and save them to a new dataframes for the next step"
   ],
   "id": "f6e1a7cfd5e66df2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_transition_windows(signal_df, transitions, metabolic_window_min=20, glycemic_window_min=30, allow_overlapping=True):\n",
    "    \"\"\"\n",
    "    Extract signal windows around key transitions.\n",
    "\n",
    "    Args:\n",
    "        signal_df: DataFrame with signal data containing 'Time' column\n",
    "        transitions: Dictionary of detected transitions with timestamps\n",
    "        metabolic_window_min: Minutes before/after metabolic transitions\n",
    "        glycemic_window_min: Minutes before/after glycemic transitions\n",
    "        allow_overlapping: If False, windows will be truncated to avoid overlap\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of DataFrames with transition name as key and signal window as value\n",
    "    \"\"\"\n",
    "    transition_windows = {}\n",
    "\n",
    "    # Ensure Time column is datetime type\n",
    "    if not pd.api.types.is_datetime64_any_dtype(signal_df['Time']):\n",
    "        signal_df['Time'] = pd.to_datetime(signal_df['Time'])\n",
    "\n",
    "    # Make a copy of the signal_df without Background column if it exists\n",
    "    if 'Background' in signal_df.columns:\n",
    "        signal_df = signal_df.drop(columns=['Background'])\n",
    "\n",
    "    # Define transitions of interest\n",
    "    metabolic_transitions = [\"Metabolic: First Insulin\", \"Metabolic: Ensure\", \"Metabolic: Second Insulin\"]\n",
    "    glycemic_transitions = [trans for trans in transitions.keys()\n",
    "                           if trans.startswith(\"Glycemic:\") and\n",
    "                           (\"to Hyperglycemia\" in trans or \"to Hypoglycemia\" in trans or \"to Normal\" in trans)]\n",
    "\n",
    "    # Sort transitions by time for handling overlap\n",
    "    sorted_transitions = sorted(\n",
    "        [(name, time) for name, time in transitions.items()\n",
    "         if name in metabolic_transitions or name in glycemic_transitions],\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "\n",
    "    # Get data time bounds\n",
    "    min_time = signal_df['Time'].min()\n",
    "    max_time = signal_df['Time'].max()\n",
    "\n",
    "    # Process each transition\n",
    "    for i, (transition_name, transition_time) in enumerate(sorted_transitions):\n",
    "        # Skip if transition is outside of available data\n",
    "        if transition_time < min_time or transition_time > max_time:\n",
    "            print(f\"Skipping {transition_name}: transition time {transition_time} is outside available data range\")\n",
    "            continue\n",
    "\n",
    "        # Set window size based on transition type\n",
    "        window_min = metabolic_window_min if transition_name in metabolic_transitions else glycemic_window_min\n",
    "\n",
    "        # Calculate initial window boundaries\n",
    "        window_start = transition_time - pd.Timedelta(minutes=window_min)\n",
    "        window_end = transition_time + pd.Timedelta(minutes=window_min)\n",
    "\n",
    "        # Initial check if window extends beyond available data\n",
    "        has_before_data = window_start >= min_time\n",
    "        has_after_data = window_end <= max_time\n",
    "\n",
    "        if not (has_before_data and has_after_data):\n",
    "            print(f\"Initial window for {transition_name} extends beyond available data range\")\n",
    "            if not has_before_data:\n",
    "                window_start = min_time\n",
    "            if not has_after_data:\n",
    "                window_end = max_time\n",
    "\n",
    "        # If not allowing overlaps, adjust window boundaries\n",
    "        if not allow_overlapping:\n",
    "            # Check overlap with previous transition\n",
    "            if i > 0:\n",
    "                prev_name, prev_time = sorted_transitions[i-1]\n",
    "                prev_window_min = metabolic_window_min if prev_name in metabolic_transitions else glycemic_window_min\n",
    "                prev_window_end = prev_time + pd.Timedelta(minutes=prev_window_min)\n",
    "\n",
    "                # If current window starts before previous window ends, adjust\n",
    "                if window_start < prev_window_end:\n",
    "                    original_start = window_start\n",
    "                    window_start = prev_window_end\n",
    "                    print(f\"Truncated {transition_name} window start from {original_start} to {window_start} to avoid overlap with {prev_name}\")\n",
    "\n",
    "            # Check overlap with next transition\n",
    "            if i < len(sorted_transitions) - 1:\n",
    "                next_name, next_time = sorted_transitions[i+1]\n",
    "                next_window_min = metabolic_window_min if next_name in metabolic_transitions else glycemic_window_min\n",
    "                next_window_start = next_time - pd.Timedelta(minutes=next_window_min)\n",
    "\n",
    "                # If current window ends after next window starts, adjust\n",
    "                if window_end > next_window_start:\n",
    "                    original_end = window_end\n",
    "                    window_end = next_window_start\n",
    "                    print(f\"Truncated {transition_name} window end from {original_end} to {window_end} to avoid overlap with {next_name}\")\n",
    "\n",
    "        # Extract signal within window\n",
    "        window_data = signal_df[(signal_df['Time'] >= window_start) &\n",
    "                              (signal_df['Time'] <= window_end)].copy()\n",
    "\n",
    "        # Check if after all adjustments, we still have data points before AND after the transition\n",
    "        data_before = window_data[window_data['Time'] < transition_time]\n",
    "        data_after = window_data[window_data['Time'] > transition_time]\n",
    "\n",
    "        if len(data_before) == 0:\n",
    "            print(f\"Skipping {transition_name}: no data points before transition after truncation\")\n",
    "            continue\n",
    "\n",
    "        if len(data_after) == 0:\n",
    "            print(f\"Skipping {transition_name}: no data points after transition after truncation\")\n",
    "            continue\n",
    "\n",
    "        # Add reference columns\n",
    "        window_data['transition_name'] = transition_name\n",
    "        window_data['transition_time'] = transition_time\n",
    "        window_data['relative_time_min'] = (window_data['Time'] - transition_time).dt.total_seconds() / 60\n",
    "\n",
    "        # Store in dictionary\n",
    "        if not window_data.empty:\n",
    "            transition_windows[transition_name] = window_data\n",
    "            print(f\"\\n->Extracted window for {transition_name}: {len(window_data)} samples, [{window_start} to {window_end}]\")\n",
    "            print(f\"  - Data points before: {len(data_before)}, after: {len(data_after)}\")\n",
    "        else:\n",
    "            print(f\"No data available for {transition_name} window\")\n",
    "\n",
    "    return transition_windows"
   ],
   "id": "f308c48a7968cd83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_all_transition_windows(transition_windows, df_glucose=None, highlight_events=True, output_dir=output_dir, patient_id=patient):\n",
    "    \"\"\"\n",
    "    Plot all transition windows with all available signal columns and synchronized glucose events.\n",
    "\n",
    "    Args:\n",
    "        transition_windows: Dictionary with transition names as keys and signal dataframes as values\n",
    "        df_glucose: DataFrame containing glucose values and events data\n",
    "        highlight_events: Whether to highlight events with markers (default: True)\n",
    "    \"\"\"\n",
    "    # Create output directory if specified\n",
    "    if output_dir:\n",
    "        figures_dir = output_dir / f\"{patient_id}_biomarkers\" / \"Signals\"\n",
    "        figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Saving figures to: {figures_dir}\")\n",
    "\n",
    "    # Get signal columns (excluding metadata columns)\n",
    "    if not transition_windows:\n",
    "        print(\"No transition windows available to plot\")\n",
    "        return\n",
    "\n",
    "    sample_window = next(iter(transition_windows.values()))\n",
    "    signal_cols = [col for col in sample_window.columns\n",
    "                  if col not in ['Time', 'transition_name', 'transition_time', 'relative_time_min']]\n",
    "\n",
    "    # Plot each transition window\n",
    "    for transition_name, window_data in transition_windows.items():\n",
    "        # Create a figure with subplots for each signal column\n",
    "        fig, axes = plt.subplots(len(signal_cols), 1, figsize=(14, 3*len(signal_cols)), sharex=True)\n",
    "        fig.suptitle(f'EMF Signals Around {transition_name}', fontsize=16)\n",
    "\n",
    "        # Convert axes to array if only one subplot\n",
    "        axes = np.atleast_1d(axes)\n",
    "\n",
    "        # Get time range for this transition\n",
    "        start_time = window_data['Time'].min()\n",
    "        end_time = window_data['Time'].max()\n",
    "        transition_time = window_data['transition_time'].iloc[0]\n",
    "\n",
    "        # Plot each signal column\n",
    "        for i, col in enumerate(signal_cols):\n",
    "            ax = axes[i]\n",
    "            ax.plot(window_data['relative_time_min'], window_data[col], label=col)\n",
    "            ax.axvline(x=0, color='r', linestyle='--', label='Transition')\n",
    "            ax.set_ylabel(f'{col} EMF Signal')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "            # Add glucose data if provided\n",
    "            if df_glucose is not None:\n",
    "                # Create secondary y-axis for glucose\n",
    "                ax2 = ax.twinx()\n",
    "\n",
    "                # Filter glucose data to the time range of this window\n",
    "                mask_glucose = (df_glucose['time'] >= start_time) & (df_glucose['time'] <= end_time)\n",
    "                relevant_glucose = df_glucose[mask_glucose].copy() if mask_glucose.any() else None\n",
    "\n",
    "                if relevant_glucose is not None and len(relevant_glucose) > 1:\n",
    "                    # Convert glucose times to relative minutes for x-axis alignment\n",
    "                    relevant_glucose['relative_min'] = [(t - transition_time).total_seconds() / 60\n",
    "                                                       for t in relevant_glucose['time']]\n",
    "\n",
    "                    # Sort by time\n",
    "                    relevant_glucose = relevant_glucose.sort_values('relative_min')\n",
    "                    times = relevant_glucose['relative_min']\n",
    "                    glucose = relevant_glucose['Glucose']\n",
    "\n",
    "                    # Plot glucose line\n",
    "                    ax2.plot(times, glucose, 'k--', marker='o', linewidth=1, alpha=0.7, label='Glucose')\n",
    "                    ax2.set_ylabel('Glucose (mg/dL)')\n",
    "                    ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "                    # Add threshold lines\n",
    "                    ax2.axhline(y=70, color='blue', linestyle=':', alpha=0.5, label='Hypo Threshold (70)')\n",
    "                    ax2.axhline(y=180, color='red', linestyle=':', alpha=0.5, label='Hyper Threshold (180)')\n",
    "\n",
    "                    # Color-code regions based on glucose levels\n",
    "                    for j in range(len(times) - 1):\n",
    "                        start = times.iloc[j]\n",
    "                        end = times.iloc[j+1]\n",
    "                        g_value = glucose.iloc[j]\n",
    "\n",
    "                        if g_value < 70:  # Hypoglycemia\n",
    "                            color = 'blue'\n",
    "                            label = 'Hypoglycemia (<70)' if j == 0 or glucose.iloc[j-1] >= 70 else None\n",
    "                        elif g_value > 180:  # Hyperglycemia\n",
    "                            color = 'red'\n",
    "                            label = 'Hyperglycemia (>180)' if j == 0 or glucose.iloc[j-1] <= 180 else None\n",
    "                        else:  # Normal\n",
    "                            color = 'green'\n",
    "                            label = 'Normal (70-180)' if j == 0 or (glucose.iloc[j-1] < 70 or glucose.iloc[j-1] > 180) else None\n",
    "\n",
    "                        ax.axvspan(start, end, alpha=0.2, color=color, label=label)\n",
    "\n",
    "                    # Add event markers if requested\n",
    "                    if highlight_events:\n",
    "                        y_min, y_max = ax.get_ylim()\n",
    "                        y_position = y_min + 0.1 * (y_max - y_min)  # Position markers 10% up from bottom\n",
    "\n",
    "                        # Add Ensure (sugar) markers\n",
    "                        ensure_events = relevant_glucose[relevant_glucose['Events'] == 'Ensure']\n",
    "                        if len(ensure_events) > 0:\n",
    "                            ensure_times = ensure_events['relative_min']\n",
    "                            ax.scatter(ensure_times, [y_position] * len(ensure_times),\n",
    "                                    marker='x', color='purple', s=100, label='Ensure (Sugar)', zorder=10)\n",
    "\n",
    "                            # Add vertical lines\n",
    "                            for t in ensure_times:\n",
    "                                ax.axvline(x=t, color='purple', linestyle='--', alpha=0.3)\n",
    "\n",
    "                        # Add Insulin markers\n",
    "                        insulin_events = relevant_glucose[relevant_glucose['Events'] == 'Insulin Injection']\n",
    "                        if len(insulin_events) > 0:\n",
    "                            insulin_times = insulin_events['relative_min']\n",
    "                            ax.scatter(insulin_times, [y_position] * len(insulin_times),\n",
    "                                    marker='^', color='orange', s=100, label='Insulin', zorder=10)\n",
    "\n",
    "                            # Add vertical lines\n",
    "                            for t in insulin_times:\n",
    "                                ax.axvline(x=t, color='orange', linestyle='--', alpha=0.3)\n",
    "\n",
    "            # Combine legends from both axes\n",
    "            handles1, labels1 = ax.get_legend_handles_labels()\n",
    "            if 'ax2' in locals():\n",
    "                handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "                handles = handles1 + handles2\n",
    "                labels = labels1 + labels2\n",
    "            else:\n",
    "                handles, labels = handles1, labels1\n",
    "\n",
    "            # Remove duplicates while preserving order\n",
    "            seen = set()\n",
    "            unique_handles, unique_labels = [], []\n",
    "            for h, l in zip(handles, labels):\n",
    "                if l not in seen:\n",
    "                    unique_handles.append(h)\n",
    "                    unique_labels.append(l)\n",
    "                    seen.add(l)\n",
    "\n",
    "            ax.legend(unique_handles, unique_labels, loc='upper right', fontsize=8)\n",
    "\n",
    "        # Set common x-axis label\n",
    "        axes[-1].set_xlabel('Time relative to transition (minutes)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.92)\n",
    "\n",
    "        # Save figure if output directory is specified\n",
    "        if output_dir:\n",
    "            # Clean up transition name for filename\n",
    "            safe_transition_name = transition_name.replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "            filename = f\"{patient_id}_transition_{safe_transition_name}.png\"\n",
    "            filepath = figures_dir / filename\n",
    "            plt.savefig(filepath, dpi=100, bbox_inches='tight')\n",
    "\n",
    "        plt.show()"
   ],
   "id": "24c1be4093c4dc50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract windows around transitions with fixed metabolic and glycemic window sizes\n",
    "transition_windows = extract_transition_windows(df, transitions, allow_overlapping=overlap_events, metabolic_window_min=metabolic_window, glycemic_window_min=glycemic_window)\n",
    "\n",
    "# Plot all transition windows\n",
    "plot_all_transition_windows(transition_windows, df_glucose=df_labels,\n",
    "                           output_dir=output_dir, patient_id=patient)"
   ],
   "id": "5bbe600f57c095c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### \"Before\" and \"After\" Feature Extraction Pipeline:",
   "id": "d647a03a27e37cee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### This pipeline performs detailed characterization of signal biomarkers during metabolic and glycemic state changes\n",
    "\n",
    "**Features:**\n",
    "- Multi-domain feature extraction: Time-domain, frequency-domain, and wavelet-domain analysis\n",
    "- Statistical validation: Paired statistical tests with confidence intervals\n",
    "- Discriminative analysis: LDA-based feature importance ranking\n",
    "- Visualization: Generates multiple visualization types for understanding feature dynamics\n",
    "\n",
    "The function expects segmented signal data around transition events (before/after) in the form of a dictionary of DataFrames, where each DataFrame contains time series data with metadata about the transition type.\n",
    "\n",
    "**Feature Extraction**\n",
    "The code extracts a set of features from EMF signals:\n",
    "\n",
    "**Time-domain features**:\n",
    "\n",
    "- Basic statistics (mean, standard deviation, skewness, kurtosis)\n",
    "- Zero-crossing rate\n",
    "- Hjorth parameters (activity, mobility, complexity)\n",
    "- Shannon entropy\n",
    "\n",
    "**Frequency-domain features**:\n",
    "\n",
    "- Relative band power across defined frequency bands\n",
    "- Spectral properties (PSD) via Welch's method\n",
    "\n",
    "**Wavelet-domain features**:\n",
    "\n",
    "- Energy in wavelet subbands using discrete wavelet transform (DWT)\n",
    "- Customizable frequency limit (default: 40Hz)\n",
    "- Adaptable wavelet basis (default: 'db4')\n",
    "\n",
    "**Statistical Analysis**\n",
    "For each feature, the code performs:\n",
    "\n",
    "- Paired statistical testing (Wilcoxon signed-rank test, paired t-test)\n",
    "- Effect size calculation (Cohen's d)\n",
    "- Confidence interval estimation\n",
    "- Significance determination (p < 0.05)\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)**:\n",
    "\n",
    "- Projects feature space to maximize separability between \"before\"/\"after\" states\n",
    "- Identifies most discriminative features\n",
    "- Visualizes class separation via LDA projection plots: Density distribution of \"before\"/\"after\" samples in LDA space\n",
    "- Visualizes Confusion Matrices for classification evaluation of state separation\n",
    "- Evaluates classification performance\n",
    "- Reports feature importance rankings\n",
    "\n",
    "**The pipeline generates several additional visualization types**:\n",
    "\n",
    "- Feature trajectory matrices (smooth line plots of feature values over time, aligned to transition events)\n",
    "- Boxplot matrices - \"before\" vs. \"after\" comparison for each feature\n",
    "- Feature importance bar charts\n",
    "\n",
    "**The pipeline identifies discriminative features through**:\n",
    "\n",
    "- Cohen's d effect size ranking (statistical approach)\n",
    "- LDA coefficient magnitude (machine learning approach)\n",
    "- Per-channel and overall rankings\n",
    "- Detailed summary report and tables"
   ],
   "id": "16dac6e68df9912a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Score Calculation in 'analyze_transition_dynamics':\n",
    "\n",
    "different scores based on the transition type:\n",
    "\n",
    "### Metabolic Score (considered as \"sharp\" transitions)\n",
    "- **Components:**\n",
    "  - **Amplitude Shift**: Absolute mean difference between pre/post values, normalized by pre-standard deviation\n",
    "  - **Peak Ratio**: Ratio of maximum deviation after transition to maximum deviation before\n",
    "\n",
    "- **Formula:**\n",
    "  ```\n",
    "  metabolic_score = min(3.0, amplitude_shift/pre_std) * 0.5 + min(3.0, peak_ratio) * 0.5\n",
    "  ```\n",
    "\n",
    "### Glycemic Score (considered as \"gradual\" transitions, since it is a virtual margin to cross)\n",
    "- **Components:**\n",
    "  - **Trend Continuation** (40%): Binary score (1.0/0.0) if slope direction continues\n",
    "  - **Trend Acceleration** (15%): Ratio of post-slope to pre-slope magnitude (capped at 2.0)\n",
    "  - **Smoothness** (30%): Inverse of roughness ratio between post/pre data\n",
    "\n",
    "- **Formula:**\n",
    "  ```\n",
    "  glycemic_score = trend_continuation * 0.4 + min(2.0, trend_acceleration) * 0.3/2.0 + smoothness * 0.3\n",
    "  ```\n",
    "\n",
    "Both scores use small constants (1e-10) to prevent division by zero and include caps to prevent outliers from dominating the score.\n",
    "\n",
    "The values 3.0 and 2.0 are suggested \"capping constants\" that prevent extreme outliers from disproportionately affecting the scores while accommodating expected transition characteristics:\n",
    "- The amplitude shift normalized by pre-standard deviation could legitimately reach 2-3 sigma for significant metabolic events and the 3.0 limit allows detection of strong responses without letting extreme values dominate.\n",
    "- Trend acceleration (ratio of slopes) is expected to be more moderate in glycemic transitions and 2.0 cap prevents scoring bias toward unusually steep slope changes\n",
    "\n",
    "Different cap values could be used in theory based on the specific characteristics of the data and transition types.\n",
    "\n",
    "The optimal caps could be determined through empirical validation against known events, possibly using ROC analysis to balance sensitivity and specificity in biomarker detection."
   ],
   "id": "e77abe9e3da32ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_transition_biomarkers(transition_windows, window_size_sec=60, fs=250, FREQUENCY_BANDS=FREQUENCY_BANDS,\n",
    "                                 output_dir=None, patient_id=\"unknown\"):\n",
    "    \"\"\"\n",
    "    Analyze EMF signal biomarkers before and after transitions.\n",
    "\n",
    "    Args:\n",
    "        transition_windows: Dictionary with transition windows\n",
    "        window_size_sec: Size of analysis window in seconds (default: 60s)\n",
    "        fs: Sampling frequency (default: 250Hz)\n",
    "        output_dir: Directory to save output files (default: None)\n",
    "        patient_id: Patient identifier for output files (default: \"unknown\")\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing results for each transition, plus top features dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directory if specified\n",
    "    if output_dir:\n",
    "        output_dir = Path(output_dir) / (patient_id+\"_biomarkers\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Saving results to: {output_dir}\")\n",
    "\n",
    "    def analyze_transition_dynamics(df, transition_type, time_col='relative_time_min', value_col='value'):\n",
    "        \"\"\"\n",
    "        Analyze feature dynamics across transition event based on transition type.\n",
    "        Simple analysis for either glycemic (gradual) or metabolic (sharp) transitions.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with time and value columns\n",
    "            transition_type: 'Glycemic' or 'Metabolic'\n",
    "            time_col: Column name for time values\n",
    "            value_col: Column name for feature values\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of transition dynamics metrics\n",
    "        \"\"\"\n",
    "        if df is None or df.empty or len(df) < 10:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Extract pre and post event data\n",
    "            pre_data = df[df[time_col] < 0]\n",
    "            post_data = df[df[time_col] > 0]\n",
    "\n",
    "            if len(pre_data) < 5 or len(post_data) < 5:\n",
    "                return None\n",
    "\n",
    "            pre_times = pre_data[time_col].values\n",
    "            pre_values = pre_data[value_col].values\n",
    "            post_times = post_data[time_col].values\n",
    "            post_values = post_data[value_col].values\n",
    "\n",
    "            # Basic statistics\n",
    "            pre_mean = np.mean(pre_values)\n",
    "            post_mean = np.mean(post_values)\n",
    "            mean_shift = post_mean - pre_mean\n",
    "\n",
    "            # Calculate trends (slopes)\n",
    "            from scipy import stats\n",
    "            pre_slope, _, _, _, _ = stats.linregress(pre_times, pre_values)\n",
    "            post_slope, _, _, _, _ = stats.linregress(post_times, post_values)\n",
    "            slope_change = post_slope - pre_slope\n",
    "\n",
    "            # For metabolic transitions: detect sharp changes\n",
    "            if 'Metabolic' in transition_type:\n",
    "                # 1. Detect amplitude shift after transition\n",
    "                amplitude_shift = abs(mean_shift)\n",
    "\n",
    "                # 2. Detect peak amplitude changes\n",
    "                pre_peak = max(abs(pre_values - pre_mean))\n",
    "                post_peak = max(abs(post_values - post_mean))\n",
    "                peak_ratio = post_peak / (pre_peak + 1e-10)\n",
    "\n",
    "                # 3. Detect response delay - time to first significant deviation\n",
    "                # (defined as >1.5 std dev from pre-mean)\n",
    "                response_delay = np.inf\n",
    "                pre_std = np.std(pre_values)\n",
    "                threshold = 1.5 * pre_std\n",
    "                for i, val in enumerate(post_values):\n",
    "                    if abs(val - pre_mean) > threshold:\n",
    "                        response_delay = post_times[i]\n",
    "                        break\n",
    "\n",
    "                # If no response detected, set to infinity\n",
    "                if response_delay == np.inf:\n",
    "                    response_delay = np.nan\n",
    "\n",
    "                # Combine into a single metabolic score\n",
    "                metabolic_score = (\n",
    "                    min(3.0, amplitude_shift / (pre_std + 1e-10)) * 0.5 +  # Normalized amplitude shift\n",
    "                    min(3.0, peak_ratio) * 0.5                             # Peak ratio (capped)\n",
    "                )\n",
    "\n",
    "                return {\n",
    "                    'is_glycemic': False,\n",
    "                    'is_metabolic': True,\n",
    "                    'mean_shift': mean_shift,\n",
    "                    'amplitude_shift': amplitude_shift,\n",
    "                    'peak_ratio': peak_ratio,\n",
    "                    'response_delay': response_delay,\n",
    "                    'metabolic_score': metabolic_score,\n",
    "                    'score': metabolic_score\n",
    "                }\n",
    "\n",
    "            # For glycemic transitions: detect gradual trends\n",
    "            else:\n",
    "                # 1. Calculate trend consistency\n",
    "                # (how well the slope continues/accelerates)\n",
    "                trend_continuation = 1.0 if (\n",
    "                    (pre_slope > 0 and post_slope > 0) or\n",
    "                    (pre_slope < 0 and post_slope < 0)\n",
    "                ) else 0.0\n",
    "\n",
    "                # 2. Calculate trend acceleration\n",
    "                trend_acceleration = abs(post_slope) / (abs(pre_slope) + 1e-10)\n",
    "\n",
    "                # 3. Calculate smoothness (lack of abrupt changes)\n",
    "                # Use mean of squared second derivative\n",
    "                post_diff2 = np.diff(np.diff(post_values))\n",
    "                pre_diff2 = np.diff(np.diff(pre_values))\n",
    "                post_roughness = np.mean(post_diff2**2) if len(post_diff2) > 0 else 0\n",
    "                pre_roughness = np.mean(pre_diff2**2) if len(pre_diff2) > 0 else 0\n",
    "                smoothness = 1.0 - min(1.0, post_roughness / (pre_roughness + 1e-10))\n",
    "\n",
    "                # Combine into glycemic score\n",
    "                glycemic_score = (\n",
    "                    trend_continuation * 0.4 +\n",
    "                    min(2.0, trend_acceleration) * 0.3 / 2.0 +\n",
    "                    smoothness * 0.3\n",
    "                )\n",
    "\n",
    "                return {\n",
    "                    'is_glycemic': True,\n",
    "                    'is_metabolic': False,\n",
    "                    'trend_continuation': trend_continuation,\n",
    "                    'trend_acceleration': trend_acceleration,\n",
    "                    'smoothness': smoothness,\n",
    "                    'mean_shift': mean_shift,\n",
    "                    'glycemic_score': glycemic_score,\n",
    "                    'score': glycemic_score\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Transition dynamics analysis failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_wavelet_features(signal, fs, freq_limit=40, wavelet='db4', level=None):\n",
    "        \"\"\"Extract wavelet energy features for frequency bands below the limit.\"\"\"\n",
    "        # Auto-determine decomposition level if not provided\n",
    "        if level is None:\n",
    "            level = pywt.dwt_max_level(len(signal), pywt.Wavelet(wavelet).dec_len)\n",
    "\n",
    "        # Decompose signal using DWT\n",
    "        coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "\n",
    "        # Calculate energy in each subband\n",
    "        band_energies = {}\n",
    "        n = len(signal)  # For normalization\n",
    "\n",
    "        # Skip approximation coefficients (first element) and use only details\n",
    "        for i, c in enumerate(coeffs[1:], 1):\n",
    "            # Each wavelet level corresponds to a frequency band\n",
    "            # The band is approximately fs/(2^(i+1)) to fs/(2^i) Hz\n",
    "            f_high = fs/(2**i)\n",
    "            f_low = fs/(2**(i+1))\n",
    "\n",
    "            # Only include bands below the frequency limit\n",
    "            if f_high < freq_limit:\n",
    "                band_name = f'wavelet_band_{f_low:.2f}-{f_high:.2f}Hz'\n",
    "                band_energies[band_name] = np.sum(c**2)/n  # Normalized energy\n",
    "\n",
    "        return band_energies\n",
    "\n",
    "    def mean_ci(data, conf=0.95):\n",
    "        \"\"\"Calculate mean and confidence interval for an array.\"\"\"\n",
    "        data = np.array(data)\n",
    "        n = len(data)\n",
    "        if n == 0:\n",
    "            return np.nan, (np.nan, np.nan)\n",
    "\n",
    "        m = np.mean(data)\n",
    "        se = np.std(data, ddof=1) / np.sqrt(n)\n",
    "\n",
    "        # t-distribution for confidence interval (small samples)\n",
    "        if n > 1:\n",
    "            h = se * t.ppf((1 + conf) / 2., n-1)\n",
    "            return m, (m-h, m+h)\n",
    "        else:\n",
    "            return m, (np.nan, np.nan)\n",
    "\n",
    "    def plot_dynamics_matrix(heatmap_data, dynamics_results, top_features_per_signal=top_k, output_dir=None):\n",
    "        \"\"\"\n",
    "        Plot feature dynamics in a matrix format with features as rows and events as columns.\n",
    "\n",
    "        Args:\n",
    "            heatmap_data: Dictionary containing feature trajectories\n",
    "            dynamics_results: Dictionary containing dynamic analysis results\n",
    "            top_features_per_signal: Number of top features to display per signal\n",
    "            output_dir: Directory to save output files (optional)\n",
    "        \"\"\"\n",
    "        # Group events by transition type (Metabolic vs Glycemic)\n",
    "        metabolic_events = []\n",
    "        glycemic_events = []\n",
    "\n",
    "        for transition_name in heatmap_data.keys():\n",
    "            if \"Metabolic\" in transition_name:\n",
    "                metabolic_events.append(transition_name)\n",
    "            else:\n",
    "                glycemic_events.append(transition_name)\n",
    "\n",
    "        # Process each signal type\n",
    "        for signal_name in set(sig for _, sig, _ in dynamics_results.keys()):\n",
    "            print(f\"\\nCreating dynamics matrix for signal: {signal_name}\")\n",
    "\n",
    "            # Get top features for this signal based on dynamics scores\n",
    "            signal_features = {}\n",
    "            for key, dynamics in dynamics_results.items():\n",
    "                trans_name, sig, feat = key\n",
    "                if sig == signal_name and 'score' in dynamics:\n",
    "                    if feat not in signal_features:\n",
    "                        signal_features[feat] = dynamics['score']\n",
    "                    else:\n",
    "                        signal_features[feat] = max(signal_features[feat], dynamics['score'])\n",
    "\n",
    "            # Sort features by score and take top N\n",
    "            top_features = sorted(signal_features.items(), key=lambda x: -x[1])[:top_features_per_signal]\n",
    "            feature_names = [f for f, _ in top_features]\n",
    "\n",
    "            # Create separate matrices for metabolic and glycemic events\n",
    "            for event_type, events in [(\"Metabolic\", metabolic_events), (\"Glycemic\", glycemic_events)]:\n",
    "                if not events:\n",
    "                    continue\n",
    "\n",
    "                n_features = len(feature_names)\n",
    "                n_events = len(events)\n",
    "\n",
    "                if n_features == 0 or n_events == 0:\n",
    "                    print(f\"  No data to plot for {event_type} events\")\n",
    "                    continue\n",
    "\n",
    "                # Create figure and axes grid\n",
    "                fig, axes = plt.subplots(n_features, n_events,\n",
    "                                        figsize=(4*n_events, 3*n_features),\n",
    "                                        sharex='col', sharey='row')\n",
    "\n",
    "                # Handle single row or column case\n",
    "                if n_features == 1 and n_events == 1:\n",
    "                    axes = np.array([[axes]])\n",
    "                elif n_features == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                elif n_events == 1:\n",
    "                    axes = axes.reshape(-1, 1)\n",
    "\n",
    "                # Plot each feature-event combination\n",
    "                for i, feature in enumerate(feature_names):\n",
    "                    for j, event in enumerate(events):\n",
    "                        ax = axes[i, j]\n",
    "\n",
    "                        # Check if we have data for this combination\n",
    "                        if (event in heatmap_data and\n",
    "                            signal_name in heatmap_data[event] and\n",
    "                            feature in heatmap_data[event][signal_name]):\n",
    "\n",
    "                            # Get trajectory data\n",
    "                            df = heatmap_data[event][signal_name][feature]\n",
    "                            df = df.dropna(subset=['relative_time_min', 'value'])\n",
    "\n",
    "                            if len(df) > 5:  # Need enough points to plot\n",
    "                                times = df['relative_time_min'].values\n",
    "                                values = df['value'].values\n",
    "\n",
    "                                # Plot trajectory\n",
    "                                ax.plot(times, values, 'k-', alpha=0.5)\n",
    "\n",
    "                                # Try to add smoothed line\n",
    "                                try:\n",
    "                                    from scipy.signal import savgol_filter\n",
    "                                    if len(values) > 7:\n",
    "                                        window_len = min(7, len(values) - (len(values) % 2) - 1)\n",
    "                                        if window_len > 2:\n",
    "                                            smooth = savgol_filter(values, window_len, 3)\n",
    "                                            ax.plot(times, smooth, 'b-', lw=2)\n",
    "                                except:\n",
    "                                    pass  # Skip smoothing if it fails\n",
    "\n",
    "                                # Mark transition point\n",
    "                                ax.axvline(x=0, color='r', linestyle='-')\n",
    "\n",
    "                                # Add shading to distinguish pre/post\n",
    "                                ax.axvspan(min(times), 0, color='gray', alpha=0.1)\n",
    "\n",
    "                                # Get dynamics info if available\n",
    "                                key = (event, signal_name, feature)\n",
    "                                if key in dynamics_results:\n",
    "                                    dynamics = dynamics_results[key]\n",
    "\n",
    "                                    # For metabolic transitions\n",
    "                                    if \"Metabolic\" in event:\n",
    "                                        delay = dynamics.get('response_delay', np.nan)\n",
    "                                        if not np.isnan(delay):\n",
    "                                            # Mark response delay\n",
    "                                            ax.axvline(x=delay, color='purple', linestyle='--')\n",
    "                                            # Add score\n",
    "                                            score = dynamics.get('score', 0)\n",
    "                                            if score > 0.5:  # Only show good scores\n",
    "                                                ax.text(0.05, 0.95, f\"{score:.2f}\",\n",
    "                                                      transform=ax.transAxes, color='red',\n",
    "                                                      fontweight='bold', va='top', ha='left')\n",
    "\n",
    "                                    # For glycemic transitions\n",
    "                                    else:\n",
    "                                        # Show trend lines\n",
    "                                        pre_data = df[df['relative_time_min'] < 0]\n",
    "                                        post_data = df[df['relative_time_min'] > 0]\n",
    "\n",
    "                                        if len(pre_data) >= 2 and len(post_data) >= 2:\n",
    "                                            from scipy import stats\n",
    "                                            pre_times = pre_data['relative_time_min'].values\n",
    "                                            pre_values = pre_data['value'].values\n",
    "                                            post_times = post_data['relative_time_min'].values\n",
    "                                            post_values = post_data['value'].values\n",
    "\n",
    "                                            # Pre trend\n",
    "                                            pre_slope, pre_intercept, _, _, _ = stats.linregress(pre_times, pre_values)\n",
    "                                            pre_trend = pre_slope * pre_times + pre_intercept\n",
    "                                            ax.plot(pre_times, pre_trend, 'g--', lw=1.5)\n",
    "\n",
    "                                            # Post trend\n",
    "                                            post_slope, post_intercept, _, _, _ = stats.linregress(post_times, post_values)\n",
    "                                            post_trend = post_slope * post_times + post_intercept\n",
    "                                            ax.plot(post_times, post_trend, 'b--', lw=1.5)\n",
    "\n",
    "                                            # Add score\n",
    "                                            score = dynamics.get('score', 0)\n",
    "                                            if score > 0.5:\n",
    "                                                ax.text(0.05, 0.95, f\"{score:.2f}\",\n",
    "                                                      transform=ax.transAxes, color='green',\n",
    "                                                      fontweight='bold', va='top', ha='left')\n",
    "\n",
    "                                # Add grid\n",
    "                                ax.grid(True, alpha=0.3)\n",
    "                            else:\n",
    "                                ax.text(0.5, 0.5, \"Insufficient data\",\n",
    "                                      ha='center', va='center', transform=ax.transAxes)\n",
    "                                ax.axis('off')\n",
    "                        else:\n",
    "                            ax.text(0.5, 0.5, \"No data\",\n",
    "                                   ha='center', va='center', transform=ax.transAxes)\n",
    "                            ax.axis('off')\n",
    "\n",
    "                        # Add labels\n",
    "                        if i == 0:  # Only top row gets event titles\n",
    "                            ax.set_title(event.replace('Metabolic: ', '').replace('Glycemic: ', ''),\n",
    "                                        fontsize=10)\n",
    "                        if j == 0:  # Only first column gets feature labels\n",
    "                            ax.set_ylabel(feature, fontsize=9)\n",
    "                        if i == n_features - 1:  # Only bottom row gets x labels\n",
    "                            ax.set_xlabel('Time (min)')\n",
    "\n",
    "                plt.suptitle(f'{event_type} Dynamics Matrix for {signal_name}', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(top=0.95)\n",
    "\n",
    "                # Save figure if output directory is specified\n",
    "                if output_dir:\n",
    "                    dynamics_dir = output_dir / \"Dynamics\"\n",
    "                    dynamics_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    filename = f\"dynamics_matrix_{event_type.lower()}_{signal_name}.png\"\n",
    "                    plt.savefig(dynamics_dir / filename, dpi=120, bbox_inches='tight')\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "    def plot_feature_trajectories_matrix(heatmap_data, dynamics_results=None):\n",
    "        \"\"\"\n",
    "        Create a matrix of feature trajectory plots organized by feature and transition.\n",
    "        \"\"\"\n",
    "        # Get all available features and transitions\n",
    "        all_transitions = list(heatmap_data.keys())\n",
    "        all_signals = []\n",
    "        all_features = []\n",
    "\n",
    "        # Collect all signals and features\n",
    "        for tran in heatmap_data:\n",
    "            for sig in heatmap_data[tran]:\n",
    "                if sig not in all_signals:\n",
    "                    all_signals.append(sig)\n",
    "                for feat in heatmap_data[tran][sig]:\n",
    "                    if feat not in all_features:\n",
    "                        all_features.append(feat)\n",
    "\n",
    "        # Prioritize frequency band features\n",
    "        feature_order = [f for f in all_features if f.startswith('rel_power_')]\n",
    "        feature_order += [f for f in all_features if f.startswith('wavelet_band_')]  # Added wavelet features\n",
    "        feature_order += [f for f in all_features if not (f.startswith('rel_power_') or f.startswith('wavelet_band_')) and f in [\n",
    "            'mean', 'std', 'skew', 'kurtosis', 'zero_crossings', 'hjorth_activity',\n",
    "            'hjorth_mobility', 'hjorth_complexity', 'shannon_entropy']]\n",
    "\n",
    "        # Try to import LOWESS for smoothing\n",
    "        try:\n",
    "            from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "            have_lowess = True\n",
    "        except ImportError:\n",
    "            have_lowess = False\n",
    "\n",
    "        # Create a matrix for each signal\n",
    "        for sig in all_signals:\n",
    "            # Get features that exist for this signal\n",
    "            sig_features = []\n",
    "            for tran in all_transitions:\n",
    "                if tran in heatmap_data and sig in heatmap_data[tran]:\n",
    "                    sig_features.extend(list(heatmap_data[tran][sig].keys()))\n",
    "            sig_features = list(set(sig_features))\n",
    "\n",
    "            # Order features consistently\n",
    "            features = [f for f in feature_order if f in sig_features]\n",
    "            transition_order = [\"Metabolic: First Insulin\", \"Metabolic: Ensure\", \"Metabolic: Second Insulin\"] + [\n",
    "                t for t in all_transitions if t.startswith(\"Glycemic:\")\n",
    "            ]\n",
    "            transitions = [t for t in transition_order if t in all_transitions]\n",
    "\n",
    "            n_feat = len(features)\n",
    "            n_tran = len(transitions)\n",
    "\n",
    "            if n_feat == 0 or n_tran == 0:\n",
    "                print(f\"No data to plot for signal: {sig}\")\n",
    "                continue\n",
    "\n",
    "            # Create figure and axes\n",
    "            fig, axes = plt.subplots(n_feat, n_tran, figsize=(4*n_tran, 3*n_feat), sharex='col')\n",
    "            fig.suptitle(f'Feature Trajectories Matrix for Signal: {sig}', fontsize=16)\n",
    "\n",
    "            # Handle case of single feature or transition\n",
    "            if n_feat == 1 and n_tran == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif n_feat == 1 or n_tran == 1:\n",
    "                axes = axes.reshape((n_feat, n_tran))\n",
    "\n",
    "            # Plot each feature trajectory\n",
    "            for i, feat in enumerate(features):\n",
    "                for j, tran in enumerate(transitions):\n",
    "                    ax = axes[i, j]\n",
    "\n",
    "                    # Check if data exists for this combination\n",
    "                    if (tran in heatmap_data and\n",
    "                        sig in heatmap_data[tran] and\n",
    "                        feat in heatmap_data[tran][sig]):\n",
    "\n",
    "                        df_feat = heatmap_data[tran][sig][feat]\n",
    "                        df_feat = df_feat.dropna(subset=['relative_time_min', 'value'])\n",
    "\n",
    "                        if len(df_feat) > 2:  # Need at least 3 points\n",
    "                            times = df_feat['relative_time_min'].values\n",
    "                            values = df_feat['value'].values\n",
    "\n",
    "                            # Apply smoothing if possible\n",
    "                            if have_lowess:\n",
    "                                try:\n",
    "                                    smooth = lowess(values, times, frac=0.2, return_sorted=True)\n",
    "                                    ax.plot(smooth[:, 0], smooth[:, 1], color='blue', lw=2)\n",
    "                                except:\n",
    "                                    ax.plot(times, values, color='black', lw=1.5)\n",
    "                            else:\n",
    "                                ax.plot(times, values, color='black', lw=1.5)\n",
    "\n",
    "                            # Add dynamics indicators if available\n",
    "                            if dynamics_results and (tran, sig, feat) in dynamics_results:\n",
    "                                dynamics = dynamics_results[(tran, sig, feat)]\n",
    "\n",
    "                                # For metabolic transitions, mark response delay if detected\n",
    "                                if dynamics.get('is_metabolic', False):\n",
    "                                    delay = dynamics.get('response_delay', None)\n",
    "                                    if delay is not None and not np.isinf(delay) and not np.isnan(delay):\n",
    "                                        ax.axvline(x=delay, color='purple', linestyle='--', lw=1, alpha=0.7)\n",
    "\n",
    "                                        # Position text more carefully to avoid overlap\n",
    "                                        y_range = ax.get_ylim()\n",
    "                                        y_pos = y_range[0] + (y_range[1] - y_range[0]) * 0.85\n",
    "                                        ax.text(delay + 0.2, y_pos, f\"{delay:.1f}m\",\n",
    "                                               color='purple', fontsize=8, ha='left', va='center')\n",
    "\n",
    "                                # Add marker indicating the score\n",
    "                                score = dynamics.get('score', 0)\n",
    "                                if score > 0.5:  # Only show for good scores\n",
    "                                    y_range = ax.get_ylim()\n",
    "                                    y_span = y_range[1] - y_range[0]\n",
    "                                    # Use a fixed position relative to the plot range for consistent placement\n",
    "                                    y_pos = y_range[0] + y_span * (0.2 if dynamics.get('mean_shift', 0) < 0 else 0.8)\n",
    "                                    ax.text(times[-1]*0.8, y_pos, f\"{score:.1f}\",\n",
    "                                           color='red', fontweight='bold', ha='right', va='center')\n",
    "\n",
    "                            # Add transition line\n",
    "                            ax.axvline(x=0, color='r', linestyle='-', linewidth=1)\n",
    "\n",
    "                            # Add grid\n",
    "                            ax.grid(True, alpha=0.3)\n",
    "                        else:\n",
    "                            ax.text(0.5, 0.5, \"Insufficient data\",\n",
    "                                    horizontalalignment='center',\n",
    "                                    verticalalignment='center',\n",
    "                                    transform=ax.transAxes)\n",
    "                            ax.axis('off')\n",
    "                    else:\n",
    "                        ax.text(0.5, 0.5, \"No data\",\n",
    "                                horizontalalignment='center',\n",
    "                                verticalalignment='center',\n",
    "                                transform=ax.transAxes)\n",
    "                        ax.axis('off')\n",
    "\n",
    "                    # Add labels\n",
    "                    if i == 0:  # Only top row gets transition titles\n",
    "                        ax.set_title(tran.replace('Metabolic: ', '').replace('Glycemic: ', ''), fontsize=10)\n",
    "                    if j == 0:  # Only first column gets feature labels\n",
    "                        ax.set_ylabel(feat, fontsize=9)\n",
    "                    if i == n_feat - 1:  # Only bottom row gets x labels\n",
    "                        ax.set_xlabel('Time (min)')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.95)\n",
    "\n",
    "            # Save figure if output directory is specified\n",
    "            if output_dir:\n",
    "                filename = f\"feature_trajectories_{sig}.png\"\n",
    "                plt.savefig(output_dir / filename, dpi=100, bbox_inches='tight')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def cohens_d(x, y):\n",
    "        nx, ny = len(x), len(y)\n",
    "        if nx == 0 or ny == 0:\n",
    "            return np.nan\n",
    "        mean_x, mean_y = np.mean(x), np.mean(y)\n",
    "        s1, s2 = np.var(x, ddof=1), np.var(y, ddof=1)\n",
    "        pooled_sd = np.sqrt(((nx - 1) * s1 + (ny - 1) * s2) / (nx + ny - 2))\n",
    "        if pooled_sd == 0:\n",
    "            return np.nan\n",
    "        return (mean_y - mean_x) / pooled_sd\n",
    "\n",
    "    def extract_features(signal):\n",
    "        \"\"\"Extract all features from a signal segment\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Basic statistics\n",
    "        features['mean'] = np.mean(signal)\n",
    "        features['std'] = np.std(signal)\n",
    "        features['skew'] = skew(signal)\n",
    "        features['kurtosis'] = kurtosis(signal)\n",
    "\n",
    "        # Zero crossings\n",
    "        features['zero_crossings'] = np.where(np.diff(np.signbit(signal - np.mean(signal))))[0].size\n",
    "\n",
    "        # Frequency domain features\n",
    "        nperseg = min(4 * fs, len(signal))\n",
    "        freqs, psd = welch(signal, fs=fs, nperseg=nperseg, noverlap=nperseg//2)\n",
    "        mask_0_10 = (freqs >= 0) & (freqs <= 10)\n",
    "        total_power = np.trapz(psd[mask_0_10], freqs[mask_0_10])\n",
    "\n",
    "        # Relative band powers\n",
    "        for band_name, (low_freq, high_freq) in FREQUENCY_BANDS.items():\n",
    "            band_mask = (freqs >= low_freq) & (freqs <= high_freq)\n",
    "            if np.any(band_mask):\n",
    "                band_power = np.trapz(psd[band_mask], freqs[band_mask])\n",
    "                features[f'rel_power_{band_name}'] = band_power / total_power if total_power > 0 else np.nan\n",
    "\n",
    "        # Wavelet features\n",
    "        features.update(extract_wavelet_features(signal, fs, freq_limit=40))\n",
    "\n",
    "        # Hjorth parameters\n",
    "        activity = np.var(signal)\n",
    "        features['hjorth_activity'] = activity\n",
    "\n",
    "        if activity > 0:\n",
    "            diff1 = np.diff(signal)\n",
    "            var_diff1 = np.var(diff1)\n",
    "            mobility = np.sqrt(var_diff1 / activity)\n",
    "            features['hjorth_mobility'] = mobility\n",
    "\n",
    "            if mobility > 0:\n",
    "                diff2 = np.diff(diff1)\n",
    "                var_diff2 = np.var(diff2)\n",
    "                features['hjorth_complexity'] = np.sqrt(var_diff2 / var_diff1) / mobility if var_diff1 > 0 else np.nan\n",
    "            else:\n",
    "                features['hjorth_complexity'] = np.nan\n",
    "        else:\n",
    "            features['hjorth_mobility'] = np.nan\n",
    "            features['hjorth_complexity'] = np.nan\n",
    "\n",
    "        # Shannon entropy\n",
    "        hist, _ = np.histogram(signal, bins=20)\n",
    "        prob = hist / np.sum(hist)\n",
    "        features['shannon_entropy'] = -np.sum(prob * np.log2(prob + 1e-10))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def extract_window_features(segment_data, column, is_before=True):\n",
    "        \"\"\"Extract features from time windows in a segment\"\"\"\n",
    "        features_list = []\n",
    "        window_size_min = window_size_sec / 60\n",
    "        min_time = segment_data['relative_time_min'].min()\n",
    "        max_time = segment_data['relative_time_min'].max()\n",
    "\n",
    "        window_starts = np.arange(min_time, max_time - window_size_min, window_size_min)\n",
    "\n",
    "        for start in window_starts:\n",
    "            end = start + window_size_min\n",
    "            window = segment_data[(segment_data['relative_time_min'] >= start) &\n",
    "                                  (segment_data['relative_time_min'] < end)]\n",
    "\n",
    "            if len(window) < fs:  # Skip windows with insufficient data\n",
    "                continue\n",
    "\n",
    "            signal = window[column].values\n",
    "            features = extract_features(signal)\n",
    "\n",
    "            # Add window metadata\n",
    "            features['window_start'] = start\n",
    "            features['window_end'] = end\n",
    "            features['is_before'] = is_before\n",
    "            features['window_center_time'] = window['Time'].iloc[len(window)//2] if 'Time' in window.columns and len(window) > 0 else np.nan\n",
    "            features['relative_time_center'] = window['relative_time_min'].mean() if len(window) > 0 else np.nan\n",
    "\n",
    "            features_list.append(features)\n",
    "\n",
    "        return features_list\n",
    "\n",
    "    def perform_lda_analysis(before_df, after_df, feature_cols, transition_name, signal_name):\n",
    "        \"\"\"Perform LDA analysis on before vs after transition data\"\"\"\n",
    "        print(f\"\\nLDA Analysis for {transition_name} - {signal_name}\")\n",
    "\n",
    "        # Prepare data for LDA\n",
    "        X_before = before_df[feature_cols].fillna(0)\n",
    "        X_after = after_df[feature_cols].fillna(0)\n",
    "\n",
    "        # Combine data and create labels\n",
    "        X_combined = pd.concat([X_before, X_after])\n",
    "        y_combined = np.array([0]*len(X_before) + [1]*len(X_after))  # 0 = before, 1 = after\n",
    "\n",
    "        # Handle case of insufficient data\n",
    "        if len(X_combined) < 10 or len(np.unique(y_combined)) < 2:\n",
    "            print(f\"  Insufficient data for LDA analysis\")\n",
    "            return None\n",
    "\n",
    "        # Scale features for better LDA performance\n",
    "        scaler = RobustScaler()\n",
    "        X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "        # Fit LDA model\n",
    "        lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "        X_lda = lda.fit_transform(X_scaled, y_combined)\n",
    "\n",
    "        # Calculate feature importances\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': np.abs(lda.coef_[0])\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        # LDA classification and evaluation\n",
    "        predictions = lda.predict(X_scaled)\n",
    "\n",
    "        # Create a figure with 2 subplots side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "        # 1. Density plot visualization\n",
    "        for label, name, color in zip([0, 1], ['Before', 'After'], ['#3498db', '#e74c3c']):\n",
    "            mask = y_combined == label\n",
    "            sns.kdeplot(X_lda[mask].flatten(), fill=True, color=color, alpha=0.6, label=name, ax=ax1)\n",
    "\n",
    "        ax1.set_title(f'LDA Projection - {transition_name} - {signal_name}', fontsize=14)\n",
    "        ax1.set_xlabel('Linear Discriminant 1', fontsize=12)\n",
    "        ax1.set_ylabel('Density', fontsize=12)\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax1.legend(title='Transition', fontsize=10)\n",
    "\n",
    "        # 2. Feature importance visualization\n",
    "        top_n = min(10, len(feature_importance))\n",
    "        top_features = feature_importance.head(top_n)\n",
    "\n",
    "        ax2.barh(range(len(top_features)), top_features['Importance'][::-1], color='#2980b9')\n",
    "        ax2.set_yticks(range(len(top_features)))\n",
    "        ax2.set_yticklabels(top_features['Feature'][::-1])\n",
    "        ax2.set_xlabel('LDA Coefficient Magnitude', fontsize=12)\n",
    "        ax2.set_title(f'Top {top_n} Features', fontsize=14)\n",
    "        ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save figure if output directory is specified\n",
    "        if output_dir:\n",
    "            lda_dir = output_dir / \"LDA\"\n",
    "            lda_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            filename = f\"lda_projection_{transition_name.replace(':', '_')}_{signal_name}.png\"\n",
    "            plt.savefig(lda_dir / filename, dpi=100, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Print classification results\n",
    "        cr = classification_report(y_combined, predictions, target_names=['Before', 'After'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(cr)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_combined, predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Before', 'After'],\n",
    "                   yticklabels=['Before', 'After'])\n",
    "        plt.title(f'Confusion Matrix - {transition_name} - {signal_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save confusion matrix if output directory is specified\n",
    "        if output_dir:\n",
    "            lda_dir = output_dir / \"LDA\"\n",
    "            lda_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            filename = f\"confusion_matrix_{transition_name.replace(':', '_')}_{signal_name}.png\"\n",
    "            plt.savefig(lda_dir / filename, dpi=100, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Print top features\n",
    "        print(\"\\nTop 10 discriminative features:\")\n",
    "        for i, (feat, importance) in enumerate(zip(\n",
    "            top_features['Feature'],\n",
    "            top_features['Importance']), 1):\n",
    "            print(f\"{i}. {feat}: {importance:.4f}\")\n",
    "\n",
    "        # Save report to file if output directory is specified\n",
    "        if output_dir:\n",
    "            lda_dir = output_dir / \"LDA\"\n",
    "            lda_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with open(lda_dir / f\"lda_report_{transition_name.replace(':', '_')}_{signal_name}.txt\", 'w') as f:\n",
    "                f.write(f\"LDA Analysis for {transition_name} - {signal_name}\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\\n\")\n",
    "                f.write(\"Classification Report:\\n\")\n",
    "                f.write(cr + \"\\n\\n\")\n",
    "                f.write(\"Top 10 discriminative features:\\n\")\n",
    "                for i, (feat, importance) in enumerate(zip(\n",
    "                    top_features['Feature'],\n",
    "                    top_features['Importance']), 1):\n",
    "                    f.write(f\"{i}. {feat}: {importance:.4f}\\n\")\n",
    "\n",
    "        return {\n",
    "            'lda_model': lda,\n",
    "            'feature_importance': feature_importance,\n",
    "            'X_lda': X_lda,\n",
    "            'y': y_combined,\n",
    "            'predictions': predictions,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "    # Initialize results\n",
    "    results = {}\n",
    "    matrix_boxplot_data = []\n",
    "    heatmap_data = {}\n",
    "    dynamics_results = {}  # For storing feature dynamics analysis results\n",
    "\n",
    "    # For storing feature importance across all transitions\n",
    "    feature_matrix_all = []\n",
    "    channel_feature_scores = {}\n",
    "    lda_feature_scores = {}\n",
    "    feature_dynamics_scores = {}  # For storing dynamics scores\n",
    "\n",
    "    # Process each transition\n",
    "    for transition_name, window_data in transition_windows.items():\n",
    "        print(f\"\\nAnalyzing {transition_name}...\")\n",
    "\n",
    "        # Split data into before and after segments\n",
    "        before_data = window_data[window_data['relative_time_min'] < 0]\n",
    "        after_data = window_data[window_data['relative_time_min'] > 0]\n",
    "\n",
    "        # Get signal columns (excluding metadata columns)\n",
    "        signal_cols = [col for col in window_data.columns\n",
    "                      if col not in ['Time', 'transition_name', 'transition_time', 'relative_time_min']]\n",
    "\n",
    "        trans_results = {\n",
    "            'before_features': {},\n",
    "            'after_features': {},\n",
    "            'statistics': {},\n",
    "            'rolling_features': {},\n",
    "            'lda_results': {},          # Store LDA results\n",
    "            'dynamics': {}              # Store dynamics analysis results\n",
    "        }\n",
    "\n",
    "        # Process each signal column\n",
    "        for col in signal_cols:\n",
    "            print(f\"  Processing {col}...\")\n",
    "\n",
    "            # Extract features from windows before and after transition\n",
    "            before_features = extract_window_features(before_data, col, is_before=True)\n",
    "            after_features = extract_window_features(after_data, col, is_before=False)\n",
    "\n",
    "            before_df = pd.DataFrame(before_features)\n",
    "            after_df = pd.DataFrame(after_features)\n",
    "\n",
    "            if before_df.empty or after_df.empty:\n",
    "                print(f\"  Warning: Not enough data for {col} in before or after segment\")\n",
    "                continue\n",
    "\n",
    "            # Select relevant feature columns (including wavelet features)\n",
    "            potential_feature_cols = [c for c in before_df.columns if (\n",
    "                c.startswith('rel_power_') or\n",
    "                c.startswith('wavelet_band_') or\n",
    "                c in [\n",
    "                    'mean', 'std', 'skew', 'kurtosis', 'zero_crossings', 'hjorth_activity',\n",
    "                    'hjorth_mobility', 'hjorth_complexity', 'shannon_entropy'\n",
    "                ]\n",
    "            )]\n",
    "\n",
    "            # Only use features that exist in both DataFrames\n",
    "            common_cols = set(before_df.columns).intersection(set(after_df.columns))\n",
    "            feature_cols = [c for c in potential_feature_cols if c in common_cols]\n",
    "\n",
    "            # Normalize features\n",
    "            combined = pd.concat([before_df[feature_cols], after_df[feature_cols]])\n",
    "            means = combined.mean()\n",
    "            stds = combined.std()\n",
    "\n",
    "            for df in [before_df, after_df]:\n",
    "                for feat in feature_cols:\n",
    "                    if stds[feat] > 0:\n",
    "                        df[f'{feat}_norm'] = (df[feat] - means[feat]) / stds[feat]\n",
    "                    else:\n",
    "                        df[f'{feat}_norm'] = 0\n",
    "\n",
    "            # Collect data for boxplot matrix\n",
    "            for feat in feature_cols:\n",
    "                norm_feat = f'{feat}_norm'\n",
    "                for label, vals in [('Before', before_df[norm_feat]), ('After', after_df[norm_feat])]:\n",
    "                    matrix_boxplot_data.append({\n",
    "                        'transition': transition_name,\n",
    "                        'signal': col,\n",
    "                        'feature': feat,\n",
    "                        'label': label,\n",
    "                        'values': vals.values.tolist()\n",
    "                    })\n",
    "\n",
    "            # Calculate statistics between before and after with CI\n",
    "            stats_results = {}\n",
    "            for feat in feature_cols:\n",
    "                norm_feat = f'{feat}_norm'\n",
    "                if norm_feat in before_df.columns and norm_feat in after_df.columns:\n",
    "                    before_vals = before_df[norm_feat].dropna()\n",
    "                    after_vals = after_df[norm_feat].dropna()\n",
    "\n",
    "                    min_len = min(len(before_vals), len(after_vals))\n",
    "                    if min_len >= 5:  # Only run stats with sufficient data\n",
    "                        before_vals = before_vals.iloc[:min_len]\n",
    "                        after_vals = after_vals.iloc[:min_len]\n",
    "\n",
    "                        try:\n",
    "                            w_stat, w_p = wilcoxon(before_vals, after_vals)\n",
    "                            stats_results[f'{feat}_wilcoxon_p'] = w_p\n",
    "                        except:\n",
    "                            stats_results[f'{feat}_wilcoxon_p'] = np.nan\n",
    "\n",
    "                        t_stat, t_p = ttest_rel(before_vals, after_vals)\n",
    "                        stats_results[f'{feat}_ttest_p'] = t_p\n",
    "\n",
    "                        d = cohens_d(before_vals, after_vals)\n",
    "                        stats_results[f'{feat}_cohens_d'] = d\n",
    "\n",
    "                        mean_diff = after_vals.mean() - before_vals.mean()\n",
    "                        stats_results[f'{feat}_mean_diff'] = mean_diff\n",
    "\n",
    "                        # Add confidence intervals\n",
    "                        stats_results[f'{feat}_before_mean'], stats_results[f'{feat}_before_CI'] = mean_ci(before_vals)\n",
    "                        stats_results[f'{feat}_after_mean'], stats_results[f'{feat}_after_CI'] = mean_ci(after_vals)\n",
    "\n",
    "                        is_significant = (w_p < 0.05) or (t_p < 0.05)\n",
    "                        stats_results[f'{feat}_significant'] = is_significant\n",
    "\n",
    "                        # Store feature importance for top features list\n",
    "                        feature_matrix_all.append({\n",
    "                            'transition': transition_name,\n",
    "                            'channel': col,\n",
    "                            'feature': feat,\n",
    "                            'cohens_d': abs(d),\n",
    "                            'mean_diff': abs(mean_diff),\n",
    "                            'pvalue': min(w_p if not np.isnan(w_p) else 1.0, t_p)\n",
    "                        })\n",
    "\n",
    "                        # Store per channel\n",
    "                        if col not in channel_feature_scores:\n",
    "                            channel_feature_scores[col] = []\n",
    "                        channel_feature_scores[col].append((feat, abs(d)))\n",
    "\n",
    "            # Store results\n",
    "            trans_results['before_features'][col] = before_df\n",
    "            trans_results['after_features'][col] = after_df\n",
    "            trans_results['statistics'][col] = stats_results\n",
    "\n",
    "            # Perform LDA analysis\n",
    "            lda_result = perform_lda_analysis(before_df, after_df, feature_cols, transition_name, col)\n",
    "            if lda_result:\n",
    "                trans_results['lda_results'][col] = lda_result\n",
    "\n",
    "                # Store LDA feature importance\n",
    "                if col not in lda_feature_scores:\n",
    "                    lda_feature_scores[col] = []\n",
    "\n",
    "                # Get top features from LDA\n",
    "                top_features = lda_result['feature_importance'].head(10)\n",
    "                for _, row in top_features.iterrows():\n",
    "                    lda_feature_scores[col].append((row['Feature'], row['Importance']))\n",
    "\n",
    "            # Calculate rolling features for event-aligned trajectories\n",
    "            rolling_data = window_data.copy()\n",
    "            rolling_data.sort_values('relative_time_min', inplace=True)\n",
    "            window_size = int(window_size_sec * fs)\n",
    "            step_size = window_size // 2\n",
    "\n",
    "            rolling_features = {}\n",
    "            transition_type = \"Metabolic\" if \"Metabolic\" in transition_name else \"Glycemic\"\n",
    "\n",
    "            for feat in feature_cols:\n",
    "                rolling_vals = []\n",
    "\n",
    "                for i in range(0, len(rolling_data) - window_size, step_size):\n",
    "                    window = rolling_data.iloc[i:i+window_size]\n",
    "                    if len(window) < window_size // 2:\n",
    "                        continue\n",
    "\n",
    "                    signal = window[col].values\n",
    "                    rel_time = window['relative_time_min'].mean()\n",
    "\n",
    "                    # Skip calculation for NaN relative times\n",
    "                    if np.isnan(rel_time):\n",
    "                        continue\n",
    "\n",
    "                    # Extract feature value based on feature type\n",
    "                    features = extract_features(signal)\n",
    "                    value = features.get(feat, np.nan)\n",
    "\n",
    "                    rolling_vals.append({\n",
    "                        'relative_time_min': rel_time,\n",
    "                        'value': value\n",
    "                    })\n",
    "\n",
    "                if rolling_vals:\n",
    "                    rolling_df = pd.DataFrame(rolling_vals)\n",
    "                    rolling_features[feat] = rolling_df\n",
    "\n",
    "                    # Store for heatmap visualization\n",
    "                    if transition_name not in heatmap_data:\n",
    "                        heatmap_data[transition_name] = {}\n",
    "                    if col not in heatmap_data[transition_name]:\n",
    "                        heatmap_data[transition_name][col] = {}\n",
    "                    heatmap_data[transition_name][col][feat] = rolling_df\n",
    "\n",
    "                    # Analyze transition dynamics\n",
    "                    dynamics = analyze_transition_dynamics(rolling_df, transition_type)\n",
    "                    if dynamics:\n",
    "                        # Store dynamics results\n",
    "                        key = (transition_name, col, feat)\n",
    "                        dynamics_results[key] = dynamics\n",
    "\n",
    "                        # Record feature dynamics scores\n",
    "                        if col not in feature_dynamics_scores:\n",
    "                            feature_dynamics_scores[col] = []\n",
    "                        feature_dynamics_scores[col].append((feat, dynamics['score']))\n",
    "\n",
    "            trans_results['rolling_features'][col] = rolling_features\n",
    "            trans_results['dynamics'][col] = {feat: dynamics_results.get((transition_name, col, feat), {})\n",
    "                                             for feat in rolling_features.keys()}\n",
    "\n",
    "        results[transition_name] = trans_results\n",
    "\n",
    "    # Create feature importance summary\n",
    "    feature_matrix_df = pd.DataFrame(feature_matrix_all)\n",
    "\n",
    "    # Create summary tables by transition and channel\n",
    "    transition_channel_summary = {}  # Structure: {transition: {channel: [(feature, score)]}}\n",
    "    transition_lda_summary = {}      # Structure: {transition: {channel: [(feature, importance)]}}\n",
    "    transition_dynamics_summary = {} # Structure: {transition: {channel: [(feature, dynamics_score)]}}\n",
    "\n",
    "    # Build data structures for organizing top features by transition and channel\n",
    "    for transition, transition_data in results.items():\n",
    "        transition_channel_summary[transition] = {}\n",
    "        transition_lda_summary[transition] = {}\n",
    "        transition_dynamics_summary[transition] = {}\n",
    "\n",
    "        # Get Cohen's d scores\n",
    "        for signal_col, stats in transition_data['statistics'].items():\n",
    "            cohens_d_scores = []\n",
    "            for feat_name, value in stats.items():\n",
    "                if feat_name.endswith('_cohens_d'):\n",
    "                    feature = feat_name.replace('_cohens_d', '')\n",
    "                    cohens_d_scores.append((feature, abs(value)))\n",
    "\n",
    "            transition_channel_summary[transition][signal_col] = sorted(\n",
    "                cohens_d_scores, key=lambda x: -x[1])\n",
    "\n",
    "        # Get LDA importance scores if available\n",
    "        if 'lda_results' in transition_data:\n",
    "            for signal_col, lda_result in transition_data['lda_results'].items():\n",
    "                if lda_result is None or 'feature_importance' not in lda_result:\n",
    "                    continue\n",
    "\n",
    "                feature_imp = lda_result['feature_importance']\n",
    "                lda_scores = list(zip(feature_imp['Feature'], feature_imp['Importance']))\n",
    "                transition_lda_summary[transition][signal_col] = sorted(\n",
    "                    lda_scores, key=lambda x: -x[1])\n",
    "\n",
    "        # Get dynamics scores\n",
    "        for signal_col, dynamics_data in transition_data['dynamics'].items():\n",
    "            dynamics_scores = []\n",
    "            for feat, dynamics in dynamics_data.items():\n",
    "                if dynamics and 'score' in dynamics:\n",
    "                    dynamics_scores.append((feat, dynamics['score']))\n",
    "\n",
    "            # Always sort by score (higher first)\n",
    "            transition_dynamics_summary[transition][signal_col] = sorted(\n",
    "                dynamics_scores, key=lambda x: -x[1])\n",
    "\n",
    "    # Print summaries and build report strings\n",
    "    report_texts = []\n",
    "    report_texts.append(\"# BIOMARKER ANALYSIS SUMMARY\\n\\n\")\n",
    "\n",
    "    # Add transition dynamics summary first (most important for feature dynamics)\n",
    "    report_texts.append(\"## TOP FEATURES BY TRANSITION DYNAMICS\\n\")\n",
    "    print(\"\\nTOP FEATURES BY TRANSITION DYNAMICS:\")\n",
    "\n",
    "    for transition, channels in transition_dynamics_summary.items():\n",
    "        if not channels:\n",
    "            continue\n",
    "\n",
    "        report_texts.append(f\"\\n### Transition: {transition}\\n\")\n",
    "        print(f\"\\n  Transition: {transition}\")\n",
    "\n",
    "        is_metabolic = \"Metabolic\" in transition\n",
    "        transition_type = \"Metabolic\" if is_metabolic else \"Glycemic\"\n",
    "\n",
    "        for channel, scores in channels.items():\n",
    "            top_features = scores[:10]  # Use top 10\n",
    "            if not top_features:\n",
    "                continue\n",
    "\n",
    "            # Display differently based on transition type\n",
    "            feature_text = []\n",
    "            for feat, score in top_features:\n",
    "                if score < 0.4:  # Skip low scores\n",
    "                    continue\n",
    "\n",
    "                dynamics = dynamics_results.get((transition, channel, feat), {})\n",
    "                if is_metabolic:\n",
    "                    delay = dynamics.get('response_delay', np.nan)\n",
    "                    delay_text = f\", delay={delay:.1f}m\" if not np.isnan(delay) else \"\"\n",
    "                    feature_text.append(f\"{feat} (score={score:.2f}{delay_text})\")\n",
    "                else:\n",
    "                    trend = \"↑\" if dynamics.get('mean_shift', 0) > 0 else \"↓\"\n",
    "                    feature_text.append(f\"{feat} {trend} (score={score:.2f})\")\n",
    "\n",
    "            if feature_text:\n",
    "                feature_text_joined = ', '.join(feature_text)\n",
    "\n",
    "                report_texts.append(f\"Channel {channel}:\\n\")\n",
    "                for i, (feat, score) in enumerate(top_features, 1):\n",
    "                    if score < 0.4:\n",
    "                        continue\n",
    "                    dynamics = dynamics_results.get((transition, channel, feat), {})\n",
    "                    detail = \"\"\n",
    "                    if is_metabolic:\n",
    "                        delay = dynamics.get('response_delay', np.nan)\n",
    "                        if not np.isnan(delay):\n",
    "                            detail = f\", response delay: {delay:.1f}min\"\n",
    "                    else:\n",
    "                        trend = \"increasing\" if dynamics.get('mean_shift', 0) > 0 else \"decreasing\"\n",
    "                        acceleration = dynamics.get('trend_acceleration', 0)\n",
    "                        detail = f\", {trend} trend\"\n",
    "                        if acceleration > 1.2:\n",
    "                            detail += \", accelerating\"\n",
    "\n",
    "                    report_texts.append(f\"  {i}. {feat} ({transition_type} score={score:.2f}{detail})\\n\")\n",
    "\n",
    "                print(f\"  Channel {channel}: {feature_text_joined}\")\n",
    "\n",
    "    # Add traditional Cohen's d and LDA summaries\n",
    "    report_texts.append(\"\\n## TOP FEATURES BY CHANNEL and by event(transition)\\n\")\n",
    "\n",
    "    for transition, channels in transition_channel_summary.items():\n",
    "        report_texts.append(f\"\\n### Transition: {transition}\\n\")\n",
    "        print(f\"\\nTOP FEATURES BY CHANNEL for {transition}:\")\n",
    "\n",
    "        for channel, scores in channels.items():\n",
    "            top_features = scores[:10]  # Use top 10\n",
    "            feature_text = ', '.join([f'{feat} (d={d:.2f})' for feat, d in top_features])\n",
    "\n",
    "            report_texts.append(f\"Channel {channel}:\\n\")\n",
    "            for i, (feat, d) in enumerate(top_features, 1):\n",
    "                report_texts.append(f\"  {i}. {feat} (d={d:.2f})\\n\")\n",
    "\n",
    "            print(f\"  Channel {channel}: {feature_text}\")\n",
    "\n",
    "    report_texts.append(\"\\n## TOP FEATURES BY CHANNEL (BY LDA IMPORTANCE) sorted by event(transition)\\n\")\n",
    "\n",
    "    for transition, channels in transition_lda_summary.items():\n",
    "        report_texts.append(f\"\\n### Transition: {transition}\\n\")\n",
    "        print(f\"\\nTOP FEATURES BY LDA IMPORTANCE for {transition}:\")\n",
    "\n",
    "        for channel, scores in channels.items():\n",
    "            top_features = scores[:10]  # Use top 10\n",
    "            feature_text = ', '.join([f'{feat} (imp={imp:.2f})' for feat, imp in top_features])\n",
    "\n",
    "            report_texts.append(f\"Channel {channel}:\\n\")\n",
    "            for i, (feat, imp) in enumerate(top_features, 1):\n",
    "                report_texts.append(f\"  {i}. {feat} (importance={imp:.2f})\\n\")\n",
    "\n",
    "            print(f\"  Channel {channel}: {feature_text}\")\n",
    "\n",
    "    # Find common top features across all channels for each transition\n",
    "    report_texts.append(\"\\n## TOP 10 FEATURES OVERALL - by event(transition) that are COMMON across the channels\\n\")\n",
    "    common_features_by_transition = {}\n",
    "\n",
    "    for transition, channels in transition_channel_summary.items():\n",
    "        # Extract top features from each channel\n",
    "        channel_top_features = {}\n",
    "        for channel, scores in channels.items():\n",
    "            channel_top_features[channel] = set(feat for feat, _ in scores[:20])  # Use top 20 for better overlap\n",
    "\n",
    "        # Find common features\n",
    "        if channel_top_features:\n",
    "            common_features = set.intersection(*channel_top_features.values()) if len(channel_top_features) > 1 else next(iter(channel_top_features.values()))\n",
    "\n",
    "            # Get mean scores for common features\n",
    "            common_with_scores = []\n",
    "            for feat in common_features:\n",
    "                scores = []\n",
    "                for channel_scores in channels.values():\n",
    "                    for f, score in channel_scores:\n",
    "                        if f == feat:\n",
    "                            scores.append(abs(score))\n",
    "\n",
    "                if scores:\n",
    "                    mean_score = sum(scores) / len(scores)\n",
    "                    common_with_scores.append((feat, mean_score))\n",
    "\n",
    "            # Sort and keep top 10\n",
    "            common_features_by_transition[transition] = sorted(common_with_scores, key=lambda x: -x[1])[:10]\n",
    "\n",
    "    # Print common features by transition\n",
    "    print(\"\\nTOP 10 FEATURES OVERALL - by event(transition) that are COMMON across channels:\")\n",
    "    for transition, features in common_features_by_transition.items():\n",
    "        print(f\"\\n  Transition: {transition}\")\n",
    "        report_texts.append(f\"\\n### Transition: {transition}\\n\")\n",
    "\n",
    "        for i, (feat, score) in enumerate(features, 1):\n",
    "            print(f\"    {i}. {feat}: mean |d| = {score:.2f}\")\n",
    "            report_texts.append(f\"{i}. {feat}: mean |d| = {score:.2f}\\n\")\n",
    "\n",
    "    # Save to files\n",
    "    if output_dir:\n",
    "        # Save text report\n",
    "        with open(output_dir / \"feature_importance_summary.txt\", 'w') as f:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            f.write(f\"# BIOMARKER ANALYSIS SUMMARY - {patient_id}\\n\")\n",
    "            f.write(f\"Generated: {timestamp}\\n\\n\")\n",
    "            f.write(''.join(report_texts))\n",
    "\n",
    "        # Save CSV files for each transition's common features\n",
    "        for transition, features in common_features_by_transition.items():\n",
    "            safe_transition = transition.replace(':', '_').replace(' ', '_')\n",
    "            pd.DataFrame(features, columns=['Feature', 'Mean |Cohen\\'s d|']).to_csv(\n",
    "                output_dir / f\"common_top_features_{safe_transition}.csv\", index=False)\n",
    "\n",
    "        # Save dynamics summary\n",
    "        dynamics_dir = output_dir / \"Dynamics\"\n",
    "        dynamics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        dynamics_summary = []\n",
    "        for transition, channels in transition_dynamics_summary.items():\n",
    "            for channel, scores in channels.items():\n",
    "                for feat, score in scores:\n",
    "                    dynamics = dynamics_results.get((transition, channel, feat), {})\n",
    "                    if dynamics:\n",
    "                        # Add different fields based on transition type\n",
    "                        entry = {\n",
    "                            'transition': transition,\n",
    "                            'channel': channel,\n",
    "                            'feature': feat,\n",
    "                            'dynamics_score': score,\n",
    "                        }\n",
    "\n",
    "                        # Add metabolic or glycemic specific fields\n",
    "                        if dynamics.get('is_metabolic', False):\n",
    "                            entry.update({\n",
    "                                'type': 'Metabolic',\n",
    "                                'mean_shift': dynamics.get('mean_shift', 0),\n",
    "                                'amplitude_shift': dynamics.get('amplitude_shift', 0),\n",
    "                                'peak_ratio': dynamics.get('peak_ratio', 0),\n",
    "                                'response_delay': dynamics.get('response_delay', np.nan)\n",
    "                            })\n",
    "                        else:\n",
    "                            entry.update({\n",
    "                                'type': 'Glycemic',\n",
    "                                'mean_shift': dynamics.get('mean_shift', 0),\n",
    "                                'trend_continuation': dynamics.get('trend_continuation', 0),\n",
    "                                'trend_acceleration': dynamics.get('trend_acceleration', 0),\n",
    "                                'smoothness': dynamics.get('smoothness', 0)\n",
    "                            })\n",
    "\n",
    "                        dynamics_summary.append(entry)\n",
    "\n",
    "        if dynamics_summary:\n",
    "            pd.DataFrame(dynamics_summary).to_csv(dynamics_dir / \"transition_dynamics_summary.csv\", index=False)\n",
    "\n",
    "    # Generate visualizations\n",
    "    # --- Boxplot Matrix ---\n",
    "    matrix_df = []\n",
    "    for entry in matrix_boxplot_data:\n",
    "        for v in entry['values']:\n",
    "            matrix_df.append({\n",
    "                'transition': entry['transition'],\n",
    "                'signal': entry['signal'],\n",
    "                'feature': entry['feature'],\n",
    "                'label': entry['label'],\n",
    "                'value': v\n",
    "            })\n",
    "    matrix_df = pd.DataFrame(matrix_df)\n",
    "\n",
    "    # Order features and transitions for consistent display\n",
    "    feature_order = [f for f in matrix_df['feature'].unique() if f.startswith('rel_power_') or f.startswith('wavelet_band_') or f in [\n",
    "        'mean', 'std', 'skew', 'kurtosis', 'zero_crossings', 'hjorth_activity',\n",
    "        'hjorth_mobility', 'hjorth_complexity', 'shannon_entropy']]\n",
    "    transition_order = matrix_df['transition'].unique().tolist()\n",
    "    signal_order = matrix_df['signal'].unique().tolist()\n",
    "\n",
    "    # Generate boxplot matrices for each signal\n",
    "    for sig in signal_order:\n",
    "        df_sub = matrix_df[matrix_df['signal'] == sig]\n",
    "        features = [f for f in feature_order if f in df_sub['feature'].values]\n",
    "        transitions = transition_order\n",
    "        n_feat = len(features)\n",
    "        n_tran = len(transitions)\n",
    "\n",
    "        fig, axes = plt.subplots(n_feat, n_tran, figsize=(3*n_tran, 2*n_feat), sharey='row')\n",
    "        if n_feat == 1 and n_tran == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif n_feat == 1 or n_tran == 1:\n",
    "            axes = axes.reshape((n_feat, n_tran))\n",
    "\n",
    "        for i, feat in enumerate(features):\n",
    "            for j, tran in enumerate(transitions):\n",
    "                ax = axes[i, j]\n",
    "                sub = df_sub[(df_sub['feature'] == feat) & (df_sub['transition'] == tran)]\n",
    "\n",
    "                if sub.empty:\n",
    "                    ax.axis('off')\n",
    "                    continue\n",
    "\n",
    "                sns.boxplot(x='label', y='value', data=sub, ax=ax, palette='Set2')\n",
    "\n",
    "                if i == 0:\n",
    "                    ax.set_title(tran, fontsize=10)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(feat, fontsize=9)\n",
    "                else:\n",
    "                    ax.set_ylabel('')\n",
    "                ax.set_xlabel('')\n",
    "\n",
    "        plt.suptitle(f'Boxplot Matrix for Signal: {sig}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.92)\n",
    "\n",
    "        # Save figure if output directory is specified\n",
    "        if output_dir:\n",
    "            filename = f\"boxplot_matrix_{sig}.png\"\n",
    "            plt.savefig(output_dir / filename, dpi=100, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Plot feature trajectories with transition dynamics\n",
    "    plot_feature_trajectories_matrix(heatmap_data, dynamics_results)\n",
    "\n",
    "    # Plot dynamics matrices for each signal\n",
    "    plot_dynamics_matrix(heatmap_data, dynamics_results, output_dir=output_dir)\n",
    "\n",
    "    # Add results to the main dictionary\n",
    "    results['dynamics_results'] = dynamics_results\n",
    "    results['transition_dynamics_summary'] = transition_dynamics_summary\n",
    "\n",
    "    return results, feature_matrix_df  # Return both results and top features dataframe"
   ],
   "id": "d5e4b08ab4fa96c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the analysis\n",
    "biomarker_results, top_features_df = analyze_transition_biomarkers(\n",
    "    transition_windows,\n",
    "    output_dir=output_dir,\n",
    "    patient_id=patient\n",
    ")"
   ],
   "id": "764861ce454f46dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate the feature statistics summary per channel/transition event:",
   "id": "6c7f0b6c472039fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Processes EMF signal data surrounding transition events (metabolic or glycemic)\n",
    "\n",
    "**Extracts a set of features from signal windows:**\n",
    "\n",
    "- Time-domain features (mean, std, skew, kurtosis, zero crossings)\n",
    "- Frequency-domain features (relative power in frequency bands)\n",
    "- Wavelet-based features (energy in specific frequency bands)\n",
    "- Complexity measures (Hjorth parameters, entropy)\n",
    "\n",
    "**Analyzes transition dynamics based on transition type:**\n",
    "\n",
    "- For metabolic transitions: detects sharp changes and response delays\n",
    "- For glycemic transitions: identifies trend continuations and accelerations\n",
    "\n",
    "**Performs statistical comparisons between pre/post transition windows:**\n",
    "\n",
    "- Paired t-tests and Wilcoxon tests for significance\n",
    "- Cohen's d for effect size measurement\n",
    "- LDA (Linear Discriminant Analysis) for feature importance\n",
    "\n",
    "**Visualizes results in feature-by-event matrices:**\n",
    "\n",
    "- Features organized as rows, events as columns\n",
    "- Different visualizations for metabolic vs glycemic transitions\n",
    "- Dynamics scores displayed directly on plots\n",
    "\n",
    "**Compiles results into multiple formats:**\n",
    "\n",
    "- Ranked feature lists by statistical significance\n",
    "- Ranked feature lists by dynamics score\n",
    "- Common features across channels\n",
    "- Comprehensive dynamics summary tables\n",
    "- Returns both complete results dictionary and top features dataframe for further analysis\n",
    "\n",
    "The function specifically identifies features showing the expected dynamic patterns: sharp transitions with possible delays for metabolic events, and continuous trends for glycemic events.\n"
   ],
   "id": "1acff07d16f28f0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dynamic scores are quantitative measurements we calculate to evaluate how strongly and characteristically a feature responds to a transition event.<br>\n",
    "They're designed to capture different patterns based on transition type:\n",
    "\n",
    "**For Metabolic transitions (insulin/sugar administration), the score combines:**\n",
    "\n",
    "- Amplitude shift (how much the feature value jumps)\n",
    "- Peak ratio (change in peak amplitudes)\n",
    "- Response timing (clear response after transition)\n",
    "\n",
    "**For Glycemic transitions (gradual sugar level changes), the score combines:**\n",
    "\n",
    "- Trend continuation (consistency of direction)\n",
    "- Trend acceleration (change in slope)\n",
    "- Smoothness (lack of abrupt changes)\n",
    "\n",
    "**Higher scores (typically >0.5) indicate features that show the expected response pattern for that transition type.**\n",
    "If no score is displayed on a plot in the dynamics matrix, it means the score is below the threshold value of 0.5"
   ],
   "id": "7608e6c7dee9ef84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_statistical_summary(biomarker_results, include_dynamics=True):\n",
    "    \"\"\"\n",
    "    Generate a summary table of statistically significant features across transitions,\n",
    "    including dynamics information.\n",
    "    \"\"\"\n",
    "    summary_rows = []\n",
    "\n",
    "    # Debug: Count how many transitions we're processing\n",
    "    transitions_processed = 0\n",
    "\n",
    "    # Process each transition\n",
    "    for transition, results in biomarker_results.items():\n",
    "        # Skip non-transition entries like 'dynamics_results'\n",
    "        if not isinstance(results, dict) or 'statistics' not in results:\n",
    "            continue\n",
    "\n",
    "        transitions_processed += 1\n",
    "\n",
    "        # Determine transition type\n",
    "        is_metabolic = \"Metabolic\" in transition\n",
    "        transition_type = \"Metabolic\" if is_metabolic else \"Glycemic\"\n",
    "\n",
    "        # Get dynamics data if available and requested\n",
    "        dynamics_by_feature = {}\n",
    "        if include_dynamics and 'dynamics' in results:\n",
    "            for signal, dynamics_dict in results['dynamics'].items():\n",
    "                for feature, dynamics in dynamics_dict.items():\n",
    "                    if isinstance(dynamics, dict) and 'score' in dynamics:\n",
    "                        key = (signal, feature)\n",
    "                        dynamics_by_feature[key] = dynamics\n",
    "\n",
    "        # Process each signal's statistics\n",
    "        for signal_col, stats in results['statistics'].items():\n",
    "            # Find features with statistical significance\n",
    "            for feat_name in stats.keys():\n",
    "                # Look for significance indicators\n",
    "                if feat_name.endswith('_significant'):\n",
    "                    base_feat = feat_name.replace('_significant', '')\n",
    "                    is_significant = stats[feat_name]\n",
    "\n",
    "                    # Also check p-values directly as a backup\n",
    "                    p_value = stats.get(f'{base_feat}_ttest_p', 1.0)\n",
    "\n",
    "                    # Consider significant if explicitly marked or p < 0.05\n",
    "                    if is_significant or (p_value < 0.05):\n",
    "                        # Collect standard statistics\n",
    "                        effect = stats.get(f'{base_feat}_cohens_d', np.nan)\n",
    "                        mean_diff = stats.get(f'{base_feat}_mean_diff', np.nan)\n",
    "                        wilcoxon_p = stats.get(f'{base_feat}_wilcoxon_p', np.nan)\n",
    "                        best_p = min(p_value, wilcoxon_p) if not np.isnan(wilcoxon_p) else p_value\n",
    "\n",
    "                        # Create base row\n",
    "                        row = {\n",
    "                            'Transition': transition,\n",
    "                            'Type': transition_type,\n",
    "                            'Signal': signal_col,\n",
    "                            'Feature': base_feat,\n",
    "                            'p-value': best_p,\n",
    "                            'Effect Size': effect,\n",
    "                            'Mean Difference': mean_diff,\n",
    "                        }\n",
    "\n",
    "                        # Add dynamics information if available\n",
    "                        if include_dynamics:\n",
    "                            key = (signal_col, base_feat)\n",
    "                            if key in dynamics_by_feature:\n",
    "                                dyn = dynamics_by_feature[key]\n",
    "\n",
    "                                # Add common dynamics metrics\n",
    "                                row['Dynamics Score'] = dyn.get('score', np.nan)\n",
    "\n",
    "                                # Add type-specific metrics\n",
    "                                if is_metabolic:\n",
    "                                    row['Response Delay (min)'] = dyn.get('response_delay', np.nan)\n",
    "                                    row['Amplitude Shift'] = dyn.get('amplitude_shift', np.nan)\n",
    "                                    row['Peak Ratio'] = dyn.get('peak_ratio', np.nan)\n",
    "                                else:\n",
    "                                    row['Trend Continuation'] = dyn.get('trend_continuation', np.nan)\n",
    "                                    row['Trend Acceleration'] = dyn.get('trend_acceleration', np.nan)\n",
    "                                    row['Smoothness'] = dyn.get('smoothness', np.nan)\n",
    "                            else:\n",
    "                                row['Dynamics Score'] = np.nan\n",
    "\n",
    "                        summary_rows.append(row)\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    if summary_rows:\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "        # Calculate combined score if dynamics included\n",
    "        if include_dynamics and 'Dynamics Score' in summary_df.columns:\n",
    "            # Normalize effect size (Cohen's d) to 0-1 scale\n",
    "            abs_effect = summary_df['Effect Size'].abs()\n",
    "            max_effect = abs_effect.max()\n",
    "            if max_effect > 0:\n",
    "                norm_effect = abs_effect / max_effect\n",
    "            else:\n",
    "                norm_effect = abs_effect\n",
    "\n",
    "            # Create combined score (50% statistical, 50% dynamics)\n",
    "            dynamics_score = summary_df['Dynamics Score'].fillna(0)\n",
    "            summary_df['Combined Score'] = 0.5 * norm_effect + 0.5 * dynamics_score\n",
    "\n",
    "        # Sort first by type then by combined score or p-value\n",
    "        if 'Combined Score' in summary_df.columns:\n",
    "            summary_df = summary_df.sort_values(['Type', 'Combined Score'], ascending=[True, False])\n",
    "        else:\n",
    "            summary_df = summary_df.sort_values(['Type', 'p-value'])\n",
    "\n",
    "        # Format numeric columns\n",
    "        format_cols = {\n",
    "            'p-value': '{:.4f}',\n",
    "            'Effect Size': '{:.2f}',\n",
    "            'Mean Difference': '{:.3f}',\n",
    "            'Dynamics Score': '{:.2f}',\n",
    "            'Combined Score': '{:.2f}',\n",
    "            'Response Delay (min)': '{:.1f}',\n",
    "            'Amplitude Shift': '{:.2f}',\n",
    "            'Peak Ratio': '{:.2f}',\n",
    "            'Trend Continuation': '{:.2f}',\n",
    "            'Trend Acceleration': '{:.2f}',\n",
    "            'Smoothness': '{:.2f}'\n",
    "        }\n",
    "\n",
    "        for col, fmt in format_cols.items():\n",
    "            if col in summary_df.columns:\n",
    "                summary_df[col] = summary_df[col].apply(\n",
    "                    lambda x: fmt.format(x) if not pd.isna(x) else \"\")\n",
    "\n",
    "        # Generate descriptive change column\n",
    "        def describe_change(row):\n",
    "            try:\n",
    "                mean_diff = float(row['Mean Difference']) if row['Mean Difference'] else 0\n",
    "                effect_str = row['Effect Size']\n",
    "                effect = float(effect_str) if effect_str else 0\n",
    "\n",
    "                if abs(effect) < 0.2:\n",
    "                    magnitude = \"minimal\"\n",
    "                elif abs(effect) < 0.5:\n",
    "                    magnitude = \"small\"\n",
    "                elif abs(effect) < 0.8:\n",
    "                    magnitude = \"moderate\"\n",
    "                else:\n",
    "                    magnitude = \"large\"\n",
    "\n",
    "                direction = \"increase\" if mean_diff > 0 else \"decrease\"\n",
    "\n",
    "                # Add special details for metabolic transitions with response delay\n",
    "                if row['Type'] == 'Metabolic' and 'Response Delay (min)' in row and row['Response Delay (min)']:\n",
    "                    try:\n",
    "                        delay = float(row['Response Delay (min)'])\n",
    "                        if not np.isnan(delay):\n",
    "                            return f\"{magnitude} {direction} after {delay:.1f}min\"\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                return f\"{magnitude} {direction}\"\n",
    "            except:\n",
    "                return \"\"\n",
    "\n",
    "        summary_df['Change Description'] = summary_df.apply(describe_change, axis=1)\n",
    "\n",
    "        print(f\"Found {len(summary_rows)} significant features across {transitions_processed} transitions.\")\n",
    "        return summary_df\n",
    "    else:\n",
    "        print(f\"Processed {transitions_processed} transitions but found no significant features.\")\n",
    "        return \"No significant features found.\"\n",
    "\n",
    "# Generate and display statistical summary\n",
    "stat_summary = generate_statistical_summary(biomarker_results)\n",
    "if isinstance(stat_summary, pd.DataFrame):\n",
    "    # Display separately for metabolic and glycemic features if possible\n",
    "    if 'Type' in stat_summary.columns:\n",
    "        print(\"\\nMETABOLIC TRANSITION FEATURES\")\n",
    "        metabolic_features = stat_summary[stat_summary['Type'] == 'Metabolic']\n",
    "        if not metabolic_features.empty:\n",
    "            display(metabolic_features.reset_index(drop=True))\n",
    "        else:\n",
    "            print(\"No significant metabolic features found\")\n",
    "\n",
    "        print(\"\\nGLYCEMIC TRANSITION FEATURES\")\n",
    "        glycemic_features = stat_summary[stat_summary['Type'] == 'Glycemic']\n",
    "        if not glycemic_features.empty:\n",
    "            display(glycemic_features.reset_index(drop=True))\n",
    "        else:\n",
    "            print(\"No significant glycemic features found\")\n",
    "    else:\n",
    "        display(stat_summary)\n",
    "else:\n",
    "    print(stat_summary)"
   ],
   "id": "d532febf9f00e43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the statistical summary to CSV if it exists\n",
    "if isinstance(stat_summary, pd.DataFrame) and output_dir:\n",
    "    summary_filename = f\"{patient}_biomarker_statistics.csv\"\n",
    "    summary_path = output_dir / f\"{patient}_biomarkers\" / summary_filename\n",
    "    stat_summary.to_csv(summary_path, index=False)\n",
    "    print(f\"Statistical summary saved to: {summary_path}\")"
   ],
   "id": "4401664320d69f39",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
