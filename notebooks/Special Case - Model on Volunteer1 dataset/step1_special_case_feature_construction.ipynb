{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## EMF Signal Processing Pipeline for special case (Volunteer) - Feature Construction\n",
    "\n",
    "**Signal Normalization:**\n",
    "- Per-subject z-score normalization: standardizes each signal channel individually per patient using mean and standard deviation\n",
    "- Theoretically shall eliminate sensor placement and session-specific baseline differences while preserving physiological signal patterns\n",
    "\n",
    "**Signal Windowing:**\n",
    "- 60-second overlapping windows with 15-second overlap\n",
    "- Maintains temporal context for frequency analysis while enabling sufficient training samples\n",
    "\n",
    "**Feature Extraction:**\n",
    "- **Time-domain features:** Mean, std, RMS, min, max, range, skewness, kurtosis, MAD, and Hjorth parameters (Activity, Mobility, Complexity)\n",
    "- **Frequency-domain features:** Bandpower analysis across 8 frequency bands including critical 0.01-0.02 Hz range, plus total power\n",
    "- **Wavelet features:** 6-level decomposition energies with frequency band descriptions for multi-resolution analysis\n",
    "- **Entropy measures:** Shannon entropy for signal complexity quantification\n",
    "- **Patient metadata:** Age, weight, height, sex, and calculated BMI from patient demographics (in this case we have lack of patient metadata, so these features might be unused)\n",
    "\n",
    "**Feature Selection & Dimensionality Reduction:**\n",
    "- **PCA reduction is not applied** in this step to retain all features for comprehensive analysis\n",
    "- **All domain features retained** for comprehensive analysis\n",
    "\n",
    "**Target Construction:**\n",
    "- **Glucose regression:** 10-minute lag-corrected CGM values to account for sensor delay\n",
    "- **Glycemic state prediction:** 15-minute ahead glycemic state classification (hypoglycemic <70 mg/dL, normal 70-180 mg/dL, hyperglycemic >180 mg/dL)\n",
    "- **Hypoglycemia flag:** Binary flag (1) if glucose < 75 mg/dL within next 900 seconds (15 minutes)\n",
    "- **Hyperglycemia flag:** Binary flag (1) if glucose > 180 mg/dL within next 900 seconds (15 minutes)\n",
    "- **Metabolic state tracking:** Fasting, First Insulin, Ensure, Second Insulin phases based on experimental protocol events\n",
    "- Time-based nearest neighbor matching between feature windows and glucose measurements\n",
    "\n",
    "**Output:**\n",
    "- **Combined multi-patient dataset** with comprehensive feature set (all domain features)"
   ],
   "id": "de89fa36fb039c8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:09.683031Z",
     "start_time": "2025-08-11T19:10:06.594597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# General imports:\n",
    "\n",
    "# Disable warnings:\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Essential imports\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Plotting enhancements\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['image.cmap'] = 'turbo'  # Set default colormap to turbo for all images\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ],
   "id": "752d65719ed317d9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:09.704151Z",
     "start_time": "2025-08-11T19:10:09.693333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Key Settings:\n",
    "\n",
    "# Physical constants and sensors specifications:\n",
    "SENSITIVITY = 50  # mV/nT\n",
    "MAGNETIC_NOISE = 3  # pT/âˆšHz @ 1 Hz\n",
    "MAX_AC_LINEARITY = 250  # nT (+/- 250 nT) - Equivalent to 21.78 V\n",
    "MAX_DC_LINEARITY = 60  # nT (+/- 60 nT)\n",
    "VOLTAGE_LIMIT = 15 # V (+/-15V)\n",
    "CONVERSION_FACTOR = 20  # nT per 1V\n",
    "SAMPLING_FREQUENCY = 5000  # Hz - expected from the experimental data\n",
    "SENSOR_SATURATION = 250  # nT - saturation threshold for the sensor\n",
    "\n",
    "# Subjects and their types\n",
    "# Subject = {\"Normal\": \"Normal Subjects\",\"Clamp\": \"T1DM Clamp Subjects\", \"Additional\": \"Additional Subjects\"}\n",
    "\n",
    "# Path and directories\n",
    "base_dir = Path(\"../../../Data\")\n",
    "\n",
    "# Output directory for saving results\n",
    "output_dir = base_dir / \"ProcessedData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Directory for saving processed/downsampled signal files (parquet format)\n",
    "signals_dir = output_dir / \"Signal_Files\"\n",
    "os.makedirs(signals_dir, exist_ok=True)\n",
    "\n",
    "# Labels directory\n",
    "labels_dir = base_dir / \"RawData\"\n",
    "labels_filename = \"FilteredLabels.xlsx\"\n",
    "\n",
    "# Patients data file\n",
    "patients_file = \"patients.json\"\n",
    "\n",
    "# Patient Data\n",
    "with open(labels_dir / patients_file, 'r') as f:\n",
    "    # Load the JSON data\n",
    "    patients_data = json.load(f)\n",
    "\n",
    "# GMT zone correction for the sensor = GMT+2\n",
    "GMT = 2\n",
    "\n",
    "# Channel grouping (special case for volunteer data)\n",
    "signal_channels = {\n",
    "    'Hand': ['Hand1', 'Hand2'],\n",
    "}\n",
    "\n",
    "# Define glucose lag and glycemic prediction parameters\n",
    "GLUCOSE_LAG_MINUTES = 10  # Glucose lag in minutes\n",
    "GLYCEMIC_PREDICTION_MINUTES = 15  # Glycemic prediction in minutes\n",
    "WINDOW_SIZE_MINUTES = 1  # Window size in minutes\n",
    "WINDOW_OVERLAP_MINUTES = 0.25  # Overlap size in minutes\n"
   ],
   "id": "abdadb7ec5f1885b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the CGM/stick Glucose target values from the file",
   "id": "9b48f95b25b970a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:09.724860Z",
     "start_time": "2025-08-11T19:10:09.714989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_all_transitions(df_labels):\n",
    "    \"\"\"\n",
    "    Detect both metabolic state transitions and glycemic state transitions.\n",
    "\n",
    "    Glycemic states:\n",
    "    - Hypoglycemia: < 70 mg/dL\n",
    "    - Normal: 70-180 mg/dL\n",
    "    - Hyperglycemia: > 180 mg/dL\n",
    "    \"\"\"\n",
    "    # Initialize transitions dictionary\n",
    "    transitions = {}\n",
    "\n",
    "    # Ensure data is sorted by time\n",
    "    df_sorted = df_labels.sort(\"time\")\n",
    "\n",
    "    # Extract metabolic state transitions (already computed)\n",
    "    metabolic_states = [\"Fasting\", \"First Insulin\", \"Ensure\", \"Second Insulin\"]\n",
    "\n",
    "    # Find the first occurrence of each state\n",
    "    for state in metabolic_states:\n",
    "        state_rows = df_sorted.filter(pl.col('state') == state)\n",
    "        if len(state_rows) > 0:\n",
    "            transitions[f\"Metabolic: {state}\"] = state_rows['time'].item(0)\n",
    "\n",
    "    # Filter out rows with null glucose values and add glycemic state\n",
    "    df_glycemic = df_sorted.filter(\n",
    "        pl.col('Glucose').is_not_null() &\n",
    "        ~pl.col('Glucose').is_nan()\n",
    "    ).with_columns([\n",
    "        pl.when(pl.col('Glucose') < 70)\n",
    "          .then(pl.lit(\"Hypoglycemia\"))\n",
    "          .when(pl.col('Glucose') <= 180)\n",
    "          .then(pl.lit(\"Normal\"))\n",
    "          .otherwise(pl.lit(\"Hyperglycemia\"))\n",
    "          .alias(\"glycemic_state\")\n",
    "    ])\n",
    "\n",
    "    # Record the initial glycemic state\n",
    "    if len(df_glycemic) > 0:\n",
    "        initial_state = df_glycemic['glycemic_state'].item(0)\n",
    "        initial_time = df_glycemic['time'].item(0)\n",
    "        transitions[f\"Initial Glycemic State: {initial_state}\"] = initial_time\n",
    "\n",
    "    # Detect transitions using a window-based approach\n",
    "    previous_state = None\n",
    "    for row in df_glycemic.iter_rows(named=True):\n",
    "        current_state = row['glycemic_state']\n",
    "        current_time = row['time']\n",
    "\n",
    "        if previous_state is not None and current_state != previous_state:\n",
    "            transition_name = f\"Glycemic: {previous_state} to {current_state}\"\n",
    "            transitions[transition_name] = current_time\n",
    "\n",
    "        previous_state = current_state\n",
    "\n",
    "    return transitions"
   ],
   "id": "befd595887de8074",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to load the downsampled signals",
   "id": "601ab9d85b63f7a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:09.743698Z",
     "start_time": "2025-08-11T19:10:09.736764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the downsampled data from parquet file\n",
    "def load_downsampled_data(patient_name, signals_dir):\n",
    "    \"\"\"\n",
    "    Load downsampled data from parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - patient_name: name of the patient\n",
    "    - signals_dir: directory containing the parquet signal files\n",
    "\n",
    "    Returns:\n",
    "    - df_loaded: polars DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    # Create filename based on patient name\n",
    "    patient_safe_name = patient_name.replace(\" \", \"_\").replace(\"#\", \"\")\n",
    "    output_filename = f\"{patient_safe_name}_downsampled_25hz.parquet\"\n",
    "    output_path = signals_dir / output_filename\n",
    "\n",
    "    try:\n",
    "        if output_path.exists():\n",
    "            df_loaded = pl.read_parquet(output_path)\n",
    "            print(f\"Successfully loaded data from {output_filename}\")\n",
    "            print(f\"Loaded data shape: {df_loaded.shape}\")\n",
    "            return df_loaded\n",
    "        else:\n",
    "            print(f\"File not found: {output_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file: {e}\")\n",
    "        return None"
   ],
   "id": "aeec5908e9339c13",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to apply z-score normalization",
   "id": "e1edeb199662324e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:09.773174Z",
     "start_time": "2025-08-11T19:10:09.759562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_zscore_normalization(df_signal, patient_name, global_scalers=None):\n",
    "    \"\"\"\n",
    "    Apply z-score (StandardScaler) normalization to all signal channels per channel,\n",
    "    followed by cross-session joint normalization.\n",
    "\n",
    "    Parameters:\n",
    "    - df_signal: polars DataFrame containing the signal data\n",
    "    - patient_name: name of the patient\n",
    "    - global_scalers: dictionary of pre-fitted StandardScaler objects for cross-session normalization\n",
    "\n",
    "    Returns:\n",
    "    - df_normalized: polars DataFrame with normalized signals\n",
    "    - channel_scalers: dictionary of fitted StandardScaler objects for this patient\n",
    "    \"\"\"\n",
    "    if df_signal is None:\n",
    "        print(\"No signal data provided for normalization\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Applying z-score normalization for {patient_name}\")\n",
    "\n",
    "    # Find the time column\n",
    "    time_col = None\n",
    "    for col in df_signal.columns:\n",
    "        if 'time' in col.lower():\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"Error: No time column found in the data\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert to pandas temporarily for easier processing\n",
    "    df_pandas = df_signal.to_pandas()\n",
    "\n",
    "    # Get all signal columns (exclude time column)\n",
    "    signal_columns = [col for col in df_pandas.columns if col != time_col]\n",
    "\n",
    "    print(f\"Processing {len(signal_columns)} signal channels\")\n",
    "\n",
    "    # Step 1: Per-channel z-score normalization\n",
    "    df_normalized = df_pandas.copy()\n",
    "    channel_scalers = {}\n",
    "\n",
    "    for col in signal_columns:\n",
    "        if df_pandas[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "            # Remove NaN values for fitting\n",
    "            valid_data = df_pandas[col].dropna()\n",
    "\n",
    "            if len(valid_data) == 0:\n",
    "                print(f\"Warning: No valid data for channel {col}\")\n",
    "                continue\n",
    "\n",
    "            # Fit StandardScaler on this channel\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            # Reshape for sklearn (needs 2D array)\n",
    "            data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "            scaler.fit(data_reshaped)\n",
    "\n",
    "            # Transform the entire column (including NaN values)\n",
    "            # Handle NaN values by only transforming non-NaN entries\n",
    "            transformed_data = df_pandas[col].copy()\n",
    "            non_nan_mask = ~df_pandas[col].isna()\n",
    "\n",
    "            if non_nan_mask.sum() > 0:\n",
    "                transformed_data[non_nan_mask] = scaler.transform(\n",
    "                    df_pandas[col][non_nan_mask].values.reshape(-1, 1)\n",
    "                ).flatten()\n",
    "\n",
    "            df_normalized[col] = transformed_data\n",
    "            channel_scalers[col] = scaler\n",
    "\n",
    "            # Handle both scalar and array cases for mean_ and scale_\n",
    "            mean_val = scaler.mean_[0] if hasattr(scaler.mean_, '__len__') else scaler.mean_\n",
    "            scale_val = scaler.scale_[0] if hasattr(scaler.scale_, '__len__') else scaler.scale_\n",
    "            print(f\"Channel {col}: mean={mean_val:.4f}, std={scale_val:.4f}\")\n",
    "\n",
    "    # Step 2: Cross-session joint normalization (if global scalers provided)\n",
    "    if global_scalers is not None:\n",
    "        print(f\"Applying cross-session joint normalization\")\n",
    "\n",
    "        for col in signal_columns:\n",
    "            if col in global_scalers and col in df_normalized.columns:\n",
    "                # Apply global scaler to already per-channel normalized data\n",
    "                non_nan_mask = ~df_normalized[col].isna()\n",
    "\n",
    "                if non_nan_mask.sum() > 0:\n",
    "                    df_normalized.loc[non_nan_mask, col] = global_scalers[col].transform(\n",
    "                        df_normalized.loc[non_nan_mask, col].values.reshape(-1, 1)\n",
    "                    ).flatten()\n",
    "\n",
    "                print(f\"Applied cross-session normalization to {col}\")\n",
    "\n",
    "    # Convert back to polars\n",
    "    df_normalized = pl.from_pandas(df_normalized)\n",
    "\n",
    "    print(f\"Z-score normalization completed for {len(signal_columns)} channels\")\n",
    "    return df_normalized, channel_scalers\n"
   ],
   "id": "56002a14aa16c3c1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the functions of Feature Extraction",
   "id": "d9d4f15e3eab5e1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:09.906622Z",
     "start_time": "2025-08-11T19:10:09.788343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "import pywt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_hjorth_parameters(signal):\n",
    "    \"\"\"\n",
    "    Calculate Hjorth parameters (Activity, Mobility, Complexity)\n",
    "    \"\"\"\n",
    "    # First derivative\n",
    "    diff1 = np.diff(signal)\n",
    "    # Second derivative\n",
    "    diff2 = np.diff(diff1)\n",
    "\n",
    "    # Variance calculations\n",
    "    var_signal = np.var(signal)\n",
    "    var_diff1 = np.var(diff1)\n",
    "    var_diff2 = np.var(diff2)\n",
    "\n",
    "    # Hjorth parameters\n",
    "    activity = var_signal\n",
    "    mobility = np.sqrt(var_diff1 / var_signal) if var_signal > 0 else 0\n",
    "    complexity = np.sqrt(var_diff2 / var_diff1) / mobility if var_diff1 > 0 and mobility > 0 else 0\n",
    "\n",
    "    return activity, mobility, complexity\n",
    "\n",
    "def calculate_bandpower(signal, fs, freq_range):\n",
    "    \"\"\"\n",
    "    Calculate power in a specific frequency band\n",
    "    \"\"\"\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(len(signal), 1024))\n",
    "\n",
    "    # Find frequency indices\n",
    "    freq_mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])\n",
    "\n",
    "    if np.sum(freq_mask) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate power in band\n",
    "    bandpower = np.trapezoid(psd[freq_mask], freqs[freq_mask])\n",
    "    return bandpower\n",
    "\n",
    "def calculate_entropy_measures(signal):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy only - removed approximate entropy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Shannon entropy\n",
    "        hist, _ = np.histogram(signal, bins=50)\n",
    "        hist = hist[hist > 0]  # Remove zero bins\n",
    "        shannon_entropy = entropy(hist)\n",
    "\n",
    "        return shannon_entropy\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Entropy calculation failed: {e}\")\n",
    "        return 0\n",
    "\n",
    "def calculate_shannon_entropy(signal):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy\n",
    "    \"\"\"\n",
    "    return calculate_entropy_measures(signal)\n",
    "\n",
    "def calculate_bandpower_efficient(signal, fs, freq_ranges):\n",
    "    \"\"\"\n",
    "    Calculate power in multiple frequency bands with better low-frequency resolution\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For better low-frequency resolution, use longer segments\n",
    "        # Aim for frequency resolution of ~0.005 Hz to capture 0.01-0.02 Hz band\n",
    "        target_freq_resolution = 0.005\n",
    "        ideal_nperseg = int(fs / target_freq_resolution)  # ~5000 samples\n",
    "\n",
    "        # Use the full signal length if possible, but cap at reasonable limit\n",
    "        nperseg = min(len(signal), ideal_nperseg, 8192)\n",
    "\n",
    "        # Ensure nperseg is not too small\n",
    "        nperseg = max(nperseg, 512)\n",
    "\n",
    "        freqs, psd = welch(signal, fs=fs, nperseg=nperseg, noverlap=nperseg//2)\n",
    "\n",
    "        # Calculate all band powers in one pass\n",
    "        bandpowers = {}\n",
    "        for band_name, freq_range in freq_ranges.items():\n",
    "            freq_mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])\n",
    "\n",
    "            if np.sum(freq_mask) == 0:\n",
    "                # If no frequencies in range, interpolate or use nearest\n",
    "                print(f\"Warning: No frequencies found for band {band_name} ({freq_range})\")\n",
    "                bandpowers[band_name] = 0\n",
    "            else:\n",
    "                bandpowers[band_name] = np.trapezoid(psd[freq_mask], freqs[freq_mask])\n",
    "\n",
    "        return bandpowers\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Bandpower calculation failed: {e}\")\n",
    "        return {band_name: 0 for band_name in freq_ranges.keys()}\n",
    "\n",
    "def calculate_wavelet_energies(signal, fs=25, wavelet='db4', levels=6):\n",
    "    \"\"\"\n",
    "    Calculate wavelet energies with frequency band descriptions - optimized version\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Limit signal length for faster computation\n",
    "        max_samples = 5000  # Limit to ~3 minutes at 25 Hz\n",
    "        if len(signal) > max_samples:\n",
    "            signal = signal[:max_samples]\n",
    "\n",
    "        # Decompose signal using wavelet\n",
    "        coeffs = pywt.wavedec(signal, wavelet, level=levels)\n",
    "\n",
    "        # Calculate energy in each subband with frequency descriptions\n",
    "        energies = []\n",
    "\n",
    "        # Approximate frequency bands based on sampling rate and decomposition levels\n",
    "        # For fs=25Hz, Nyquist = 12.5Hz\n",
    "        nyquist = fs / 2\n",
    "\n",
    "        for i, coeff in enumerate(coeffs):\n",
    "            energy = np.sum(np.square(coeff))\n",
    "\n",
    "            # Map wavelet levels to approximate frequency bands\n",
    "            if i == 0:  # Approximation coefficients (lowest frequencies)\n",
    "                freq_desc = 'very_low_0_0.1'\n",
    "            elif i == 1:  # Detail level 6\n",
    "                freq_desc = 'low_0.1_0.2'\n",
    "            elif i == 2:  # Detail level 5\n",
    "                freq_desc = 'low_0.2_0.4'\n",
    "            elif i == 3:  # Detail level 4\n",
    "                freq_desc = 'mid_0.4_0.8'\n",
    "            elif i == 4:  # Detail level 3\n",
    "                freq_desc = 'mid_0.8_1.6'\n",
    "            elif i == 5:  # Detail level 2\n",
    "                freq_desc = 'high_1.6_3.1'\n",
    "            elif i == 6:  # Detail level 1\n",
    "                freq_desc = 'high_3.1_6.25'\n",
    "            else:\n",
    "                freq_desc = f'level_{i}'\n",
    "\n",
    "            energies.append((energy, freq_desc))\n",
    "\n",
    "        return energies\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Wavelet calculation failed: {e}\")\n",
    "        return [(0, f'level_{i}') for i in range(levels + 1)]\n",
    "\n",
    "def extract_features_from_window(window_data, patient_info, fs=25):\n",
    "    \"\"\"\n",
    "    Extract comprehensive features from a single window - optimized version with cleaned features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Find time column\n",
    "    time_col = None\n",
    "    for col in window_data.columns:\n",
    "        if 'time' in col.lower():\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    # Get signal columns (exclude time and background channels)\n",
    "    signal_cols = [col for col in window_data.columns\n",
    "                   if col != time_col and 'background' not in col.lower()]\n",
    "\n",
    "    # Add patient metadata\n",
    "    features['age'] = patient_info.get('age', 0)\n",
    "    features['weight'] = patient_info.get('weight', 0)\n",
    "    features['height'] = patient_info.get('height', 0)\n",
    "    features['sex'] = 1 if patient_info.get('sex', '').lower() == 'male' else 0\n",
    "\n",
    "    # Calculate BMI\n",
    "    if features['weight'] > 0 and features['height'] > 0:\n",
    "        features['bmi'] = features['weight'] / ((features['height'] / 100) ** 2)\n",
    "    else:\n",
    "        features['bmi'] = 0\n",
    "\n",
    "    # Define frequency ranges for efficient computation\n",
    "    freq_ranges = {\n",
    "        '0_05_0_1': [0.05, 0.1],\n",
    "        '0_1_0_5': [0.1, 0.5],\n",
    "        '0_5_2': [0.5, 2],\n",
    "        '2_5': [2, 5],\n",
    "        '5_10': [5, 10],\n",
    "        'total': [0.01, fs/2]\n",
    "    }\n",
    "\n",
    "    # Process each channel\n",
    "    for channel in signal_cols:\n",
    "        if channel in window_data.columns:\n",
    "            signal = window_data[channel].values\n",
    "\n",
    "            # Skip if signal is empty or all NaN\n",
    "            if len(signal) == 0 or np.all(np.isnan(signal)):\n",
    "                continue\n",
    "\n",
    "            # Remove NaN values\n",
    "            signal = signal[~np.isnan(signal)]\n",
    "\n",
    "            if len(signal) == 0:\n",
    "                continue\n",
    "\n",
    "            # Downsample if signal is too long (for faster processing)\n",
    "            if len(signal) > 10000:  # More than ~6 minutes at 25 Hz\n",
    "                step = len(signal) // 10000\n",
    "                signal = signal[::step]\n",
    "\n",
    "            try:\n",
    "                # Time-domain statistics (fast) - removed variance as it's duplicate of Hjorth activity\n",
    "                features[f'{channel}_mean'] = np.mean(signal)\n",
    "                features[f'{channel}_std'] = np.std(signal)\n",
    "                features[f'{channel}_rms'] = np.sqrt(np.mean(np.square(signal)))\n",
    "                features[f'{channel}_min'] = np.min(signal)\n",
    "                features[f'{channel}_max'] = np.max(signal)\n",
    "                features[f'{channel}_range'] = np.max(signal) - np.min(signal)\n",
    "                features[f'{channel}_skewness'] = stats.skew(signal)\n",
    "                features[f'{channel}_kurtosis'] = stats.kurtosis(signal)\n",
    "                features[f'{channel}_mad'] = np.mean(np.abs(signal - np.mean(signal)))\n",
    "\n",
    "                # Hjorth parameters (Activity = variance, so we keep this instead of separate variance)\n",
    "                activity, mobility, complexity = calculate_hjorth_parameters(signal)\n",
    "                features[f'{channel}_hjorth_activity'] = activity  # This is the variance\n",
    "                features[f'{channel}_hjorth_mobility'] = mobility\n",
    "                features[f'{channel}_hjorth_complexity'] = complexity\n",
    "\n",
    "                # Frequency-domain features (efficient batch computation)\n",
    "                bandpowers = calculate_bandpower_efficient(signal, fs, freq_ranges)\n",
    "                for band_name, power in bandpowers.items():\n",
    "                    features[f'{channel}_bandpower_{band_name}'] = power\n",
    "\n",
    "                # Wavelet energies with frequency band descriptions\n",
    "                wavelet_energies = calculate_wavelet_energies(signal, fs)\n",
    "                for i, (energy, freq_desc) in enumerate(wavelet_energies):\n",
    "                    features[f'{channel}_wavelet_energy_{freq_desc}'] = energy\n",
    "\n",
    "                # Entropy measures (only Shannon entropy - removed approximate entropy)\n",
    "                shannon_ent = calculate_shannon_entropy(signal)\n",
    "                features[f'{channel}_shannon_entropy'] = shannon_ent\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Feature extraction failed for channel {channel}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return features\n",
    "\n",
    "def create_windows(df_signal, window_size_min=1, overlap_min=0.25, fs=25):\n",
    "    \"\"\"\n",
    "    Create overlapping windows from the signal - optimized version\n",
    "    \"\"\"\n",
    "    if df_signal is None:\n",
    "        return []\n",
    "\n",
    "    # Convert to pandas for easier processing\n",
    "    df_pandas = df_signal.to_pandas()\n",
    "\n",
    "    # Find time column\n",
    "    time_col = None\n",
    "    for col in df_pandas.columns:\n",
    "        if 'time' in col.lower():\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        print(\"Error: No time column found\")\n",
    "        return []\n",
    "\n",
    "    # Calculate window parameters\n",
    "    window_size_samples = int(window_size_min * 60 * fs)  # Convert to samples\n",
    "    overlap_samples = int(overlap_min * 60 * fs)\n",
    "    step_samples = window_size_samples - overlap_samples\n",
    "\n",
    "    print(f\"Window size: {window_size_min} min ({window_size_samples} samples)\")\n",
    "    print(f\"Overlap: {overlap_min} min ({overlap_samples} samples)\")\n",
    "    print(f\"Step size: {step_samples} samples\")\n",
    "\n",
    "    windows = []\n",
    "    total_samples = len(df_pandas)\n",
    "\n",
    "\n",
    "    # Create windows\n",
    "    start_idx = 0\n",
    "\n",
    "    while start_idx + window_size_samples <= total_samples:\n",
    "        end_idx = start_idx + window_size_samples\n",
    "\n",
    "        window_data = df_pandas.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "        window_info = {\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'start_time': window_data[time_col].iloc[0],\n",
    "            'end_time': window_data[time_col].iloc[-1],\n",
    "            'data': window_data\n",
    "        }\n",
    "\n",
    "        windows.append(window_info)\n",
    "        start_idx += step_samples\n",
    "\n",
    "    print(f\"Created {len(windows)} windows from {total_samples} samples\")\n",
    "    return windows\n",
    "\n",
    "def construct_features_for_patient(df_normalized, patient_name, patients_data,\n",
    "                                 window_size_min=1, overlap_min=0.25, fs=25):\n",
    "    \"\"\"\n",
    "    Construct features for all windows of a patient\n",
    "\n",
    "    Parameters:\n",
    "    - df_normalized: polars DataFrame with normalized signal data\n",
    "    - patient_name: name of the patient\n",
    "    - patients_data: dictionary with patient information\n",
    "    - window_size_min: window size in minutes\n",
    "    - overlap_min: overlap size in minutes\n",
    "    - fs: sampling frequency\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with features for all windows\n",
    "    \"\"\"\n",
    "    print(f\"\\nConstructing features for patient: {patient_name}\")\n",
    "\n",
    "    # Create windows\n",
    "    windows = create_windows(df_normalized, window_size_min, overlap_min, fs)\n",
    "\n",
    "    if not windows:\n",
    "        print(\"No windows created\")\n",
    "        return None\n",
    "\n",
    "    # Get patient info\n",
    "    patient_info = patients_data[patient_name]\n",
    "\n",
    "    # Extract features from each window\n",
    "    all_features = []\n",
    "\n",
    "    for i, window in enumerate(tqdm(windows, desc=\"Extracting features\")):\n",
    "        window_features = extract_features_from_window(window['data'], patient_info, fs)\n",
    "\n",
    "        # Add window metadata\n",
    "        window_features['patient'] = patient_name\n",
    "        window_features['window_idx'] = i\n",
    "        window_features['start_time'] = window['start_time']\n",
    "        window_features['end_time'] = window['end_time']\n",
    "\n",
    "        all_features.append(window_features)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame(all_features)\n",
    "\n",
    "    print(f\"Extracted {len(features_df)} windows with {len(features_df.columns)} features each\")\n",
    "\n",
    "    return features_df"
   ],
   "id": "76c4221bb00a6315",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to construct the final dataset with features and targets",
   "id": "d62bf9e18bb8c829"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:09.963126Z",
     "start_time": "2025-08-11T19:10:09.918031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def construct_final_dataset(features_df, labels_df, patient_name, glucose_lag_minutes=10, glycemic_prediction_minutes=15):\n",
    "    \"\"\"\n",
    "    Construct the final dataset with features and targets.\n",
    "\n",
    "    Parameters:\n",
    "    - features_df: pandas DataFrame with extracted features\n",
    "    - labels_df: polars DataFrame with labels and events\n",
    "    - patient_name: name of the patient\n",
    "    - glucose_lag_minutes: minutes to lag glucose target (default 10 min)\n",
    "    - glycemic_prediction_minutes: minutes ahead to predict glycemic state (default 15 min)\n",
    "\n",
    "    Returns:\n",
    "    - final_df: pandas DataFrame with features and corresponding targets\n",
    "    \"\"\"\n",
    "    # Convert polars DataFrame to pandas if needed\n",
    "    if hasattr(labels_df, 'to_pandas'):\n",
    "        labels_pandas = labels_df.to_pandas()\n",
    "    else:\n",
    "        labels_pandas = labels_df\n",
    "\n",
    "    # Add patient column to labels if not present\n",
    "    if 'patient' not in labels_pandas.columns:\n",
    "        labels_pandas['patient'] = patient_name\n",
    "\n",
    "    print(f\"Features DataFrame shape: {features_df.shape}\")\n",
    "    print(f\"Labels DataFrame shape: {labels_pandas.shape}\")\n",
    "    print(f\"Features columns: {list(features_df.columns)}\")\n",
    "    print(f\"Labels columns: {list(labels_pandas.columns)}\")\n",
    "\n",
    "    # Sort labels by time for proper lagging\n",
    "    labels_pandas = labels_pandas.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    # Create lagged glucose target to correct for CGM lag\n",
    "    # CGM reading at time T represents the true glucose from 10 minutes ago\n",
    "    # So to get the \"true\" glucose at time T, we need the CGM reading from T+10 minutes\n",
    "    lag_offset = pd.Timedelta(minutes=glucose_lag_minutes)\n",
    "\n",
    "    # Create prediction offset for glycemic state prediction\n",
    "    prediction_offset = pd.Timedelta(minutes=glycemic_prediction_minutes)\n",
    "\n",
    "    # Create target_lagged_glucose by shifting the glucose values backwards to account for lag\n",
    "    if 'Glucose' in labels_pandas.columns:\n",
    "        # For each original time point, find the glucose value from 10 minutes later (lag correction)\n",
    "        target_lagged_glucose = []\n",
    "\n",
    "        # For glycemic state prediction, find glucose values 15 minutes ahead\n",
    "        future_glucose_values = []\n",
    "        future_glycemic_states = []\n",
    "\n",
    "        for idx, row in labels_pandas.iterrows():\n",
    "            current_time = row['time']\n",
    "\n",
    "            # 1. Lag-corrected glucose (10 minutes later)\n",
    "            future_time_lag = current_time + lag_offset\n",
    "            time_diffs_lag = np.abs((labels_pandas['time'] - future_time_lag).dt.total_seconds())\n",
    "            closest_idx_lag = time_diffs_lag.idxmin()\n",
    "\n",
    "            if time_diffs_lag[closest_idx_lag] <= 15 * 60:  # 15 minutes tolerance\n",
    "                target_lagged_glucose.append(labels_pandas.loc[closest_idx_lag, 'Glucose'])\n",
    "            else:\n",
    "                target_lagged_glucose.append(row['Glucose'])\n",
    "\n",
    "            # 2. Future glucose for glycemic state prediction (15 minutes ahead)\n",
    "            future_time_pred = current_time + prediction_offset\n",
    "            time_diffs_pred = np.abs((labels_pandas['time'] - future_time_pred).dt.total_seconds())\n",
    "            closest_idx_pred = time_diffs_pred.idxmin()\n",
    "\n",
    "            if time_diffs_pred[closest_idx_pred] <= 15 * 60:  # 15 minutes tolerance\n",
    "                future_glucose = labels_pandas.loc[closest_idx_pred, 'Glucose']\n",
    "                future_glucose_values.append(future_glucose)\n",
    "\n",
    "                # Classify future glucose into glycemic states\n",
    "                if pd.isna(future_glucose):\n",
    "                    future_glycemic_states.append(None)\n",
    "                elif future_glucose < 70:\n",
    "                    future_glycemic_states.append('Hypoglycemia')\n",
    "                elif future_glucose > 180:\n",
    "                    future_glycemic_states.append('Hyperglycemia')\n",
    "                else:\n",
    "                    future_glycemic_states.append('Normal')\n",
    "            else:\n",
    "                future_glucose_values.append(None)\n",
    "                future_glycemic_states.append(None)\n",
    "\n",
    "    # Create a time-based merge using nearest timestamp matching\n",
    "    final_features = []\n",
    "\n",
    "    for idx, row in features_df.iterrows():\n",
    "        window_start = row['start_time']\n",
    "        window_end = row['end_time']\n",
    "\n",
    "        # Find labels that fall within this window\n",
    "        window_labels = labels_pandas[\n",
    "            (labels_pandas['time'] >= window_start) &\n",
    "            (labels_pandas['time'] <= window_end)\n",
    "        ]\n",
    "\n",
    "        # If no labels in window, find the closest label before window end\n",
    "        if len(window_labels) == 0:\n",
    "            before_window = labels_pandas[labels_pandas['time'] <= window_end]\n",
    "            if len(before_window) > 0:\n",
    "                # Get the most recent label before window end\n",
    "                closest_label = before_window.loc[before_window['time'].idxmax()]\n",
    "                target_glucose = closest_label['Glucose'] if 'Glucose' in closest_label else None\n",
    "                metabolic_state = closest_label['state'] if 'state' in closest_label else None\n",
    "                target_lagged_glucose = closest_label['target_lagged_glucose'] if 'target_lagged_glucose' in closest_label else None\n",
    "                future_glucose = closest_label[f'future_glucose_{glycemic_prediction_minutes}min'] if f'future_glucose_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "                future_glycemic_state = closest_label[f'future_glycemic_state_{glycemic_prediction_minutes}min'] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "            else:\n",
    "                target_glucose = None\n",
    "                metabolic_state = None\n",
    "                target_lagged_glucose = None\n",
    "                future_glucose = None\n",
    "                future_glycemic_state = None\n",
    "        else:\n",
    "            # Use the first label in the window\n",
    "            target_glucose = window_labels['Glucose'].iloc[0] if 'Glucose' in window_labels.columns else None\n",
    "            metabolic_state = window_labels['state'].iloc[0] if 'state' in window_labels.columns else None\n",
    "            target_lagged_glucose = window_labels['target_lagged_glucose'].iloc[0] if 'target_lagged_glucose' in window_labels.columns else None\n",
    "            future_glucose = window_labels[f'future_glucose_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glucose_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "            future_glycemic_state = window_labels[f'future_glycemic_state_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "\n",
    "        # Create feature row with targets\n",
    "        feature_row = row.to_dict()\n",
    "        feature_row['CGM'] = target_glucose\n",
    "        feature_row[f'lagged_CGM_{glucose_lag_minutes}min'] = target_lagged_glucose\n",
    "        feature_row['metabolic_state'] = metabolic_state\n",
    "        feature_row[f'future_CGM_{glycemic_prediction_minutes}min'] = future_glucose\n",
    "        feature_row[f'glycemic_state_{glycemic_prediction_minutes}min'] = future_glycemic_state\n",
    "\n",
    "        final_features.append(feature_row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_df = pd.DataFrame(final_features)\n",
    "\n",
    "    # Reorder columns to place the glucose columns together\n",
    "    glucose_cols = ['CGM', f'lagged_CGM_{glucose_lag_minutes}min', f'future_CGM_{glycemic_prediction_minutes}min']\n",
    "    target_cols = ['metabolic_state', f'glycemic_state_{glycemic_prediction_minutes}min']\n",
    "    other_cols = [col for col in final_df.columns if col not in glucose_cols + target_cols]\n",
    "\n",
    "    # Place glucose columns after the basic window info but before other features\n",
    "    window_info_cols = ['patient', 'window_idx', 'start_time', 'end_time']\n",
    "    remaining_cols = [col for col in other_cols if col not in window_info_cols]\n",
    "\n",
    "    # Reorder: window info, glucose columns, target columns, then other features\n",
    "    new_column_order = window_info_cols + glucose_cols + target_cols + remaining_cols\n",
    "    final_df = final_df[new_column_order]\n",
    "\n",
    "    print(f\"Final dataset shape: {final_df.shape}\")\n",
    "    print(f\"CGM values available: {final_df['CGM'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Lagged CGM values available: {final_df[f'lagged_CGM_{glucose_lag_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future CGM values available: {final_df[f'future_CGM_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future glycemic state values available: {final_df[f'glycemic_state_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def construct_final_dataset_with_flags(features_df, labels_df, patient_name, glucose_lag_minutes=10, glycemic_prediction_minutes=15):\n",
    "    \"\"\"\n",
    "    Construct the final dataset with features and targets including hypo/hyper flags.\n",
    "\n",
    "    Parameters:\n",
    "    - features_df: pandas DataFrame with extracted features\n",
    "    - labels_df: polars DataFrame with labels and events\n",
    "    - patient_name: name of the patient\n",
    "    - glucose_lag_minutes: minutes to lag glucose target (default 10 min)\n",
    "    - glycemic_prediction_minutes: minutes ahead to predict glycemic state (default 15 min)\n",
    "\n",
    "    Returns:\n",
    "    - final_df: pandas DataFrame with features and corresponding targets including flags\n",
    "    \"\"\"\n",
    "    # Convert polars DataFrame to pandas if needed\n",
    "    if hasattr(labels_df, 'to_pandas'):\n",
    "        labels_pandas = labels_df.to_pandas()\n",
    "    else:\n",
    "        labels_pandas = labels_df\n",
    "\n",
    "    # Add patient column to labels if not present\n",
    "    if 'patient' not in labels_pandas.columns:\n",
    "        labels_pandas['patient'] = patient_name\n",
    "\n",
    "    print(f\"Features DataFrame shape: {features_df.shape}\")\n",
    "    print(f\"Labels DataFrame shape: {labels_pandas.shape}\")\n",
    "\n",
    "    # Sort labels by time for proper lagging\n",
    "    labels_pandas = labels_pandas.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    # Create lagged glucose target to correct for CGM lag\n",
    "    lag_offset = pd.Timedelta(minutes=glucose_lag_minutes)\n",
    "    prediction_offset = pd.Timedelta(minutes=glycemic_prediction_minutes)\n",
    "\n",
    "    # Create target_lagged_glucose by shifting the glucose values backwards to account for lag\n",
    "    if 'Glucose' in labels_pandas.columns:\n",
    "        # For each original time point, find the glucose value from 10 minutes later (lag correction)\n",
    "        target_lagged_glucose = []\n",
    "        future_glucose_values = []\n",
    "        future_glycemic_states = []\n",
    "        hypo_flags = []\n",
    "        hyper_flags = []\n",
    "\n",
    "        for idx, row in labels_pandas.iterrows():\n",
    "            current_time = row['time']\n",
    "\n",
    "            # 1. Lag-corrected glucose (10 minutes later)\n",
    "            future_time_lag = current_time + lag_offset\n",
    "            time_diffs_lag = np.abs((labels_pandas['time'] - future_time_lag).dt.total_seconds())\n",
    "            closest_idx_lag = time_diffs_lag.idxmin()\n",
    "\n",
    "            if time_diffs_lag[closest_idx_lag] <= 15 * 60:  # 15 minutes tolerance\n",
    "                target_lagged_glucose.append(labels_pandas.loc[closest_idx_lag, 'Glucose'])\n",
    "            else:\n",
    "                target_lagged_glucose.append(row['Glucose'])\n",
    "\n",
    "            # 2. Future glucose for glycemic state prediction (15 minutes ahead)\n",
    "            future_time_pred = current_time + prediction_offset\n",
    "            time_diffs_pred = np.abs((labels_pandas['time'] - future_time_pred).dt.total_seconds())\n",
    "            closest_idx_pred = time_diffs_pred.idxmin()\n",
    "\n",
    "            if time_diffs_pred[closest_idx_pred] <= 15 * 60:  # 15 minutes tolerance\n",
    "                future_glucose = labels_pandas.loc[closest_idx_pred, 'Glucose']\n",
    "                future_glucose_values.append(future_glucose)\n",
    "\n",
    "                # Classify future glucose into glycemic states\n",
    "                if pd.isna(future_glucose):\n",
    "                    future_glycemic_states.append(None)\n",
    "                elif future_glucose < 70:\n",
    "                    future_glycemic_states.append('Hypoglycemia')\n",
    "                elif future_glucose > 180:\n",
    "                    future_glycemic_states.append('Hyperglycemia')\n",
    "                else:\n",
    "                    future_glycemic_states.append('Normal')\n",
    "            else:\n",
    "                future_glucose_values.append(None)\n",
    "                future_glycemic_states.append(None)\n",
    "\n",
    "            # 3. Hypo/Hyper flags: check for glucose < 75 or > 180 within next 900 seconds (15 minutes)\n",
    "            future_time_flag = current_time + pd.Timedelta(seconds=900)  # 900 seconds = 15 minutes\n",
    "\n",
    "            # Find all glucose measurements within the next 900 seconds\n",
    "            future_window = labels_pandas[\n",
    "                (labels_pandas['time'] > current_time) &\n",
    "                (labels_pandas['time'] <= future_time_flag)\n",
    "            ]\n",
    "\n",
    "            hypo_flag = 0\n",
    "            hyper_flag = 0\n",
    "\n",
    "            if len(future_window) > 0:\n",
    "                valid_glucose = future_window['Glucose'].dropna()\n",
    "                if len(valid_glucose) > 0:\n",
    "                    # Check for hypoglycemia (< 75 mg/dL)\n",
    "                    if (valid_glucose < 75).any():\n",
    "                        hypo_flag = 1\n",
    "                    # Check for hyperglycemia (> 180 mg/dL)\n",
    "                    if (valid_glucose > 180).any():\n",
    "                        hyper_flag = 1\n",
    "\n",
    "            hypo_flags.append(hypo_flag)\n",
    "            hyper_flags.append(hyper_flag)\n",
    "\n",
    "        labels_pandas['target_lagged_glucose'] = target_lagged_glucose\n",
    "        labels_pandas[f'future_glucose_{glycemic_prediction_minutes}min'] = future_glucose_values\n",
    "        labels_pandas[f'future_glycemic_state_{glycemic_prediction_minutes}min'] = future_glycemic_states\n",
    "        labels_pandas['hypo_flag'] = hypo_flags\n",
    "        labels_pandas['hyper_flag'] = hyper_flags\n",
    "    else:\n",
    "        labels_pandas['target_lagged_glucose'] = None\n",
    "        labels_pandas[f'future_glucose_{glycemic_prediction_minutes}min'] = None\n",
    "        labels_pandas[f'future_glycemic_state_{glycemic_prediction_minutes}min'] = None\n",
    "        labels_pandas['hypo_flag'] = 0\n",
    "        labels_pandas['hyper_flag'] = 0\n",
    "\n",
    "    print(f\"Applied {glucose_lag_minutes}-minute lag correction to glucose targets\")\n",
    "    print(f\"Created {glycemic_prediction_minutes}-minute ahead glycemic state predictions\")\n",
    "    print(f\"Created hypo/hyper flags for glucose events within 900 seconds\")\n",
    "\n",
    "    # Create a time-based merge using nearest timestamp matching\n",
    "    final_features = []\n",
    "\n",
    "    for idx, row in features_df.iterrows():\n",
    "        window_start = row['start_time']\n",
    "        window_end = row['end_time']\n",
    "\n",
    "        # Find labels that fall within this window\n",
    "        window_labels = labels_pandas[\n",
    "            (labels_pandas['time'] >= window_start) &\n",
    "            (labels_pandas['time'] <= window_end)\n",
    "        ]\n",
    "\n",
    "        # If no labels in window, find the closest label before window end\n",
    "        if len(window_labels) == 0:\n",
    "            before_window = labels_pandas[labels_pandas['time'] <= window_end]\n",
    "            if len(before_window) > 0:\n",
    "                # Get the most recent label before window end\n",
    "                closest_label = before_window.loc[before_window['time'].idxmax()]\n",
    "                target_glucose = closest_label['Glucose'] if 'Glucose' in closest_label else None\n",
    "                metabolic_state = closest_label['state'] if 'state' in closest_label else None\n",
    "                target_lagged_glucose = closest_label['target_lagged_glucose'] if 'target_lagged_glucose' in closest_label else None\n",
    "                future_glucose = closest_label[f'future_glucose_{glycemic_prediction_minutes}min'] if f'future_glucose_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "                future_glycemic_state = closest_label[f'future_glycemic_state_{glycemic_prediction_minutes}min'] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in closest_label else None\n",
    "                hypo_flag = closest_label['hypo_flag'] if 'hypo_flag' in closest_label else 0\n",
    "                hyper_flag = closest_label['hyper_flag'] if 'hyper_flag' in closest_label else 0\n",
    "            else:\n",
    "                target_glucose = None\n",
    "                metabolic_state = None\n",
    "                target_lagged_glucose = None\n",
    "                future_glucose = None\n",
    "                future_glycemic_state = None\n",
    "                hypo_flag = 0\n",
    "                hyper_flag = 0\n",
    "        else:\n",
    "            # Use the first label in the window\n",
    "            target_glucose = window_labels['Glucose'].iloc[0] if 'Glucose' in window_labels.columns else None\n",
    "            metabolic_state = window_labels['state'].iloc[0] if 'state' in window_labels.columns else None\n",
    "            target_lagged_glucose = window_labels['target_lagged_glucose'].iloc[0] if 'target_lagged_glucose' in window_labels.columns else None\n",
    "            future_glucose = window_labels[f'future_glucose_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glucose_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "            future_glycemic_state = window_labels[f'future_glycemic_state_{glycemic_prediction_minutes}min'].iloc[0] if f'future_glycemic_state_{glycemic_prediction_minutes}min' in window_labels.columns else None\n",
    "            hypo_flag = window_labels['hypo_flag'].iloc[0] if 'hypo_flag' in window_labels.columns else 0\n",
    "            hyper_flag = window_labels['hyper_flag'].iloc[0] if 'hyper_flag' in window_labels.columns else 0\n",
    "\n",
    "        # Create feature row with targets\n",
    "        feature_row = row.to_dict()\n",
    "        feature_row['CGM'] = target_glucose\n",
    "        feature_row[f'lagged_CGM_{glucose_lag_minutes}min'] = target_lagged_glucose\n",
    "        feature_row['metabolic_state'] = metabolic_state\n",
    "        feature_row[f'future_CGM_{glycemic_prediction_minutes}min'] = future_glucose\n",
    "        feature_row[f'glycemic_state_{glycemic_prediction_minutes}min'] = future_glycemic_state\n",
    "        feature_row['hypo_flag'] = hypo_flag\n",
    "        feature_row['hyper_flag'] = hyper_flag\n",
    "\n",
    "        final_features.append(feature_row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_df = pd.DataFrame(final_features)\n",
    "\n",
    "    # Reorder columns to place the glucose columns together\n",
    "    glucose_cols = ['CGM', f'lagged_CGM_{glucose_lag_minutes}min', f'future_CGM_{glycemic_prediction_minutes}min']\n",
    "    target_cols = ['metabolic_state', f'glycemic_state_{glycemic_prediction_minutes}min', 'hypo_flag', 'hyper_flag']\n",
    "    other_cols = [col for col in final_df.columns if col not in glucose_cols + target_cols]\n",
    "\n",
    "    # Place glucose columns after the basic window info but before other features\n",
    "    window_info_cols = ['patient', 'window_idx', 'start_time', 'end_time']\n",
    "    remaining_cols = [col for col in other_cols if col not in window_info_cols]\n",
    "\n",
    "    # Reorder: window info, glucose columns, target columns, then other features\n",
    "    new_column_order = window_info_cols + glucose_cols + target_cols + remaining_cols\n",
    "    final_df = final_df[new_column_order]\n",
    "\n",
    "    print(f\"Final dataset shape: {final_df.shape}\")\n",
    "    print(f\"CGM values available: {final_df['CGM'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Lagged CGM values available: {final_df[f'lagged_CGM_{glucose_lag_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future CGM values available: {final_df[f'future_CGM_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Future glycemic state values available: {final_df[f'glycemic_state_{glycemic_prediction_minutes}min'].notna().sum()}/{len(final_df)}\")\n",
    "    print(f\"Hypo flags: {final_df['hypo_flag'].sum()}/{len(final_df)} ({final_df['hypo_flag'].sum()/len(final_df)*100:.1f}%)\")\n",
    "    print(f\"Hyper flags: {final_df['hyper_flag'].sum()}/{len(final_df)} ({final_df['hyper_flag'].sum()/len(final_df)*100:.1f}%)\")\n",
    "\n",
    "    return final_df"
   ],
   "id": "4221c9877c0c366a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process all \"Volunteer\" patients to generate combined features dataframe",
   "id": "dc2b0b0254bc511c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:13.487596Z",
     "start_time": "2025-08-11T19:10:13.478046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "volunteer_patients_data = {}\n",
    "\n",
    "# Display patients data to understand the structure\n",
    "print(\"Available Volunteer patients in patients_data:\")\n",
    "for patient_name in patients_data.keys():\n",
    "    if \"volunteer\" in patient_name:\n",
    "        patient_info = patients_data[patient_name]\n",
    "        print(patient_name)\n",
    "        volunteer_patients_data[patient_name] = patient_info\n"
   ],
   "id": "5779a071258c0470",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Volunteer patients in patients_data:\n",
      "volunteer_part1\n",
      "volunteer_part2\n",
      "volunteer_part3\n",
      "volunteer_part4\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the function to process \"Volunteer\" patients",
   "id": "9a6404114d1109cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:10:20.060757Z",
     "start_time": "2025-08-11T19:10:20.026661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process volunteer patients to generate combined features dataframe\n",
    "def process_volunteer_patients(volunteer_patients_data, signals_dir, labels_dir, labels_filename, output_dir):\n",
    "    \"\"\"\n",
    "    Process all Volunteer patients to generate combined features dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - volunteer_patients_data: dictionary containing patient information\n",
    "    - signals_dir: directory containing signal files\n",
    "    - labels_dir: directory containing label files\n",
    "    - labels_filename: name of the labels Excel file\n",
    "    - output_dir: directory to save results\n",
    "\n",
    "    Returns:\n",
    "    - combined_df: pandas DataFrame with features from all patients\n",
    "    \"\"\"\n",
    "    all_final_datasets = []\n",
    "    processing_summary = []\n",
    "\n",
    "    # Define glucose lag and glycemic prediction parameters\n",
    "    glucose_lag_minutes = GLUCOSE_LAG_MINUTES\n",
    "    glycemic_prediction_minutes = GLYCEMIC_PREDICTION_MINUTES\n",
    "    window_size = WINDOW_SIZE_MINUTES\n",
    "    window_overlap = WINDOW_OVERLAP_MINUTES\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROCESSING VOLUNTEER PATIENTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for patient_name in volunteer_patients_data.keys():\n",
    "        print(f\"\\n{'='*20} Processing: {patient_name} {'='*20}\")\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load downsampled signal data\n",
    "            print(f\"Step 1: Loading downsampled data for {patient_name}...\")\n",
    "            df_loaded = load_downsampled_data(patient_name, signals_dir)\n",
    "\n",
    "            if df_loaded is None:\n",
    "                print(f\"âŒ No signal data found for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - No signal data',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Step 2: Apply StandardScaler normalization\n",
    "            print(f\"Step 2: Applying StandardScaler normalization for {patient_name}...\")\n",
    "            df_normalized, channel_scalers = apply_zscore_normalization(df_loaded, patient_name)\n",
    "\n",
    "            if df_normalized is None:\n",
    "                print(f\"âŒ Normalization failed for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - Normalization error',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Step 3: Construct features\n",
    "            print(f\"Step 3: Constructing features for {patient_name}...\")\n",
    "            features_df = construct_features_for_patient(\n",
    "                df_normalized,\n",
    "                patient_name,\n",
    "                volunteer_patients_data,\n",
    "                window_size_min=window_size,\n",
    "                overlap_min=window_overlap,\n",
    "                fs=25\n",
    "            )\n",
    "\n",
    "            if features_df is None:\n",
    "                print(f\"âŒ Feature construction failed for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - Feature construction error',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Step 4: Load labels (skip for Normal patients who don't have labels)\n",
    "            df_labels = None\n",
    "            if \"Normal\" not in patient_name:\n",
    "                print(f\"Step 4: Loading labels for {patient_name}...\")\n",
    "                try:\n",
    "                    df_labels = pl.read_excel(\n",
    "                        labels_dir / labels_filename,\n",
    "                        sheet_name=patient_name,\n",
    "                        columns=[0, 1, 2, 3],\n",
    "                        schema_overrides={\n",
    "                            \"time\": pl.Datetime,\n",
    "                            \"Glucose\": pl.Float64,\n",
    "                            \"Events\": pl.Utf8,\n",
    "                            \"Remarks\": pl.Utf8\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    if len(df_labels.columns) >= 4:\n",
    "                        df_labels = df_labels.select(df_labels.columns[:4])\n",
    "\n",
    "                    # Add metabolic state column (skip, for now, for Volunteer patients)\n",
    "                    if \"Volunteer\" not in patient_name:\n",
    "                        pass\n",
    "                        # df_labels = add_metabolic_state_column(df_labels)\n",
    "\n",
    "                    print(f\"âœ… Loaded {df_labels.shape[0]} labels for {patient_name}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Could not load labels for {patient_name}: {e}\")\n",
    "                    df_labels = None\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Skipping labels for {patient_name} (Normal patient)\")\n",
    "\n",
    "            # Step 5: Construct final dataset with new target labels\n",
    "            print(f\"Step 5: Constructing final dataset for {patient_name}...\")\n",
    "            if df_labels is not None:\n",
    "                final_df = construct_final_dataset_with_flags(\n",
    "                    features_df, df_labels, patient_name,\n",
    "                    glucose_lag_minutes, glycemic_prediction_minutes\n",
    "                )\n",
    "            else:\n",
    "                # For Normal patients without labels, create a simplified final dataset\n",
    "                final_df = features_df.copy()\n",
    "                final_df['CGM'] = None\n",
    "                final_df[f'lagged_CGM_{glucose_lag_minutes}min'] = None\n",
    "                final_df['metabolic_state'] = 'Normal' if 'Normal' in patient_name else None\n",
    "                final_df[f'future_CGM_{glycemic_prediction_minutes}min'] = None\n",
    "                final_df[f'glycemic_state_{glycemic_prediction_minutes}min'] = None\n",
    "                final_df['hypo_flag'] = 0  # Normal patients don't have hypoglycemia\n",
    "                final_df['hyper_flag'] = 0  # Normal patients don't have hyperglycemia\n",
    "\n",
    "                # Reorder columns to match the expected structure\n",
    "                glucose_cols = ['CGM', f'lagged_CGM_{glucose_lag_minutes}min', f'future_CGM_{glycemic_prediction_minutes}min']\n",
    "                target_cols = ['metabolic_state', f'glycemic_state_{glycemic_prediction_minutes}min', 'hypo_flag', 'hyper_flag']\n",
    "                other_cols = [col for col in final_df.columns if col not in glucose_cols + target_cols]\n",
    "\n",
    "                window_info_cols = ['patient', 'window_idx', 'start_time', 'end_time']\n",
    "                remaining_cols = [col for col in other_cols if col not in window_info_cols]\n",
    "\n",
    "                new_column_order = window_info_cols + glucose_cols + target_cols + remaining_cols\n",
    "                final_df = final_df[new_column_order]\n",
    "\n",
    "            if final_df is not None:\n",
    "                all_final_datasets.append(final_df)\n",
    "                print(f\"âœ… Successfully processed {patient_name}\")\n",
    "                print(f\"   - Windows: {len(final_df)}\")\n",
    "                print(f\"   - Features: {len(final_df.columns)}\")\n",
    "\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Success',\n",
    "                    'features_count': len(final_df.columns),\n",
    "                    'windows_count': len(final_df)\n",
    "                })\n",
    "            else:\n",
    "                print(f\"âŒ Final dataset construction failed for {patient_name}\")\n",
    "                processing_summary.append({\n",
    "                    'patient': patient_name,\n",
    "                    'status': 'Failed - Final dataset construction error',\n",
    "                    'features_count': 0,\n",
    "                    'windows_count': 0\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {patient_name}: {str(e)}\")\n",
    "            processing_summary.append({\n",
    "                'patient': patient_name,\n",
    "                'status': f'Failed - {str(e)}',\n",
    "                'features_count': 0,\n",
    "                'windows_count': 0\n",
    "            })\n",
    "\n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "\n",
    "    # Combine all datasets\n",
    "    if all_final_datasets:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"COMBINING ALL DATASETS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        combined_df = pd.concat(all_final_datasets, ignore_index=True)\n",
    "\n",
    "        print(f\"âœ… Combined dataset created successfully!\")\n",
    "        print(f\"   - Total patients processed: {len(all_final_datasets)}\")\n",
    "        print(f\"   - Total windows: {len(combined_df)}\")\n",
    "        print(f\"   - Total features: {len(combined_df.columns)}\")\n",
    "\n",
    "        # Save combined dataset (all features)\n",
    "        combined_output_file = output_dir / \"combined_features_and_targets_volunteer1.csv\"\n",
    "        combined_df.to_csv(combined_output_file, index=False)\n",
    "        print(f\"   - Full dataset saved to: {combined_output_file}\")\n",
    "\n",
    "        # Display summary statistics\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PROCESSING SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        summary_df = pd.DataFrame(processing_summary)\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "        # Dataset statistics\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Patient distribution\n",
    "        print(f\"\\nPatient distribution:\")\n",
    "        print(combined_df['patient'].value_counts())\n",
    "\n",
    "        # Target statistics (for patients with labels)\n",
    "        if 'CGM' in combined_df.columns:\n",
    "            cgm_available = combined_df['CGM'].notna().sum()\n",
    "            print(f\"\\nCGM targets available: {cgm_available}/{len(combined_df)} ({cgm_available/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "        if 'metabolic_state' in combined_df.columns:\n",
    "            print(f\"\\nMetabolic state distribution:\")\n",
    "            print(combined_df['metabolic_state'].value_counts())\n",
    "\n",
    "        if f'glycemic_state_{glycemic_prediction_minutes}min' in combined_df.columns:\n",
    "            glycemic_available = combined_df[f'glycemic_state_{glycemic_prediction_minutes}min'].notna().sum()\n",
    "            print(f\"\\nGlycemic state targets available: {glycemic_available}/{len(combined_df)} ({glycemic_available/len(combined_df)*100:.1f}%)\")\n",
    "            if glycemic_available > 0:\n",
    "                print(f\"Glycemic state distribution:\")\n",
    "                print(combined_df[f'glycemic_state_{glycemic_prediction_minutes}min'].value_counts())\n",
    "\n",
    "        # New flag statistics\n",
    "        if 'hypo_flag' in combined_df.columns:\n",
    "            hypo_flags = combined_df['hypo_flag'].sum()\n",
    "            print(f\"\\nHypoglycemia flags (within 900s): {hypo_flags}/{len(combined_df)} ({hypo_flags/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "        if 'hyper_flag' in combined_df.columns:\n",
    "            hyper_flags = combined_df['hyper_flag'].sum()\n",
    "            print(f\"Hyperglycemia flags (within 900s): {hyper_flags}/{len(combined_df)} ({hyper_flags/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"âŒ No datasets were successfully processed!\")\n",
    "        return None"
   ],
   "id": "69c1cd38b1187ed6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Execute the feature construction pipeline for all Volunteer patients",
   "id": "7812dd942de37da2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T19:11:01.793879Z",
     "start_time": "2025-08-11T19:10:21.717375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Execute the special case feature construction pipeline\n",
    "print(\"Starting the special case feature construction pipeline...\")\n",
    "\n",
    "# Process all patients and create the combined volunteer features dataset\n",
    "volunteer_features_df = process_volunteer_patients(\n",
    "    volunteer_patients_data=volunteer_patients_data,\n",
    "    signals_dir=signals_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    labels_filename=labels_filename,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "if volunteer_features_df is not None:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FEATURE CONSTRUCTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  ðŸ“Š Shape: {volunteer_features_df.shape}\")\n",
    "    print(f\"  ðŸ‘¥ Patients: {volunteer_features_df['patient'].nunique()}\")\n",
    "    print(f\"  ðŸªŸ Windows: {len(volunteer_features_df)}\")\n",
    "    print(f\"  ðŸ“ˆ Features: {len(volunteer_features_df.columns)}\")\n",
    "\n",
    "    # Display first few rows info\n",
    "    print(f\"\\nColumn overview:\")\n",
    "    print(f\"  - Metadata columns: {['patient', 'window_idx', 'start_time', 'end_time']}\")\n",
    "    print(f\"  - Target columns: {[col for col in volunteer_features_df.columns if 'CGM' in col or 'state' in col or 'flag' in col]}\")\n",
    "    print(f\"  - Feature columns: {len([col for col in volunteer_features_df.columns if col not in ['patient', 'window_idx', 'start_time', 'end_time'] and 'CGM' not in col and 'state' not in col and 'flag' not in col])}\")\n",
    "\n",
    "    print(f\"\\nâœ… Combined features dataset is now available as 'volunteer_features_df'\")\n",
    "    print(f\"âœ… Feature construction pipeline completed successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Feature construction pipeline failed!\")\n",
    "    print(\"Please check the error messages above and ensure:\")\n",
    "    print(\"  - Signal files are available in the signals_dir\")\n",
    "    print(\"  - Patient data is correctly loaded\")\n",
    "    print(\"  - Directory permissions are correct\")\n"
   ],
   "id": "400307221acfc3c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the special case feature construction pipeline...\n",
      "============================================================\n",
      "PROCESSING VOLUNTEER PATIENTS\n",
      "============================================================\n",
      "\n",
      "==================== Processing: volunteer_part1 ====================\n",
      "Step 1: Loading downsampled data for volunteer_part1...\n",
      "Successfully loaded data from volunteer_part1_downsampled_25hz.parquet\n",
      "Loaded data shape: (612512, 3)\n",
      "Step 2: Applying StandardScaler normalization for volunteer_part1...\n",
      "Applying z-score normalization for volunteer_part1\n",
      "Processing 2 signal channels\n",
      "Channel Hand1: mean=-0.1187, std=27.9219\n",
      "Channel Hand2: mean=-0.0934, std=27.7747\n",
      "Z-score normalization completed for 2 channels\n",
      "Step 3: Constructing features for volunteer_part1...\n",
      "\n",
      "Constructing features for patient: volunteer_part1\n",
      "Window size: 1 min (1500 samples)\n",
      "Overlap: 0.25 min (375 samples)\n",
      "Step size: 1125 samples\n",
      "Created 544 windows from 612512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 544/544 [00:06<00:00, 90.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 544 windows with 61 features each\n",
      "Step 4: Loading labels for volunteer_part1...\n",
      "âœ… Loaded 215 labels for volunteer_part1\n",
      "Step 5: Constructing final dataset for volunteer_part1...\n",
      "Features DataFrame shape: (544, 61)\n",
      "Labels DataFrame shape: (215, 5)\n",
      "Applied 10-minute lag correction to glucose targets\n",
      "Created 15-minute ahead glycemic state predictions\n",
      "Created hypo/hyper flags for glucose events within 900 seconds\n",
      "Final dataset shape: (544, 68)\n",
      "CGM values available: 544/544\n",
      "Lagged CGM values available: 544/544\n",
      "Future CGM values available: 544/544\n",
      "Future glycemic state values available: 544/544\n",
      "Hypo flags: 56/544 (10.3%)\n",
      "Hyper flags: 25/544 (4.6%)\n",
      "âœ… Successfully processed volunteer_part1\n",
      "   - Windows: 544\n",
      "   - Features: 68\n",
      "\n",
      "==================== Processing: volunteer_part2 ====================\n",
      "Step 1: Loading downsampled data for volunteer_part2...\n",
      "Successfully loaded data from volunteer_part2_downsampled_25hz.parquet\n",
      "Loaded data shape: (656312, 3)\n",
      "Step 2: Applying StandardScaler normalization for volunteer_part2...\n",
      "Applying z-score normalization for volunteer_part2\n",
      "Processing 2 signal channels\n",
      "Channel Hand1: mean=-0.2675, std=34.3208\n",
      "Channel Hand2: mean=-0.2533, std=32.9548\n",
      "Z-score normalization completed for 2 channels\n",
      "Step 3: Constructing features for volunteer_part2...\n",
      "\n",
      "Constructing features for patient: volunteer_part2\n",
      "Window size: 1 min (1500 samples)\n",
      "Overlap: 0.25 min (375 samples)\n",
      "Step size: 1125 samples\n",
      "Created 583 windows from 656312 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 583/583 [00:06<00:00, 96.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 583 windows with 61 features each\n",
      "Step 4: Loading labels for volunteer_part2...\n",
      "âœ… Loaded 205 labels for volunteer_part2\n",
      "Step 5: Constructing final dataset for volunteer_part2...\n",
      "Features DataFrame shape: (583, 61)\n",
      "Labels DataFrame shape: (205, 5)\n",
      "Applied 10-minute lag correction to glucose targets\n",
      "Created 15-minute ahead glycemic state predictions\n",
      "Created hypo/hyper flags for glucose events within 900 seconds\n",
      "Final dataset shape: (583, 68)\n",
      "CGM values available: 583/583\n",
      "Lagged CGM values available: 583/583\n",
      "Future CGM values available: 583/583\n",
      "Future glycemic state values available: 583/583\n",
      "Hypo flags: 112/583 (19.2%)\n",
      "Hyper flags: 24/583 (4.1%)\n",
      "âœ… Successfully processed volunteer_part2\n",
      "   - Windows: 583\n",
      "   - Features: 68\n",
      "\n",
      "==================== Processing: volunteer_part3 ====================\n",
      "Step 1: Loading downsampled data for volunteer_part3...\n",
      "Successfully loaded data from volunteer_part3_downsampled_25hz.parquet\n",
      "Loaded data shape: (872087, 3)\n",
      "Step 2: Applying StandardScaler normalization for volunteer_part3...\n",
      "Applying z-score normalization for volunteer_part3\n",
      "Processing 2 signal channels\n",
      "Channel Hand1: mean=0.0932, std=34.6691\n",
      "Channel Hand2: mean=-0.0142, std=33.5811\n",
      "Z-score normalization completed for 2 channels\n",
      "Step 3: Constructing features for volunteer_part3...\n",
      "\n",
      "Constructing features for patient: volunteer_part3\n",
      "Window size: 1 min (1500 samples)\n",
      "Overlap: 0.25 min (375 samples)\n",
      "Step size: 1125 samples\n",
      "Created 774 windows from 872087 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 774/774 [00:08<00:00, 89.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 774 windows with 61 features each\n",
      "Step 4: Loading labels for volunteer_part3...\n",
      "âœ… Loaded 242 labels for volunteer_part3\n",
      "Step 5: Constructing final dataset for volunteer_part3...\n",
      "Features DataFrame shape: (774, 61)\n",
      "Labels DataFrame shape: (242, 5)\n",
      "Applied 10-minute lag correction to glucose targets\n",
      "Created 15-minute ahead glycemic state predictions\n",
      "Created hypo/hyper flags for glucose events within 900 seconds\n",
      "Final dataset shape: (774, 68)\n",
      "CGM values available: 774/774\n",
      "Lagged CGM values available: 774/774\n",
      "Future CGM values available: 774/774\n",
      "Future glycemic state values available: 774/774\n",
      "Hypo flags: 23/774 (3.0%)\n",
      "Hyper flags: 184/774 (23.8%)\n",
      "âœ… Successfully processed volunteer_part3\n",
      "   - Windows: 774\n",
      "   - Features: 68\n",
      "\n",
      "==================== Processing: volunteer_part4 ====================\n",
      "Step 1: Loading downsampled data for volunteer_part4...\n",
      "Successfully loaded data from volunteer_part4_downsampled_25hz.parquet\n",
      "Loaded data shape: (716200, 3)\n",
      "Step 2: Applying StandardScaler normalization for volunteer_part4...\n",
      "Applying z-score normalization for volunteer_part4\n",
      "Processing 2 signal channels\n",
      "Channel Hand1: mean=0.2511, std=42.3888\n",
      "Channel Hand2: mean=0.0100, std=10.1067\n",
      "Z-score normalization completed for 2 channels\n",
      "Step 3: Constructing features for volunteer_part4...\n",
      "\n",
      "Constructing features for patient: volunteer_part4\n",
      "Window size: 1 min (1500 samples)\n",
      "Overlap: 0.25 min (375 samples)\n",
      "Step size: 1125 samples\n",
      "Created 636 windows from 716200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636/636 [00:08<00:00, 78.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 636 windows with 61 features each\n",
      "Step 4: Loading labels for volunteer_part4...\n",
      "âœ… Loaded 242 labels for volunteer_part4\n",
      "Step 5: Constructing final dataset for volunteer_part4...\n",
      "Features DataFrame shape: (636, 61)\n",
      "Labels DataFrame shape: (242, 5)\n",
      "Applied 10-minute lag correction to glucose targets\n",
      "Created 15-minute ahead glycemic state predictions\n",
      "Created hypo/hyper flags for glucose events within 900 seconds\n",
      "Final dataset shape: (636, 68)\n",
      "CGM values available: 636/636\n",
      "Lagged CGM values available: 636/636\n",
      "Future CGM values available: 636/636\n",
      "Future glycemic state values available: 636/636\n",
      "Hypo flags: 27/636 (4.2%)\n",
      "Hyper flags: 23/636 (3.6%)\n",
      "âœ… Successfully processed volunteer_part4\n",
      "   - Windows: 636\n",
      "   - Features: 68\n",
      "\n",
      "============================================================\n",
      "COMBINING ALL DATASETS\n",
      "============================================================\n",
      "âœ… Combined dataset created successfully!\n",
      "   - Total patients processed: 4\n",
      "   - Total windows: 2537\n",
      "   - Total features: 68\n",
      "   - Full dataset saved to: ..\\..\\..\\Data\\ProcessedData\\combined_features_and_targets_volunteer1.csv\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "        patient  status  features_count  windows_count\n",
      "volunteer_part1 Success              68            544\n",
      "volunteer_part2 Success              68            583\n",
      "volunteer_part3 Success              68            774\n",
      "volunteer_part4 Success              68            636\n",
      "\n",
      "============================================================\n",
      "DATASET STATISTICS\n",
      "============================================================\n",
      "\n",
      "Patient distribution:\n",
      "patient\n",
      "volunteer_part3    774\n",
      "volunteer_part4    636\n",
      "volunteer_part2    583\n",
      "volunteer_part1    544\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CGM targets available: 2537/2537 (100.0%)\n",
      "\n",
      "Metabolic state distribution:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Glycemic state targets available: 2537/2537 (100.0%)\n",
      "Glycemic state distribution:\n",
      "glycemic_state_15min\n",
      "Normal           1446\n",
      "Hypoglycemia      595\n",
      "Hyperglycemia     496\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Hypoglycemia flags (within 900s): 218/2537 (8.6%)\n",
      "Hyperglycemia flags (within 900s): 256/2537 (10.1%)\n",
      "\n",
      "================================================================================\n",
      "FEATURE CONSTRUCTION PIPELINE COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Dataset Overview:\n",
      "  ðŸ“Š Shape: (2537, 68)\n",
      "  ðŸ‘¥ Patients: 4\n",
      "  ðŸªŸ Windows: 2537\n",
      "  ðŸ“ˆ Features: 68\n",
      "\n",
      "Column overview:\n",
      "  - Metadata columns: ['patient', 'window_idx', 'start_time', 'end_time']\n",
      "  - Target columns: ['CGM', 'lagged_CGM_10min', 'future_CGM_15min', 'metabolic_state', 'glycemic_state_15min', 'hypo_flag', 'hyper_flag']\n",
      "  - Feature columns: 57\n",
      "\n",
      "âœ… Combined features dataset is now available as 'volunteer_features_df'\n",
      "âœ… Feature construction pipeline completed successfully!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gc.collect()",
   "id": "387a8e60605a5c21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e6c47e018c22e357",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
