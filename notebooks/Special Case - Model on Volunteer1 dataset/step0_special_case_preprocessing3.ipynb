{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TDMS Signal Preprocessing Pipeline for Volunteer Data\n",
    "\n",
    "This notebook processes a special case multi-part TDMS sensor data from a volunteer patient, handling large files with memory optimization and preparing data for analysis.\n",
    "\n",
    "**Key Operations:**\n",
    "- **Data Integration**: Loads and combines 4 separate TDMS files from different recording dates (Nov-Dec 2021)\n",
    "- **Signal Processing**: Converts voltage measurements to nanoTesla units, applies band-pass filtering (0.05-10 Hz), and downsamples from 5000 Hz to 25 Hz\n",
    "- **Memory Management**: Uses chunked reading and garbage collection for large file handling\n",
    "- **Period Segmentation**: Splits combined data into analysis periods (Nov 16-17, Dec 7-8, Dec 14-15 and Dec 15-16) for focused analysis\n",
    "- **Output Generation**: Saves period-specific datasets as parquet files for downstream analysis\n",
    "\n",
    "The pipeline includes saturation detection, anti-aliasing filters, and comprehensive error handling to ensure robust data preprocessing for magnetometer sensor analysis."
   ],
   "id": "eb04984b17e33eed"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-11T16:40:16.768530Z",
     "start_time": "2025-08-11T16:40:14.190014Z"
    }
   },
   "source": [
    "# General imports:\n",
    "\n",
    "# Disable warnings:\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Essential imports\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add TDMS reading functionality\n",
    "from nptdms import TdmsFile\n",
    "\n",
    "# Add signal processing imports for antialiasing filter\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Plotting enhancements\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:40:17.990065Z",
     "start_time": "2025-08-11T16:40:17.980538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Key Settings:\n",
    "Visualize_Signal = True # Signal visualization (only if required)\n",
    "\n",
    "# Physical constants and sensors specifications:\n",
    "SENSITIVITY = 50  # mV/nT\n",
    "MAGNETIC_NOISE = 3  # pT/√Hz @ 1 Hz\n",
    "MAX_AC_LINEARITY = 250  # nT (+/- 250 nT) - Equivalent to 21.78 V\n",
    "MAX_DC_LINEARITY = 60  # nT (+/- 60 nT)\n",
    "VOLTAGE_LIMIT = 15 # V (+/-15V)\n",
    "CONVERSION_FACTOR = 20  # nT per 1V\n",
    "SAMPLING_FREQUENCY = 5000  # Hz - expected from the experimental data\n",
    "SENSOR_SATURATION = 250  # nT - saturation threshold for the sensor\n",
    "\n",
    "# Subjects and their types # Not relevant for this notebook\n",
    "# Subject = {\"Normal\": \"Normal Subjects\",\"Clamp\": \"T1DM Clamp Subjects\", \"Additional\": \"Additional Subjects\"}\n",
    "\n",
    "# Path and directories\n",
    "base_dir = Path(\"../../../Data\")\n",
    "\n",
    "# Output directory for saving results\n",
    "output_dir = base_dir / \"ProcessedData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Directory for saving processed/downsampled signal files (parquet format)\n",
    "signals_dir = output_dir / \"Signal_Files\"\n",
    "os.makedirs(signals_dir, exist_ok=True)\n",
    "\n",
    "# GMT zone correction for the sensor = GMT+2\n",
    "GMT = 2\n",
    "\n",
    "# Key frequencies from background noise analysis\n",
    "POWER_LINE_FREQ = 50  # Hz\n",
    "POWERLINE_HARMONICS = [POWER_LINE_FREQ*i for i in range(1, 4)]  # 50, 100, 150 Hz.\n",
    "\n",
    "# Filter parameters:\n",
    "HIGHCUT_FREQ = 10  # Hz low-pass filter cutoff frequency\n",
    "LOWCUT_FREQ = 0.05 # Hz high-pass filter cutoff frequency\n",
    "FILTER_ORDER = 6 # for steeper roll-off\n",
    "\n",
    "# Channel grouping\n",
    "signal_channels = {\n",
    "    'Head': ['Head_left', 'Head_right'],\n",
    "    'Hand': ['Hand1', 'Hand2'],\n",
    "    'Liver': ['Liver1', 'Liver2'],\n",
    "    'Background': ['Background1', 'Background2']\n",
    "}\n",
    "\n",
    "input_file_set = {'part1': base_dir / \"RawData/Volunteers/2021/2021_11_16/fruit_1.tdms\",\n",
    "                 'part2': base_dir / \"RawData/Volunteers/2021/2021_12_07/fruit_1.tdms\",\n",
    "                 'part3': base_dir / \"RawData/Volunteers/2021/2021_12_14/fruit_1.tdms\",\n",
    "                 'part4': base_dir / \"RawData/Volunteers/2021/2021_12_15/fruit_9.tdms\",\n",
    "                 'background': base_dir / \"RawData/Volunteers/2021/2021_12_03_background\"}"
   ],
   "id": "7ada2aa134b045d3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:40:21.765526Z",
     "start_time": "2025-08-11T16:40:21.756392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_voltage_to_nanotesla_flexible(df, conversion_factor, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Convert raw voltage signals to nanoTesla (nT) for ALL columns except specified exclusions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with voltage signal data\n",
    "    - conversion_factor: conversion factor in nT per Volt (default: 20 nT/V)\n",
    "    - exclude_columns: list of column names to exclude from conversion (e.g., ['Time'])\n",
    "\n",
    "    Returns:\n",
    "    - df_converted: polars DataFrame with signals converted to nT\n",
    "    \"\"\"\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "\n",
    "    # Make exclude_columns case-insensitive\n",
    "    exclude_columns_lower = [col.lower() for col in exclude_columns]\n",
    "\n",
    "    # Find all columns that are NOT in the exclude list\n",
    "    signal_columns = []\n",
    "    time_columns = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col.lower() in exclude_columns_lower or col.lower() == 'time':\n",
    "            time_columns.append(col)\n",
    "            print(f\"Preserving column '{col}' (not converting)\")\n",
    "        else:\n",
    "            signal_columns.append(col)\n",
    "\n",
    "    print(f\"Found {len(signal_columns)} signal channels to convert: {signal_columns}\")\n",
    "    print(f\"Found {len(time_columns)} non-signal columns to preserve: {time_columns}\")\n",
    "\n",
    "    # Apply conversion to each signal channel\n",
    "    converted_data = {}\n",
    "\n",
    "    # Keep non-signal columns unchanged\n",
    "    for col in time_columns:\n",
    "        converted_data[col] = df[col].to_numpy()\n",
    "\n",
    "    print(f\"Converting voltage signals to nanoTesla using factor: {conversion_factor} nT/V\")\n",
    "\n",
    "    for channel in tqdm(signal_columns, desc=\"Converting channels\"):\n",
    "        if channel in df.columns:\n",
    "            # Extract voltage signal data\n",
    "            voltage_signal = df[channel].to_numpy()\n",
    "\n",
    "            # Convert to nanoTesla: nT = V × conversion_factor\n",
    "            nanotesla_signal = voltage_signal * conversion_factor\n",
    "\n",
    "            # Store converted signal\n",
    "            converted_data[channel] = nanotesla_signal\n",
    "        else:\n",
    "            print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "    # Convert back to polars DataFrame\n",
    "    df_converted = pl.DataFrame(converted_data)\n",
    "\n",
    "    print(f\"Voltage to nanoTesla conversion completed. Converted {len(signal_columns)} channels.\")\n",
    "    print(f\"Signal values are now in nanoTesla (nT) units.\")\n",
    "\n",
    "    return df_converted"
   ],
   "id": "4f194ffcf9c90601",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:40:23.487480Z",
     "start_time": "2025-08-11T16:40:23.468581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_tdms_file_improved(tdms_path, GMT=2, chunk_size=500000, downsample_factor=None, memory_threshold_mb=500):\n",
    "    \"\"\"\n",
    "    Improved TDMS file reader with memory management and chunked processing.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    tdms_path : str or Path\n",
    "        Path to the TDMS file\n",
    "    GMT : int, default=2\n",
    "        GMT offset in hours to apply to the Time column\n",
    "    chunk_size : int, default=500000\n",
    "        Number of samples to read at a time for large files\n",
    "    downsample_factor : int, optional\n",
    "        Factor by which to downsample the data (e.g., 2 means keep every 2nd sample)\n",
    "    memory_threshold_mb : int, default=500\n",
    "        Memory threshold in MB above which chunked reading is used\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pl.DataFrame or None\n",
    "        Polars DataFrame containing the TDMS data, or None if reading fails\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tdms_path) or not str(tdms_path).lower().endswith('.tdms'):\n",
    "        print(f\"Invalid file path or not a TDMS file: {tdms_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        print(f\"Reading {tdms_path}...\")\n",
    "\n",
    "        # Read the TDMS file header first to get metadata\n",
    "        tdms_file = TdmsFile.read(tdms_path)\n",
    "\n",
    "        # Check if the 'Untitled' group exists\n",
    "        if 'Untitled' not in tdms_file:\n",
    "            # Get the available groups\n",
    "            groups = list(tdms_file.groups())\n",
    "            if not groups:\n",
    "                print(f\"Warning: No groups found in {tdms_path}\")\n",
    "                return None\n",
    "\n",
    "            # Use the first available group\n",
    "            group = groups[0]\n",
    "            print(f\"Using group '{group.name}' instead of 'Untitled'\")\n",
    "        else:\n",
    "            group = tdms_file['Untitled']\n",
    "\n",
    "        # Get channel information and data length\n",
    "        channels = list(group.channels())\n",
    "        if not channels:\n",
    "            print(f\"Warning: No channels found in group\")\n",
    "            return None\n",
    "\n",
    "        # Get the length of data from the first channel\n",
    "        first_channel = channels[0]\n",
    "        total_length = len(first_channel)\n",
    "        print(f\"Total data length: {total_length:,} samples\")\n",
    "\n",
    "        # Calculate memory requirements\n",
    "        estimated_memory_mb = (total_length * len(channels) * 8) / (1024 * 1024)  # 8 bytes per float64\n",
    "        print(f\"Estimated memory requirement: {estimated_memory_mb:.1f} MB\")\n",
    "\n",
    "        if estimated_memory_mb > memory_threshold_mb:\n",
    "            print(\"Large file detected, using chunked reading approach...\")\n",
    "            return _read_tdms_chunked_local(group, GMT, chunk_size, downsample_factor, total_length)\n",
    "        else:\n",
    "            # For smaller files, use the standard approach with optimizations\n",
    "            return _read_tdms_standard_local(group, GMT, downsample_factor)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing TDMS file {tdms_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _read_tdms_standard_local(group, GMT, downsample_factor=None):\n",
    "    \"\"\"Standard reading approach for smaller files\"\"\"\n",
    "    try:\n",
    "        data = {}\n",
    "\n",
    "        for channel in group.channels():\n",
    "            channel_data = channel[:]\n",
    "\n",
    "            # Apply downsampling if specified\n",
    "            if downsample_factor and downsample_factor > 1:\n",
    "                channel_data = channel_data[::downsample_factor]\n",
    "\n",
    "            # Use appropriate data type to save memory\n",
    "            if channel.name.lower() == 'time':\n",
    "                data[channel.name] = channel_data\n",
    "            else:\n",
    "                # Convert to float32 to save memory (vs default float64)\n",
    "                data[channel.name] = channel_data.astype(np.float32)\n",
    "\n",
    "        # Create a Polars DataFrame\n",
    "        df = pl.DataFrame(data)\n",
    "\n",
    "        # If the DataFrame has a 'Time' column, adjust it by GMT offset\n",
    "        if 'Time' in df.columns:\n",
    "            try:\n",
    "                df = df.with_columns([pl.col('Time') + pl.duration(hours=GMT)])\n",
    "                print(f\"Adjusted 'Time' column by GMT+{GMT} hours\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not adjust 'Time' column: {e}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in standard reading: {e}\")\n",
    "        return None\n",
    "\n",
    "def _read_tdms_chunked_local(group, GMT, chunk_size, downsample_factor, total_length):\n",
    "    \"\"\"Chunked reading approach for large files\"\"\"\n",
    "    try:\n",
    "        channels = list(group.channels())\n",
    "\n",
    "        if downsample_factor and downsample_factor > 1:\n",
    "            print(f\"Applying downsampling factor: {downsample_factor}\")\n",
    "            effective_chunk_size = chunk_size * downsample_factor\n",
    "        else:\n",
    "            effective_chunk_size = chunk_size\n",
    "\n",
    "        # Process data in chunks\n",
    "        chunk_dfs = []\n",
    "\n",
    "        for start_idx in range(0, total_length, effective_chunk_size):\n",
    "            end_idx = min(start_idx + effective_chunk_size, total_length)\n",
    "            progress = (end_idx / total_length) * 100\n",
    "            print(f\"Processing chunk {start_idx:,} to {end_idx:,} ({progress:.1f}%)\")\n",
    "\n",
    "            chunk_data = {}\n",
    "\n",
    "            for channel in channels:\n",
    "                try:\n",
    "                    channel_data = channel[start_idx:end_idx]\n",
    "\n",
    "                    # Apply downsampling if specified\n",
    "                    if downsample_factor and downsample_factor > 1:\n",
    "                        channel_data = channel_data[::downsample_factor]\n",
    "\n",
    "                    # Use appropriate data type\n",
    "                    if channel.name.lower() == 'time':\n",
    "                        chunk_data[channel.name] = channel_data\n",
    "                    else:\n",
    "                        chunk_data[channel.name] = channel_data.astype(np.float32)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error reading channel {channel.name} in chunk: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if chunk_data:  # Only create DataFrame if we have data\n",
    "                chunk_df = pl.DataFrame(chunk_data)\n",
    "                chunk_dfs.append(chunk_df)\n",
    "\n",
    "            # Force garbage collection to free memory\n",
    "            gc.collect()\n",
    "\n",
    "        if not chunk_dfs:\n",
    "            print(\"No valid chunks were processed\")\n",
    "            return None\n",
    "\n",
    "        # Concatenate all chunks\n",
    "        print(\"Combining chunks...\")\n",
    "        df = pl.concat(chunk_dfs, how='vertical')\n",
    "\n",
    "        # Clear chunk list to free memory\n",
    "        del chunk_dfs\n",
    "        gc.collect()\n",
    "\n",
    "        # If the DataFrame has a 'Time' column, adjust it by GMT offset\n",
    "        if 'Time' in df.columns:\n",
    "            try:\n",
    "                df = df.with_columns([pl.col('Time') + pl.duration(hours=GMT)])\n",
    "                print(f\"Adjusted 'Time' column by GMT+{GMT} hours\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not adjust 'Time' column: {e}\")\n",
    "\n",
    "        print(f\"Successfully processed large file with chunked reading\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chunked reading: {e}\")\n",
    "        return None\n"
   ],
   "id": "abb85eab189f63df",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:59:34.386169Z",
     "start_time": "2025-08-11T16:47:21.561208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process each part of the TDMS file set and save them as individual parquet files\n",
    "processed_files = {}  # Track successfully processed files\n",
    "\n",
    "# Filter out the background file for processing\n",
    "parts_to_process = {k: v for k, v in input_file_set.items() if k != 'background'}\n",
    "\n",
    "# Parameters for handling large files\n",
    "CHUNK_SIZE = 500000  # Reduced chunk size for better memory management\n",
    "DOWNSAMPLE_FACTOR = 2  # Downsample by factor of 2 if files are too large (optional)\n",
    "MEMORY_THRESHOLD = 300  # Use chunked reading for files estimated to use more than 300MB\n",
    "\n",
    "for part_name, path in parts_to_process.items():\n",
    "    print(f\"\\nProcessing {part_name} file: {path}\")\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File {path} does not exist. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Get file size to estimate if downsampling might be needed\n",
    "    file_size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    # Determine if downsampling is needed based on file size\n",
    "    use_downsampling = file_size_mb > 100  # Use downsampling for files larger than 100MB\n",
    "    downsample_factor = DOWNSAMPLE_FACTOR if use_downsampling else None\n",
    "\n",
    "    if use_downsampling:\n",
    "        print(f\"Large file detected. Will apply downsampling factor: {downsample_factor}\")\n",
    "\n",
    "    # Read the TDMS file using the improved function\n",
    "    try:\n",
    "        df = read_tdms_file_improved(\n",
    "            str(path),\n",
    "            GMT=GMT,\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            downsample_factor=downsample_factor,\n",
    "            memory_threshold_mb=MEMORY_THRESHOLD\n",
    "        )\n",
    "\n",
    "        if df is None:\n",
    "            print(f\"Failed to load data from {part_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Successfully loaded data with {df.shape[0]:,} samples and {df.shape[1]} channels\")\n",
    "\n",
    "        # Display all available channels in this file\n",
    "        print(f\"All channels found in {part_name}: {df.columns}\")\n",
    "\n",
    "        # Separate time and signal channels\n",
    "        time_cols = [col for col in df.columns if col.lower() == 'time']\n",
    "        signal_cols = [col for col in df.columns if col.lower() != 'time']\n",
    "\n",
    "        print(f\"Time columns: {time_cols}\")\n",
    "        print(f\"Signal channels ({len(signal_cols)}): {signal_cols}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TDMS file for {part_name}: {e}\")\n",
    "        # Force garbage collection to free memory before continuing\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    # Convert voltage signals to nanoTesla using the flexible function\n",
    "    try:\n",
    "        df = convert_voltage_to_nanotesla_flexible(df, CONVERSION_FACTOR)\n",
    "        print(f\"Voltage conversion completed for {part_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during voltage conversion for {part_name}: {e}\")\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    # Save individual parquet file for this TDMS part\n",
    "    try:\n",
    "        output_file = signals_dir / f\"volunteer_{part_name}.parquet\"\n",
    "        print(f\"Saving {part_name} data to {output_file}...\")\n",
    "        df.write_parquet(str(output_file))\n",
    "        print(f\"Successfully saved {df.shape[0]:,} samples from {part_name} to {output_file}\")\n",
    "\n",
    "        # Print summary statistics for this file\n",
    "        print(f\"DataFrame shape for {part_name}: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns}\")\n",
    "\n",
    "        # Print time range for this file\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            time_min = df[time_col].min()\n",
    "            time_max = df[time_col].max()\n",
    "            duration_hours = (time_max - time_min).total_seconds() / 3600\n",
    "            print(f\"Time range for {part_name}: {time_min} to {time_max}\")\n",
    "            print(f\"Duration: {duration_hours:.2f} hours\")\n",
    "\n",
    "        # Calculate file size\n",
    "        if output_file.exists():\n",
    "            output_size_mb = output_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Output file size for {part_name}: {output_size_mb:.1f} MB\")\n",
    "\n",
    "        # Track successful processing\n",
    "        processed_files[part_name] = {\n",
    "            'file_path': output_file,\n",
    "            'shape': df.shape,\n",
    "            'time_range': (time_min, time_max) if time_cols else None,\n",
    "            'duration_hours': duration_hours if time_cols else None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {part_name} data: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Clear the current DataFrame to free memory\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "# Print summary of all processed files\n",
    "if processed_files:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Successfully processed {len(processed_files)} TDMS files:\")\n",
    "\n",
    "    for part_name, info in processed_files.items():\n",
    "        print(f\"\\n{part_name}:\")\n",
    "        print(f\"  - File: {info['file_path']}\")\n",
    "        print(f\"  - Shape: {info['shape']}\")\n",
    "        if info['time_range']:\n",
    "            print(f\"  - Time range: {info['time_range'][0]} to {info['time_range'][1]}\")\n",
    "            print(f\"  - Duration: {info['duration_hours']:.2f} hours\")\n",
    "else:\n",
    "    print(\"No data was successfully processed. No output files created.\")\n",
    "\n",
    "print(\"\\nIndividual TDMS processing completed!\")"
   ],
   "id": "de94fb73d94bac9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing part1 file: ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_11_16\\fruit_1.tdms\n",
      "File size: 7486.1 MB\n",
      "Large file detected. Will apply downsampling factor: 2\n",
      "Reading ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_11_16\\fruit_1.tdms...\n",
      "Total data length: 245,005,000 samples\n",
      "Estimated memory requirement: 5607.7 MB\n",
      "Large file detected, using chunked reading approach...\n",
      "Applying downsampling factor: 2\n",
      "Processing chunk 0 to 1,000,000 (0.4%)\n",
      "Processing chunk 1,000,000 to 2,000,000 (0.8%)\n",
      "Processing chunk 2,000,000 to 3,000,000 (1.2%)\n",
      "Processing chunk 3,000,000 to 4,000,000 (1.6%)\n",
      "Processing chunk 4,000,000 to 5,000,000 (2.0%)\n",
      "Processing chunk 5,000,000 to 6,000,000 (2.4%)\n",
      "Processing chunk 6,000,000 to 7,000,000 (2.9%)\n",
      "Processing chunk 7,000,000 to 8,000,000 (3.3%)\n",
      "Processing chunk 8,000,000 to 9,000,000 (3.7%)\n",
      "Processing chunk 9,000,000 to 10,000,000 (4.1%)\n",
      "Processing chunk 10,000,000 to 11,000,000 (4.5%)\n",
      "Processing chunk 11,000,000 to 12,000,000 (4.9%)\n",
      "Processing chunk 12,000,000 to 13,000,000 (5.3%)\n",
      "Processing chunk 13,000,000 to 14,000,000 (5.7%)\n",
      "Processing chunk 14,000,000 to 15,000,000 (6.1%)\n",
      "Processing chunk 15,000,000 to 16,000,000 (6.5%)\n",
      "Processing chunk 16,000,000 to 17,000,000 (6.9%)\n",
      "Processing chunk 17,000,000 to 18,000,000 (7.3%)\n",
      "Processing chunk 18,000,000 to 19,000,000 (7.8%)\n",
      "Processing chunk 19,000,000 to 20,000,000 (8.2%)\n",
      "Processing chunk 20,000,000 to 21,000,000 (8.6%)\n",
      "Processing chunk 21,000,000 to 22,000,000 (9.0%)\n",
      "Processing chunk 22,000,000 to 23,000,000 (9.4%)\n",
      "Processing chunk 23,000,000 to 24,000,000 (9.8%)\n",
      "Processing chunk 24,000,000 to 25,000,000 (10.2%)\n",
      "Processing chunk 25,000,000 to 26,000,000 (10.6%)\n",
      "Processing chunk 26,000,000 to 27,000,000 (11.0%)\n",
      "Processing chunk 27,000,000 to 28,000,000 (11.4%)\n",
      "Processing chunk 28,000,000 to 29,000,000 (11.8%)\n",
      "Processing chunk 29,000,000 to 30,000,000 (12.2%)\n",
      "Processing chunk 30,000,000 to 31,000,000 (12.7%)\n",
      "Processing chunk 31,000,000 to 32,000,000 (13.1%)\n",
      "Processing chunk 32,000,000 to 33,000,000 (13.5%)\n",
      "Processing chunk 33,000,000 to 34,000,000 (13.9%)\n",
      "Processing chunk 34,000,000 to 35,000,000 (14.3%)\n",
      "Processing chunk 35,000,000 to 36,000,000 (14.7%)\n",
      "Processing chunk 36,000,000 to 37,000,000 (15.1%)\n",
      "Processing chunk 37,000,000 to 38,000,000 (15.5%)\n",
      "Processing chunk 38,000,000 to 39,000,000 (15.9%)\n",
      "Processing chunk 39,000,000 to 40,000,000 (16.3%)\n",
      "Processing chunk 40,000,000 to 41,000,000 (16.7%)\n",
      "Processing chunk 41,000,000 to 42,000,000 (17.1%)\n",
      "Processing chunk 42,000,000 to 43,000,000 (17.6%)\n",
      "Processing chunk 43,000,000 to 44,000,000 (18.0%)\n",
      "Processing chunk 44,000,000 to 45,000,000 (18.4%)\n",
      "Processing chunk 45,000,000 to 46,000,000 (18.8%)\n",
      "Processing chunk 46,000,000 to 47,000,000 (19.2%)\n",
      "Processing chunk 47,000,000 to 48,000,000 (19.6%)\n",
      "Processing chunk 48,000,000 to 49,000,000 (20.0%)\n",
      "Processing chunk 49,000,000 to 50,000,000 (20.4%)\n",
      "Processing chunk 50,000,000 to 51,000,000 (20.8%)\n",
      "Processing chunk 51,000,000 to 52,000,000 (21.2%)\n",
      "Processing chunk 52,000,000 to 53,000,000 (21.6%)\n",
      "Processing chunk 53,000,000 to 54,000,000 (22.0%)\n",
      "Processing chunk 54,000,000 to 55,000,000 (22.4%)\n",
      "Processing chunk 55,000,000 to 56,000,000 (22.9%)\n",
      "Processing chunk 56,000,000 to 57,000,000 (23.3%)\n",
      "Processing chunk 57,000,000 to 58,000,000 (23.7%)\n",
      "Processing chunk 58,000,000 to 59,000,000 (24.1%)\n",
      "Processing chunk 59,000,000 to 60,000,000 (24.5%)\n",
      "Processing chunk 60,000,000 to 61,000,000 (24.9%)\n",
      "Processing chunk 61,000,000 to 62,000,000 (25.3%)\n",
      "Processing chunk 62,000,000 to 63,000,000 (25.7%)\n",
      "Processing chunk 63,000,000 to 64,000,000 (26.1%)\n",
      "Processing chunk 64,000,000 to 65,000,000 (26.5%)\n",
      "Processing chunk 65,000,000 to 66,000,000 (26.9%)\n",
      "Processing chunk 66,000,000 to 67,000,000 (27.3%)\n",
      "Processing chunk 67,000,000 to 68,000,000 (27.8%)\n",
      "Processing chunk 68,000,000 to 69,000,000 (28.2%)\n",
      "Processing chunk 69,000,000 to 70,000,000 (28.6%)\n",
      "Processing chunk 70,000,000 to 71,000,000 (29.0%)\n",
      "Processing chunk 71,000,000 to 72,000,000 (29.4%)\n",
      "Processing chunk 72,000,000 to 73,000,000 (29.8%)\n",
      "Processing chunk 73,000,000 to 74,000,000 (30.2%)\n",
      "Processing chunk 74,000,000 to 75,000,000 (30.6%)\n",
      "Processing chunk 75,000,000 to 76,000,000 (31.0%)\n",
      "Processing chunk 76,000,000 to 77,000,000 (31.4%)\n",
      "Processing chunk 77,000,000 to 78,000,000 (31.8%)\n",
      "Processing chunk 78,000,000 to 79,000,000 (32.2%)\n",
      "Processing chunk 79,000,000 to 80,000,000 (32.7%)\n",
      "Processing chunk 80,000,000 to 81,000,000 (33.1%)\n",
      "Processing chunk 81,000,000 to 82,000,000 (33.5%)\n",
      "Processing chunk 82,000,000 to 83,000,000 (33.9%)\n",
      "Processing chunk 83,000,000 to 84,000,000 (34.3%)\n",
      "Processing chunk 84,000,000 to 85,000,000 (34.7%)\n",
      "Processing chunk 85,000,000 to 86,000,000 (35.1%)\n",
      "Processing chunk 86,000,000 to 87,000,000 (35.5%)\n",
      "Processing chunk 87,000,000 to 88,000,000 (35.9%)\n",
      "Processing chunk 88,000,000 to 89,000,000 (36.3%)\n",
      "Processing chunk 89,000,000 to 90,000,000 (36.7%)\n",
      "Processing chunk 90,000,000 to 91,000,000 (37.1%)\n",
      "Processing chunk 91,000,000 to 92,000,000 (37.6%)\n",
      "Processing chunk 92,000,000 to 93,000,000 (38.0%)\n",
      "Processing chunk 93,000,000 to 94,000,000 (38.4%)\n",
      "Processing chunk 94,000,000 to 95,000,000 (38.8%)\n",
      "Processing chunk 95,000,000 to 96,000,000 (39.2%)\n",
      "Processing chunk 96,000,000 to 97,000,000 (39.6%)\n",
      "Processing chunk 97,000,000 to 98,000,000 (40.0%)\n",
      "Processing chunk 98,000,000 to 99,000,000 (40.4%)\n",
      "Processing chunk 99,000,000 to 100,000,000 (40.8%)\n",
      "Processing chunk 100,000,000 to 101,000,000 (41.2%)\n",
      "Processing chunk 101,000,000 to 102,000,000 (41.6%)\n",
      "Processing chunk 102,000,000 to 103,000,000 (42.0%)\n",
      "Processing chunk 103,000,000 to 104,000,000 (42.4%)\n",
      "Processing chunk 104,000,000 to 105,000,000 (42.9%)\n",
      "Processing chunk 105,000,000 to 106,000,000 (43.3%)\n",
      "Processing chunk 106,000,000 to 107,000,000 (43.7%)\n",
      "Processing chunk 107,000,000 to 108,000,000 (44.1%)\n",
      "Processing chunk 108,000,000 to 109,000,000 (44.5%)\n",
      "Processing chunk 109,000,000 to 110,000,000 (44.9%)\n",
      "Processing chunk 110,000,000 to 111,000,000 (45.3%)\n",
      "Processing chunk 111,000,000 to 112,000,000 (45.7%)\n",
      "Processing chunk 112,000,000 to 113,000,000 (46.1%)\n",
      "Processing chunk 113,000,000 to 114,000,000 (46.5%)\n",
      "Processing chunk 114,000,000 to 115,000,000 (46.9%)\n",
      "Processing chunk 115,000,000 to 116,000,000 (47.3%)\n",
      "Processing chunk 116,000,000 to 117,000,000 (47.8%)\n",
      "Processing chunk 117,000,000 to 118,000,000 (48.2%)\n",
      "Processing chunk 118,000,000 to 119,000,000 (48.6%)\n",
      "Processing chunk 119,000,000 to 120,000,000 (49.0%)\n",
      "Processing chunk 120,000,000 to 121,000,000 (49.4%)\n",
      "Processing chunk 121,000,000 to 122,000,000 (49.8%)\n",
      "Processing chunk 122,000,000 to 123,000,000 (50.2%)\n",
      "Processing chunk 123,000,000 to 124,000,000 (50.6%)\n",
      "Processing chunk 124,000,000 to 125,000,000 (51.0%)\n",
      "Processing chunk 125,000,000 to 126,000,000 (51.4%)\n",
      "Processing chunk 126,000,000 to 127,000,000 (51.8%)\n",
      "Processing chunk 127,000,000 to 128,000,000 (52.2%)\n",
      "Processing chunk 128,000,000 to 129,000,000 (52.7%)\n",
      "Processing chunk 129,000,000 to 130,000,000 (53.1%)\n",
      "Processing chunk 130,000,000 to 131,000,000 (53.5%)\n",
      "Processing chunk 131,000,000 to 132,000,000 (53.9%)\n",
      "Processing chunk 132,000,000 to 133,000,000 (54.3%)\n",
      "Processing chunk 133,000,000 to 134,000,000 (54.7%)\n",
      "Processing chunk 134,000,000 to 135,000,000 (55.1%)\n",
      "Processing chunk 135,000,000 to 136,000,000 (55.5%)\n",
      "Processing chunk 136,000,000 to 137,000,000 (55.9%)\n",
      "Processing chunk 137,000,000 to 138,000,000 (56.3%)\n",
      "Processing chunk 138,000,000 to 139,000,000 (56.7%)\n",
      "Processing chunk 139,000,000 to 140,000,000 (57.1%)\n",
      "Processing chunk 140,000,000 to 141,000,000 (57.5%)\n",
      "Processing chunk 141,000,000 to 142,000,000 (58.0%)\n",
      "Processing chunk 142,000,000 to 143,000,000 (58.4%)\n",
      "Processing chunk 143,000,000 to 144,000,000 (58.8%)\n",
      "Processing chunk 144,000,000 to 145,000,000 (59.2%)\n",
      "Processing chunk 145,000,000 to 146,000,000 (59.6%)\n",
      "Processing chunk 146,000,000 to 147,000,000 (60.0%)\n",
      "Processing chunk 147,000,000 to 148,000,000 (60.4%)\n",
      "Processing chunk 148,000,000 to 149,000,000 (60.8%)\n",
      "Processing chunk 149,000,000 to 150,000,000 (61.2%)\n",
      "Processing chunk 150,000,000 to 151,000,000 (61.6%)\n",
      "Processing chunk 151,000,000 to 152,000,000 (62.0%)\n",
      "Processing chunk 152,000,000 to 153,000,000 (62.4%)\n",
      "Processing chunk 153,000,000 to 154,000,000 (62.9%)\n",
      "Processing chunk 154,000,000 to 155,000,000 (63.3%)\n",
      "Processing chunk 155,000,000 to 156,000,000 (63.7%)\n",
      "Processing chunk 156,000,000 to 157,000,000 (64.1%)\n",
      "Processing chunk 157,000,000 to 158,000,000 (64.5%)\n",
      "Processing chunk 158,000,000 to 159,000,000 (64.9%)\n",
      "Processing chunk 159,000,000 to 160,000,000 (65.3%)\n",
      "Processing chunk 160,000,000 to 161,000,000 (65.7%)\n",
      "Processing chunk 161,000,000 to 162,000,000 (66.1%)\n",
      "Processing chunk 162,000,000 to 163,000,000 (66.5%)\n",
      "Processing chunk 163,000,000 to 164,000,000 (66.9%)\n",
      "Processing chunk 164,000,000 to 165,000,000 (67.3%)\n",
      "Processing chunk 165,000,000 to 166,000,000 (67.8%)\n",
      "Processing chunk 166,000,000 to 167,000,000 (68.2%)\n",
      "Processing chunk 167,000,000 to 168,000,000 (68.6%)\n",
      "Processing chunk 168,000,000 to 169,000,000 (69.0%)\n",
      "Processing chunk 169,000,000 to 170,000,000 (69.4%)\n",
      "Processing chunk 170,000,000 to 171,000,000 (69.8%)\n",
      "Processing chunk 171,000,000 to 172,000,000 (70.2%)\n",
      "Processing chunk 172,000,000 to 173,000,000 (70.6%)\n",
      "Processing chunk 173,000,000 to 174,000,000 (71.0%)\n",
      "Processing chunk 174,000,000 to 175,000,000 (71.4%)\n",
      "Processing chunk 175,000,000 to 176,000,000 (71.8%)\n",
      "Processing chunk 176,000,000 to 177,000,000 (72.2%)\n",
      "Processing chunk 177,000,000 to 178,000,000 (72.7%)\n",
      "Processing chunk 178,000,000 to 179,000,000 (73.1%)\n",
      "Processing chunk 179,000,000 to 180,000,000 (73.5%)\n",
      "Processing chunk 180,000,000 to 181,000,000 (73.9%)\n",
      "Processing chunk 181,000,000 to 182,000,000 (74.3%)\n",
      "Processing chunk 182,000,000 to 183,000,000 (74.7%)\n",
      "Processing chunk 183,000,000 to 184,000,000 (75.1%)\n",
      "Processing chunk 184,000,000 to 185,000,000 (75.5%)\n",
      "Processing chunk 185,000,000 to 186,000,000 (75.9%)\n",
      "Processing chunk 186,000,000 to 187,000,000 (76.3%)\n",
      "Processing chunk 187,000,000 to 188,000,000 (76.7%)\n",
      "Processing chunk 188,000,000 to 189,000,000 (77.1%)\n",
      "Processing chunk 189,000,000 to 190,000,000 (77.5%)\n",
      "Processing chunk 190,000,000 to 191,000,000 (78.0%)\n",
      "Processing chunk 191,000,000 to 192,000,000 (78.4%)\n",
      "Processing chunk 192,000,000 to 193,000,000 (78.8%)\n",
      "Processing chunk 193,000,000 to 194,000,000 (79.2%)\n",
      "Processing chunk 194,000,000 to 195,000,000 (79.6%)\n",
      "Processing chunk 195,000,000 to 196,000,000 (80.0%)\n",
      "Processing chunk 196,000,000 to 197,000,000 (80.4%)\n",
      "Processing chunk 197,000,000 to 198,000,000 (80.8%)\n",
      "Processing chunk 198,000,000 to 199,000,000 (81.2%)\n",
      "Processing chunk 199,000,000 to 200,000,000 (81.6%)\n",
      "Processing chunk 200,000,000 to 201,000,000 (82.0%)\n",
      "Processing chunk 201,000,000 to 202,000,000 (82.4%)\n",
      "Processing chunk 202,000,000 to 203,000,000 (82.9%)\n",
      "Processing chunk 203,000,000 to 204,000,000 (83.3%)\n",
      "Processing chunk 204,000,000 to 205,000,000 (83.7%)\n",
      "Processing chunk 205,000,000 to 206,000,000 (84.1%)\n",
      "Processing chunk 206,000,000 to 207,000,000 (84.5%)\n",
      "Processing chunk 207,000,000 to 208,000,000 (84.9%)\n",
      "Processing chunk 208,000,000 to 209,000,000 (85.3%)\n",
      "Processing chunk 209,000,000 to 210,000,000 (85.7%)\n",
      "Processing chunk 210,000,000 to 211,000,000 (86.1%)\n",
      "Processing chunk 211,000,000 to 212,000,000 (86.5%)\n",
      "Processing chunk 212,000,000 to 213,000,000 (86.9%)\n",
      "Processing chunk 213,000,000 to 214,000,000 (87.3%)\n",
      "Processing chunk 214,000,000 to 215,000,000 (87.8%)\n",
      "Processing chunk 215,000,000 to 216,000,000 (88.2%)\n",
      "Processing chunk 216,000,000 to 217,000,000 (88.6%)\n",
      "Processing chunk 217,000,000 to 218,000,000 (89.0%)\n",
      "Processing chunk 218,000,000 to 219,000,000 (89.4%)\n",
      "Processing chunk 219,000,000 to 220,000,000 (89.8%)\n",
      "Processing chunk 220,000,000 to 221,000,000 (90.2%)\n",
      "Processing chunk 221,000,000 to 222,000,000 (90.6%)\n",
      "Processing chunk 222,000,000 to 223,000,000 (91.0%)\n",
      "Processing chunk 223,000,000 to 224,000,000 (91.4%)\n",
      "Processing chunk 224,000,000 to 225,000,000 (91.8%)\n",
      "Processing chunk 225,000,000 to 226,000,000 (92.2%)\n",
      "Processing chunk 226,000,000 to 227,000,000 (92.7%)\n",
      "Processing chunk 227,000,000 to 228,000,000 (93.1%)\n",
      "Processing chunk 228,000,000 to 229,000,000 (93.5%)\n",
      "Processing chunk 229,000,000 to 230,000,000 (93.9%)\n",
      "Processing chunk 230,000,000 to 231,000,000 (94.3%)\n",
      "Processing chunk 231,000,000 to 232,000,000 (94.7%)\n",
      "Processing chunk 232,000,000 to 233,000,000 (95.1%)\n",
      "Processing chunk 233,000,000 to 234,000,000 (95.5%)\n",
      "Processing chunk 234,000,000 to 235,000,000 (95.9%)\n",
      "Processing chunk 235,000,000 to 236,000,000 (96.3%)\n",
      "Processing chunk 236,000,000 to 237,000,000 (96.7%)\n",
      "Processing chunk 237,000,000 to 238,000,000 (97.1%)\n",
      "Processing chunk 238,000,000 to 239,000,000 (97.5%)\n",
      "Processing chunk 239,000,000 to 240,000,000 (98.0%)\n",
      "Processing chunk 240,000,000 to 241,000,000 (98.4%)\n",
      "Processing chunk 241,000,000 to 242,000,000 (98.8%)\n",
      "Processing chunk 242,000,000 to 243,000,000 (99.2%)\n",
      "Processing chunk 243,000,000 to 244,000,000 (99.6%)\n",
      "Processing chunk 244,000,000 to 245,000,000 (100.0%)\n",
      "Processing chunk 245,000,000 to 245,005,000 (100.0%)\n",
      "Combining chunks...\n",
      "Adjusted 'Time' column by GMT+2 hours\n",
      "Successfully processed large file with chunked reading\n",
      "Successfully loaded data with 122,502,500 samples and 3 channels\n",
      "All channels found in part1: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time columns: ['Time']\n",
      "Signal channels (2): ['Voltage_0', 'Voltage_1']\n",
      "Preserving column 'Time' (not converting)\n",
      "Found 2 signal channels to convert: ['Voltage_0', 'Voltage_1']\n",
      "Found 1 non-signal columns to preserve: ['Time']\n",
      "Converting voltage signals to nanoTesla using factor: 20 nT/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting channels: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage to nanoTesla conversion completed. Converted 2 channels.\n",
      "Signal values are now in nanoTesla (nT) units.\n",
      "Voltage conversion completed for part1\n",
      "Saving part1 data to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part1.parquet...\n",
      "Successfully saved 122,502,500 samples from part1 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part1.parquet\n",
      "DataFrame shape for part1: (122502500, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range for part1: 2021-11-16 22:24:59.171050 to 2021-11-17 04:49:32.170950\n",
      "Duration: 6.41 hours\n",
      "Output file size for part1: 610.0 MB\n",
      "\n",
      "Processing part2 file: ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_12_07\\fruit_1.tdms\n",
      "File size: 8021.4 MB\n",
      "Large file detected. Will apply downsampling factor: 2\n",
      "Reading ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_12_07\\fruit_1.tdms...\n",
      "Total data length: 262,525,000 samples\n",
      "Estimated memory requirement: 6008.7 MB\n",
      "Large file detected, using chunked reading approach...\n",
      "Applying downsampling factor: 2\n",
      "Processing chunk 0 to 1,000,000 (0.4%)\n",
      "Processing chunk 1,000,000 to 2,000,000 (0.8%)\n",
      "Processing chunk 2,000,000 to 3,000,000 (1.1%)\n",
      "Processing chunk 3,000,000 to 4,000,000 (1.5%)\n",
      "Processing chunk 4,000,000 to 5,000,000 (1.9%)\n",
      "Processing chunk 5,000,000 to 6,000,000 (2.3%)\n",
      "Processing chunk 6,000,000 to 7,000,000 (2.7%)\n",
      "Processing chunk 7,000,000 to 8,000,000 (3.0%)\n",
      "Processing chunk 8,000,000 to 9,000,000 (3.4%)\n",
      "Processing chunk 9,000,000 to 10,000,000 (3.8%)\n",
      "Processing chunk 10,000,000 to 11,000,000 (4.2%)\n",
      "Processing chunk 11,000,000 to 12,000,000 (4.6%)\n",
      "Processing chunk 12,000,000 to 13,000,000 (5.0%)\n",
      "Processing chunk 13,000,000 to 14,000,000 (5.3%)\n",
      "Processing chunk 14,000,000 to 15,000,000 (5.7%)\n",
      "Processing chunk 15,000,000 to 16,000,000 (6.1%)\n",
      "Processing chunk 16,000,000 to 17,000,000 (6.5%)\n",
      "Processing chunk 17,000,000 to 18,000,000 (6.9%)\n",
      "Processing chunk 18,000,000 to 19,000,000 (7.2%)\n",
      "Processing chunk 19,000,000 to 20,000,000 (7.6%)\n",
      "Processing chunk 20,000,000 to 21,000,000 (8.0%)\n",
      "Processing chunk 21,000,000 to 22,000,000 (8.4%)\n",
      "Processing chunk 22,000,000 to 23,000,000 (8.8%)\n",
      "Processing chunk 23,000,000 to 24,000,000 (9.1%)\n",
      "Processing chunk 24,000,000 to 25,000,000 (9.5%)\n",
      "Processing chunk 25,000,000 to 26,000,000 (9.9%)\n",
      "Processing chunk 26,000,000 to 27,000,000 (10.3%)\n",
      "Processing chunk 27,000,000 to 28,000,000 (10.7%)\n",
      "Processing chunk 28,000,000 to 29,000,000 (11.0%)\n",
      "Processing chunk 29,000,000 to 30,000,000 (11.4%)\n",
      "Processing chunk 30,000,000 to 31,000,000 (11.8%)\n",
      "Processing chunk 31,000,000 to 32,000,000 (12.2%)\n",
      "Processing chunk 32,000,000 to 33,000,000 (12.6%)\n",
      "Processing chunk 33,000,000 to 34,000,000 (13.0%)\n",
      "Processing chunk 34,000,000 to 35,000,000 (13.3%)\n",
      "Processing chunk 35,000,000 to 36,000,000 (13.7%)\n",
      "Processing chunk 36,000,000 to 37,000,000 (14.1%)\n",
      "Processing chunk 37,000,000 to 38,000,000 (14.5%)\n",
      "Processing chunk 38,000,000 to 39,000,000 (14.9%)\n",
      "Processing chunk 39,000,000 to 40,000,000 (15.2%)\n",
      "Processing chunk 40,000,000 to 41,000,000 (15.6%)\n",
      "Processing chunk 41,000,000 to 42,000,000 (16.0%)\n",
      "Processing chunk 42,000,000 to 43,000,000 (16.4%)\n",
      "Processing chunk 43,000,000 to 44,000,000 (16.8%)\n",
      "Processing chunk 44,000,000 to 45,000,000 (17.1%)\n",
      "Processing chunk 45,000,000 to 46,000,000 (17.5%)\n",
      "Processing chunk 46,000,000 to 47,000,000 (17.9%)\n",
      "Processing chunk 47,000,000 to 48,000,000 (18.3%)\n",
      "Processing chunk 48,000,000 to 49,000,000 (18.7%)\n",
      "Processing chunk 49,000,000 to 50,000,000 (19.0%)\n",
      "Processing chunk 50,000,000 to 51,000,000 (19.4%)\n",
      "Processing chunk 51,000,000 to 52,000,000 (19.8%)\n",
      "Processing chunk 52,000,000 to 53,000,000 (20.2%)\n",
      "Processing chunk 53,000,000 to 54,000,000 (20.6%)\n",
      "Processing chunk 54,000,000 to 55,000,000 (21.0%)\n",
      "Processing chunk 55,000,000 to 56,000,000 (21.3%)\n",
      "Processing chunk 56,000,000 to 57,000,000 (21.7%)\n",
      "Processing chunk 57,000,000 to 58,000,000 (22.1%)\n",
      "Processing chunk 58,000,000 to 59,000,000 (22.5%)\n",
      "Processing chunk 59,000,000 to 60,000,000 (22.9%)\n",
      "Processing chunk 60,000,000 to 61,000,000 (23.2%)\n",
      "Processing chunk 61,000,000 to 62,000,000 (23.6%)\n",
      "Processing chunk 62,000,000 to 63,000,000 (24.0%)\n",
      "Processing chunk 63,000,000 to 64,000,000 (24.4%)\n",
      "Processing chunk 64,000,000 to 65,000,000 (24.8%)\n",
      "Processing chunk 65,000,000 to 66,000,000 (25.1%)\n",
      "Processing chunk 66,000,000 to 67,000,000 (25.5%)\n",
      "Processing chunk 67,000,000 to 68,000,000 (25.9%)\n",
      "Processing chunk 68,000,000 to 69,000,000 (26.3%)\n",
      "Processing chunk 69,000,000 to 70,000,000 (26.7%)\n",
      "Processing chunk 70,000,000 to 71,000,000 (27.0%)\n",
      "Processing chunk 71,000,000 to 72,000,000 (27.4%)\n",
      "Processing chunk 72,000,000 to 73,000,000 (27.8%)\n",
      "Processing chunk 73,000,000 to 74,000,000 (28.2%)\n",
      "Processing chunk 74,000,000 to 75,000,000 (28.6%)\n",
      "Processing chunk 75,000,000 to 76,000,000 (28.9%)\n",
      "Processing chunk 76,000,000 to 77,000,000 (29.3%)\n",
      "Processing chunk 77,000,000 to 78,000,000 (29.7%)\n",
      "Processing chunk 78,000,000 to 79,000,000 (30.1%)\n",
      "Processing chunk 79,000,000 to 80,000,000 (30.5%)\n",
      "Processing chunk 80,000,000 to 81,000,000 (30.9%)\n",
      "Processing chunk 81,000,000 to 82,000,000 (31.2%)\n",
      "Processing chunk 82,000,000 to 83,000,000 (31.6%)\n",
      "Processing chunk 83,000,000 to 84,000,000 (32.0%)\n",
      "Processing chunk 84,000,000 to 85,000,000 (32.4%)\n",
      "Processing chunk 85,000,000 to 86,000,000 (32.8%)\n",
      "Processing chunk 86,000,000 to 87,000,000 (33.1%)\n",
      "Processing chunk 87,000,000 to 88,000,000 (33.5%)\n",
      "Processing chunk 88,000,000 to 89,000,000 (33.9%)\n",
      "Processing chunk 89,000,000 to 90,000,000 (34.3%)\n",
      "Processing chunk 90,000,000 to 91,000,000 (34.7%)\n",
      "Processing chunk 91,000,000 to 92,000,000 (35.0%)\n",
      "Processing chunk 92,000,000 to 93,000,000 (35.4%)\n",
      "Processing chunk 93,000,000 to 94,000,000 (35.8%)\n",
      "Processing chunk 94,000,000 to 95,000,000 (36.2%)\n",
      "Processing chunk 95,000,000 to 96,000,000 (36.6%)\n",
      "Processing chunk 96,000,000 to 97,000,000 (36.9%)\n",
      "Processing chunk 97,000,000 to 98,000,000 (37.3%)\n",
      "Processing chunk 98,000,000 to 99,000,000 (37.7%)\n",
      "Processing chunk 99,000,000 to 100,000,000 (38.1%)\n",
      "Processing chunk 100,000,000 to 101,000,000 (38.5%)\n",
      "Processing chunk 101,000,000 to 102,000,000 (38.9%)\n",
      "Processing chunk 102,000,000 to 103,000,000 (39.2%)\n",
      "Processing chunk 103,000,000 to 104,000,000 (39.6%)\n",
      "Processing chunk 104,000,000 to 105,000,000 (40.0%)\n",
      "Processing chunk 105,000,000 to 106,000,000 (40.4%)\n",
      "Processing chunk 106,000,000 to 107,000,000 (40.8%)\n",
      "Processing chunk 107,000,000 to 108,000,000 (41.1%)\n",
      "Processing chunk 108,000,000 to 109,000,000 (41.5%)\n",
      "Processing chunk 109,000,000 to 110,000,000 (41.9%)\n",
      "Processing chunk 110,000,000 to 111,000,000 (42.3%)\n",
      "Processing chunk 111,000,000 to 112,000,000 (42.7%)\n",
      "Processing chunk 112,000,000 to 113,000,000 (43.0%)\n",
      "Processing chunk 113,000,000 to 114,000,000 (43.4%)\n",
      "Processing chunk 114,000,000 to 115,000,000 (43.8%)\n",
      "Processing chunk 115,000,000 to 116,000,000 (44.2%)\n",
      "Processing chunk 116,000,000 to 117,000,000 (44.6%)\n",
      "Processing chunk 117,000,000 to 118,000,000 (44.9%)\n",
      "Processing chunk 118,000,000 to 119,000,000 (45.3%)\n",
      "Processing chunk 119,000,000 to 120,000,000 (45.7%)\n",
      "Processing chunk 120,000,000 to 121,000,000 (46.1%)\n",
      "Processing chunk 121,000,000 to 122,000,000 (46.5%)\n",
      "Processing chunk 122,000,000 to 123,000,000 (46.9%)\n",
      "Processing chunk 123,000,000 to 124,000,000 (47.2%)\n",
      "Processing chunk 124,000,000 to 125,000,000 (47.6%)\n",
      "Processing chunk 125,000,000 to 126,000,000 (48.0%)\n",
      "Processing chunk 126,000,000 to 127,000,000 (48.4%)\n",
      "Processing chunk 127,000,000 to 128,000,000 (48.8%)\n",
      "Processing chunk 128,000,000 to 129,000,000 (49.1%)\n",
      "Processing chunk 129,000,000 to 130,000,000 (49.5%)\n",
      "Processing chunk 130,000,000 to 131,000,000 (49.9%)\n",
      "Processing chunk 131,000,000 to 132,000,000 (50.3%)\n",
      "Processing chunk 132,000,000 to 133,000,000 (50.7%)\n",
      "Processing chunk 133,000,000 to 134,000,000 (51.0%)\n",
      "Processing chunk 134,000,000 to 135,000,000 (51.4%)\n",
      "Processing chunk 135,000,000 to 136,000,000 (51.8%)\n",
      "Processing chunk 136,000,000 to 137,000,000 (52.2%)\n",
      "Processing chunk 137,000,000 to 138,000,000 (52.6%)\n",
      "Processing chunk 138,000,000 to 139,000,000 (52.9%)\n",
      "Processing chunk 139,000,000 to 140,000,000 (53.3%)\n",
      "Processing chunk 140,000,000 to 141,000,000 (53.7%)\n",
      "Processing chunk 141,000,000 to 142,000,000 (54.1%)\n",
      "Processing chunk 142,000,000 to 143,000,000 (54.5%)\n",
      "Processing chunk 143,000,000 to 144,000,000 (54.9%)\n",
      "Processing chunk 144,000,000 to 145,000,000 (55.2%)\n",
      "Processing chunk 145,000,000 to 146,000,000 (55.6%)\n",
      "Processing chunk 146,000,000 to 147,000,000 (56.0%)\n",
      "Processing chunk 147,000,000 to 148,000,000 (56.4%)\n",
      "Processing chunk 148,000,000 to 149,000,000 (56.8%)\n",
      "Processing chunk 149,000,000 to 150,000,000 (57.1%)\n",
      "Processing chunk 150,000,000 to 151,000,000 (57.5%)\n",
      "Processing chunk 151,000,000 to 152,000,000 (57.9%)\n",
      "Processing chunk 152,000,000 to 153,000,000 (58.3%)\n",
      "Processing chunk 153,000,000 to 154,000,000 (58.7%)\n",
      "Processing chunk 154,000,000 to 155,000,000 (59.0%)\n",
      "Processing chunk 155,000,000 to 156,000,000 (59.4%)\n",
      "Processing chunk 156,000,000 to 157,000,000 (59.8%)\n",
      "Processing chunk 157,000,000 to 158,000,000 (60.2%)\n",
      "Processing chunk 158,000,000 to 159,000,000 (60.6%)\n",
      "Processing chunk 159,000,000 to 160,000,000 (60.9%)\n",
      "Processing chunk 160,000,000 to 161,000,000 (61.3%)\n",
      "Processing chunk 161,000,000 to 162,000,000 (61.7%)\n",
      "Processing chunk 162,000,000 to 163,000,000 (62.1%)\n",
      "Processing chunk 163,000,000 to 164,000,000 (62.5%)\n",
      "Processing chunk 164,000,000 to 165,000,000 (62.9%)\n",
      "Processing chunk 165,000,000 to 166,000,000 (63.2%)\n",
      "Processing chunk 166,000,000 to 167,000,000 (63.6%)\n",
      "Processing chunk 167,000,000 to 168,000,000 (64.0%)\n",
      "Processing chunk 168,000,000 to 169,000,000 (64.4%)\n",
      "Processing chunk 169,000,000 to 170,000,000 (64.8%)\n",
      "Processing chunk 170,000,000 to 171,000,000 (65.1%)\n",
      "Processing chunk 171,000,000 to 172,000,000 (65.5%)\n",
      "Processing chunk 172,000,000 to 173,000,000 (65.9%)\n",
      "Processing chunk 173,000,000 to 174,000,000 (66.3%)\n",
      "Processing chunk 174,000,000 to 175,000,000 (66.7%)\n",
      "Processing chunk 175,000,000 to 176,000,000 (67.0%)\n",
      "Processing chunk 176,000,000 to 177,000,000 (67.4%)\n",
      "Processing chunk 177,000,000 to 178,000,000 (67.8%)\n",
      "Processing chunk 178,000,000 to 179,000,000 (68.2%)\n",
      "Processing chunk 179,000,000 to 180,000,000 (68.6%)\n",
      "Processing chunk 180,000,000 to 181,000,000 (68.9%)\n",
      "Processing chunk 181,000,000 to 182,000,000 (69.3%)\n",
      "Processing chunk 182,000,000 to 183,000,000 (69.7%)\n",
      "Processing chunk 183,000,000 to 184,000,000 (70.1%)\n",
      "Processing chunk 184,000,000 to 185,000,000 (70.5%)\n",
      "Processing chunk 185,000,000 to 186,000,000 (70.9%)\n",
      "Processing chunk 186,000,000 to 187,000,000 (71.2%)\n",
      "Processing chunk 187,000,000 to 188,000,000 (71.6%)\n",
      "Processing chunk 188,000,000 to 189,000,000 (72.0%)\n",
      "Processing chunk 189,000,000 to 190,000,000 (72.4%)\n",
      "Processing chunk 190,000,000 to 191,000,000 (72.8%)\n",
      "Processing chunk 191,000,000 to 192,000,000 (73.1%)\n",
      "Processing chunk 192,000,000 to 193,000,000 (73.5%)\n",
      "Processing chunk 193,000,000 to 194,000,000 (73.9%)\n",
      "Processing chunk 194,000,000 to 195,000,000 (74.3%)\n",
      "Processing chunk 195,000,000 to 196,000,000 (74.7%)\n",
      "Processing chunk 196,000,000 to 197,000,000 (75.0%)\n",
      "Processing chunk 197,000,000 to 198,000,000 (75.4%)\n",
      "Processing chunk 198,000,000 to 199,000,000 (75.8%)\n",
      "Processing chunk 199,000,000 to 200,000,000 (76.2%)\n",
      "Processing chunk 200,000,000 to 201,000,000 (76.6%)\n",
      "Processing chunk 201,000,000 to 202,000,000 (76.9%)\n",
      "Processing chunk 202,000,000 to 203,000,000 (77.3%)\n",
      "Processing chunk 203,000,000 to 204,000,000 (77.7%)\n",
      "Processing chunk 204,000,000 to 205,000,000 (78.1%)\n",
      "Processing chunk 205,000,000 to 206,000,000 (78.5%)\n",
      "Processing chunk 206,000,000 to 207,000,000 (78.8%)\n",
      "Processing chunk 207,000,000 to 208,000,000 (79.2%)\n",
      "Processing chunk 208,000,000 to 209,000,000 (79.6%)\n",
      "Processing chunk 209,000,000 to 210,000,000 (80.0%)\n",
      "Processing chunk 210,000,000 to 211,000,000 (80.4%)\n",
      "Processing chunk 211,000,000 to 212,000,000 (80.8%)\n",
      "Processing chunk 212,000,000 to 213,000,000 (81.1%)\n",
      "Processing chunk 213,000,000 to 214,000,000 (81.5%)\n",
      "Processing chunk 214,000,000 to 215,000,000 (81.9%)\n",
      "Processing chunk 215,000,000 to 216,000,000 (82.3%)\n",
      "Processing chunk 216,000,000 to 217,000,000 (82.7%)\n",
      "Processing chunk 217,000,000 to 218,000,000 (83.0%)\n",
      "Processing chunk 218,000,000 to 219,000,000 (83.4%)\n",
      "Processing chunk 219,000,000 to 220,000,000 (83.8%)\n",
      "Processing chunk 220,000,000 to 221,000,000 (84.2%)\n",
      "Processing chunk 221,000,000 to 222,000,000 (84.6%)\n",
      "Processing chunk 222,000,000 to 223,000,000 (84.9%)\n",
      "Processing chunk 223,000,000 to 224,000,000 (85.3%)\n",
      "Processing chunk 224,000,000 to 225,000,000 (85.7%)\n",
      "Processing chunk 225,000,000 to 226,000,000 (86.1%)\n",
      "Processing chunk 226,000,000 to 227,000,000 (86.5%)\n",
      "Processing chunk 227,000,000 to 228,000,000 (86.8%)\n",
      "Processing chunk 228,000,000 to 229,000,000 (87.2%)\n",
      "Processing chunk 229,000,000 to 230,000,000 (87.6%)\n",
      "Processing chunk 230,000,000 to 231,000,000 (88.0%)\n",
      "Processing chunk 231,000,000 to 232,000,000 (88.4%)\n",
      "Processing chunk 232,000,000 to 233,000,000 (88.8%)\n",
      "Processing chunk 233,000,000 to 234,000,000 (89.1%)\n",
      "Processing chunk 234,000,000 to 235,000,000 (89.5%)\n",
      "Processing chunk 235,000,000 to 236,000,000 (89.9%)\n",
      "Processing chunk 236,000,000 to 237,000,000 (90.3%)\n",
      "Processing chunk 237,000,000 to 238,000,000 (90.7%)\n",
      "Processing chunk 238,000,000 to 239,000,000 (91.0%)\n",
      "Processing chunk 239,000,000 to 240,000,000 (91.4%)\n",
      "Processing chunk 240,000,000 to 241,000,000 (91.8%)\n",
      "Processing chunk 241,000,000 to 242,000,000 (92.2%)\n",
      "Processing chunk 242,000,000 to 243,000,000 (92.6%)\n",
      "Processing chunk 243,000,000 to 244,000,000 (92.9%)\n",
      "Processing chunk 244,000,000 to 245,000,000 (93.3%)\n",
      "Processing chunk 245,000,000 to 246,000,000 (93.7%)\n",
      "Processing chunk 246,000,000 to 247,000,000 (94.1%)\n",
      "Processing chunk 247,000,000 to 248,000,000 (94.5%)\n",
      "Processing chunk 248,000,000 to 249,000,000 (94.8%)\n",
      "Processing chunk 249,000,000 to 250,000,000 (95.2%)\n",
      "Processing chunk 250,000,000 to 251,000,000 (95.6%)\n",
      "Processing chunk 251,000,000 to 252,000,000 (96.0%)\n",
      "Processing chunk 252,000,000 to 253,000,000 (96.4%)\n",
      "Processing chunk 253,000,000 to 254,000,000 (96.8%)\n",
      "Processing chunk 254,000,000 to 255,000,000 (97.1%)\n",
      "Processing chunk 255,000,000 to 256,000,000 (97.5%)\n",
      "Processing chunk 256,000,000 to 257,000,000 (97.9%)\n",
      "Processing chunk 257,000,000 to 258,000,000 (98.3%)\n",
      "Processing chunk 258,000,000 to 259,000,000 (98.7%)\n",
      "Processing chunk 259,000,000 to 260,000,000 (99.0%)\n",
      "Processing chunk 260,000,000 to 261,000,000 (99.4%)\n",
      "Processing chunk 261,000,000 to 262,000,000 (99.8%)\n",
      "Processing chunk 262,000,000 to 262,525,000 (100.0%)\n",
      "Combining chunks...\n",
      "Adjusted 'Time' column by GMT+2 hours\n",
      "Successfully processed large file with chunked reading\n",
      "Successfully loaded data with 131,262,500 samples and 3 channels\n",
      "All channels found in part2: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time columns: ['Time']\n",
      "Signal channels (2): ['Voltage_0', 'Voltage_1']\n",
      "Preserving column 'Time' (not converting)\n",
      "Found 2 signal channels to convert: ['Voltage_0', 'Voltage_1']\n",
      "Found 1 non-signal columns to preserve: ['Time']\n",
      "Converting voltage signals to nanoTesla using factor: 20 nT/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting channels: 100%|██████████| 2/2 [00:00<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage to nanoTesla conversion completed. Converted 2 channels.\n",
      "Signal values are now in nanoTesla (nT) units.\n",
      "Voltage conversion completed for part2\n",
      "Saving part2 data to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part2.parquet...\n",
      "Successfully saved 131,262,500 samples from part2 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part2.parquet\n",
      "DataFrame shape for part2: (131262500, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range for part2: 2021-12-07 22:19:36.521900 to 2021-12-08 06:51:52.271800\n",
      "Duration: 8.54 hours\n",
      "Output file size for part2: 641.0 MB\n",
      "\n",
      "Processing part3 file: ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_12_14\\fruit_1.tdms\n",
      "File size: 10658.6 MB\n",
      "Large file detected. Will apply downsampling factor: 2\n",
      "Reading ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_12_14\\fruit_1.tdms...\n",
      "Total data length: 348,835,000 samples\n",
      "Estimated memory requirement: 7984.2 MB\n",
      "Large file detected, using chunked reading approach...\n",
      "Applying downsampling factor: 2\n",
      "Processing chunk 0 to 1,000,000 (0.3%)\n",
      "Processing chunk 1,000,000 to 2,000,000 (0.6%)\n",
      "Processing chunk 2,000,000 to 3,000,000 (0.9%)\n",
      "Processing chunk 3,000,000 to 4,000,000 (1.1%)\n",
      "Processing chunk 4,000,000 to 5,000,000 (1.4%)\n",
      "Processing chunk 5,000,000 to 6,000,000 (1.7%)\n",
      "Processing chunk 6,000,000 to 7,000,000 (2.0%)\n",
      "Processing chunk 7,000,000 to 8,000,000 (2.3%)\n",
      "Processing chunk 8,000,000 to 9,000,000 (2.6%)\n",
      "Processing chunk 9,000,000 to 10,000,000 (2.9%)\n",
      "Processing chunk 10,000,000 to 11,000,000 (3.2%)\n",
      "Processing chunk 11,000,000 to 12,000,000 (3.4%)\n",
      "Processing chunk 12,000,000 to 13,000,000 (3.7%)\n",
      "Processing chunk 13,000,000 to 14,000,000 (4.0%)\n",
      "Processing chunk 14,000,000 to 15,000,000 (4.3%)\n",
      "Processing chunk 15,000,000 to 16,000,000 (4.6%)\n",
      "Processing chunk 16,000,000 to 17,000,000 (4.9%)\n",
      "Processing chunk 17,000,000 to 18,000,000 (5.2%)\n",
      "Processing chunk 18,000,000 to 19,000,000 (5.4%)\n",
      "Processing chunk 19,000,000 to 20,000,000 (5.7%)\n",
      "Processing chunk 20,000,000 to 21,000,000 (6.0%)\n",
      "Processing chunk 21,000,000 to 22,000,000 (6.3%)\n",
      "Processing chunk 22,000,000 to 23,000,000 (6.6%)\n",
      "Processing chunk 23,000,000 to 24,000,000 (6.9%)\n",
      "Processing chunk 24,000,000 to 25,000,000 (7.2%)\n",
      "Processing chunk 25,000,000 to 26,000,000 (7.5%)\n",
      "Processing chunk 26,000,000 to 27,000,000 (7.7%)\n",
      "Processing chunk 27,000,000 to 28,000,000 (8.0%)\n",
      "Processing chunk 28,000,000 to 29,000,000 (8.3%)\n",
      "Processing chunk 29,000,000 to 30,000,000 (8.6%)\n",
      "Processing chunk 30,000,000 to 31,000,000 (8.9%)\n",
      "Processing chunk 31,000,000 to 32,000,000 (9.2%)\n",
      "Processing chunk 32,000,000 to 33,000,000 (9.5%)\n",
      "Processing chunk 33,000,000 to 34,000,000 (9.7%)\n",
      "Processing chunk 34,000,000 to 35,000,000 (10.0%)\n",
      "Processing chunk 35,000,000 to 36,000,000 (10.3%)\n",
      "Processing chunk 36,000,000 to 37,000,000 (10.6%)\n",
      "Processing chunk 37,000,000 to 38,000,000 (10.9%)\n",
      "Processing chunk 38,000,000 to 39,000,000 (11.2%)\n",
      "Processing chunk 39,000,000 to 40,000,000 (11.5%)\n",
      "Processing chunk 40,000,000 to 41,000,000 (11.8%)\n",
      "Processing chunk 41,000,000 to 42,000,000 (12.0%)\n",
      "Processing chunk 42,000,000 to 43,000,000 (12.3%)\n",
      "Processing chunk 43,000,000 to 44,000,000 (12.6%)\n",
      "Processing chunk 44,000,000 to 45,000,000 (12.9%)\n",
      "Processing chunk 45,000,000 to 46,000,000 (13.2%)\n",
      "Processing chunk 46,000,000 to 47,000,000 (13.5%)\n",
      "Processing chunk 47,000,000 to 48,000,000 (13.8%)\n",
      "Processing chunk 48,000,000 to 49,000,000 (14.0%)\n",
      "Processing chunk 49,000,000 to 50,000,000 (14.3%)\n",
      "Processing chunk 50,000,000 to 51,000,000 (14.6%)\n",
      "Processing chunk 51,000,000 to 52,000,000 (14.9%)\n",
      "Processing chunk 52,000,000 to 53,000,000 (15.2%)\n",
      "Processing chunk 53,000,000 to 54,000,000 (15.5%)\n",
      "Processing chunk 54,000,000 to 55,000,000 (15.8%)\n",
      "Processing chunk 55,000,000 to 56,000,000 (16.1%)\n",
      "Processing chunk 56,000,000 to 57,000,000 (16.3%)\n",
      "Processing chunk 57,000,000 to 58,000,000 (16.6%)\n",
      "Processing chunk 58,000,000 to 59,000,000 (16.9%)\n",
      "Processing chunk 59,000,000 to 60,000,000 (17.2%)\n",
      "Processing chunk 60,000,000 to 61,000,000 (17.5%)\n",
      "Processing chunk 61,000,000 to 62,000,000 (17.8%)\n",
      "Processing chunk 62,000,000 to 63,000,000 (18.1%)\n",
      "Processing chunk 63,000,000 to 64,000,000 (18.3%)\n",
      "Processing chunk 64,000,000 to 65,000,000 (18.6%)\n",
      "Processing chunk 65,000,000 to 66,000,000 (18.9%)\n",
      "Processing chunk 66,000,000 to 67,000,000 (19.2%)\n",
      "Processing chunk 67,000,000 to 68,000,000 (19.5%)\n",
      "Processing chunk 68,000,000 to 69,000,000 (19.8%)\n",
      "Processing chunk 69,000,000 to 70,000,000 (20.1%)\n",
      "Processing chunk 70,000,000 to 71,000,000 (20.4%)\n",
      "Processing chunk 71,000,000 to 72,000,000 (20.6%)\n",
      "Processing chunk 72,000,000 to 73,000,000 (20.9%)\n",
      "Processing chunk 73,000,000 to 74,000,000 (21.2%)\n",
      "Processing chunk 74,000,000 to 75,000,000 (21.5%)\n",
      "Processing chunk 75,000,000 to 76,000,000 (21.8%)\n",
      "Processing chunk 76,000,000 to 77,000,000 (22.1%)\n",
      "Processing chunk 77,000,000 to 78,000,000 (22.4%)\n",
      "Processing chunk 78,000,000 to 79,000,000 (22.6%)\n",
      "Processing chunk 79,000,000 to 80,000,000 (22.9%)\n",
      "Processing chunk 80,000,000 to 81,000,000 (23.2%)\n",
      "Processing chunk 81,000,000 to 82,000,000 (23.5%)\n",
      "Processing chunk 82,000,000 to 83,000,000 (23.8%)\n",
      "Processing chunk 83,000,000 to 84,000,000 (24.1%)\n",
      "Processing chunk 84,000,000 to 85,000,000 (24.4%)\n",
      "Processing chunk 85,000,000 to 86,000,000 (24.7%)\n",
      "Processing chunk 86,000,000 to 87,000,000 (24.9%)\n",
      "Processing chunk 87,000,000 to 88,000,000 (25.2%)\n",
      "Processing chunk 88,000,000 to 89,000,000 (25.5%)\n",
      "Processing chunk 89,000,000 to 90,000,000 (25.8%)\n",
      "Processing chunk 90,000,000 to 91,000,000 (26.1%)\n",
      "Processing chunk 91,000,000 to 92,000,000 (26.4%)\n",
      "Processing chunk 92,000,000 to 93,000,000 (26.7%)\n",
      "Processing chunk 93,000,000 to 94,000,000 (26.9%)\n",
      "Processing chunk 94,000,000 to 95,000,000 (27.2%)\n",
      "Processing chunk 95,000,000 to 96,000,000 (27.5%)\n",
      "Processing chunk 96,000,000 to 97,000,000 (27.8%)\n",
      "Processing chunk 97,000,000 to 98,000,000 (28.1%)\n",
      "Processing chunk 98,000,000 to 99,000,000 (28.4%)\n",
      "Processing chunk 99,000,000 to 100,000,000 (28.7%)\n",
      "Processing chunk 100,000,000 to 101,000,000 (29.0%)\n",
      "Processing chunk 101,000,000 to 102,000,000 (29.2%)\n",
      "Processing chunk 102,000,000 to 103,000,000 (29.5%)\n",
      "Processing chunk 103,000,000 to 104,000,000 (29.8%)\n",
      "Processing chunk 104,000,000 to 105,000,000 (30.1%)\n",
      "Processing chunk 105,000,000 to 106,000,000 (30.4%)\n",
      "Processing chunk 106,000,000 to 107,000,000 (30.7%)\n",
      "Processing chunk 107,000,000 to 108,000,000 (31.0%)\n",
      "Processing chunk 108,000,000 to 109,000,000 (31.2%)\n",
      "Processing chunk 109,000,000 to 110,000,000 (31.5%)\n",
      "Processing chunk 110,000,000 to 111,000,000 (31.8%)\n",
      "Processing chunk 111,000,000 to 112,000,000 (32.1%)\n",
      "Processing chunk 112,000,000 to 113,000,000 (32.4%)\n",
      "Processing chunk 113,000,000 to 114,000,000 (32.7%)\n",
      "Processing chunk 114,000,000 to 115,000,000 (33.0%)\n",
      "Processing chunk 115,000,000 to 116,000,000 (33.3%)\n",
      "Processing chunk 116,000,000 to 117,000,000 (33.5%)\n",
      "Processing chunk 117,000,000 to 118,000,000 (33.8%)\n",
      "Processing chunk 118,000,000 to 119,000,000 (34.1%)\n",
      "Processing chunk 119,000,000 to 120,000,000 (34.4%)\n",
      "Processing chunk 120,000,000 to 121,000,000 (34.7%)\n",
      "Processing chunk 121,000,000 to 122,000,000 (35.0%)\n",
      "Processing chunk 122,000,000 to 123,000,000 (35.3%)\n",
      "Processing chunk 123,000,000 to 124,000,000 (35.5%)\n",
      "Processing chunk 124,000,000 to 125,000,000 (35.8%)\n",
      "Processing chunk 125,000,000 to 126,000,000 (36.1%)\n",
      "Processing chunk 126,000,000 to 127,000,000 (36.4%)\n",
      "Processing chunk 127,000,000 to 128,000,000 (36.7%)\n",
      "Processing chunk 128,000,000 to 129,000,000 (37.0%)\n",
      "Processing chunk 129,000,000 to 130,000,000 (37.3%)\n",
      "Processing chunk 130,000,000 to 131,000,000 (37.6%)\n",
      "Processing chunk 131,000,000 to 132,000,000 (37.8%)\n",
      "Processing chunk 132,000,000 to 133,000,000 (38.1%)\n",
      "Processing chunk 133,000,000 to 134,000,000 (38.4%)\n",
      "Processing chunk 134,000,000 to 135,000,000 (38.7%)\n",
      "Processing chunk 135,000,000 to 136,000,000 (39.0%)\n",
      "Processing chunk 136,000,000 to 137,000,000 (39.3%)\n",
      "Processing chunk 137,000,000 to 138,000,000 (39.6%)\n",
      "Processing chunk 138,000,000 to 139,000,000 (39.8%)\n",
      "Processing chunk 139,000,000 to 140,000,000 (40.1%)\n",
      "Processing chunk 140,000,000 to 141,000,000 (40.4%)\n",
      "Processing chunk 141,000,000 to 142,000,000 (40.7%)\n",
      "Processing chunk 142,000,000 to 143,000,000 (41.0%)\n",
      "Processing chunk 143,000,000 to 144,000,000 (41.3%)\n",
      "Processing chunk 144,000,000 to 145,000,000 (41.6%)\n",
      "Processing chunk 145,000,000 to 146,000,000 (41.9%)\n",
      "Processing chunk 146,000,000 to 147,000,000 (42.1%)\n",
      "Processing chunk 147,000,000 to 148,000,000 (42.4%)\n",
      "Processing chunk 148,000,000 to 149,000,000 (42.7%)\n",
      "Processing chunk 149,000,000 to 150,000,000 (43.0%)\n",
      "Processing chunk 150,000,000 to 151,000,000 (43.3%)\n",
      "Processing chunk 151,000,000 to 152,000,000 (43.6%)\n",
      "Processing chunk 152,000,000 to 153,000,000 (43.9%)\n",
      "Processing chunk 153,000,000 to 154,000,000 (44.1%)\n",
      "Processing chunk 154,000,000 to 155,000,000 (44.4%)\n",
      "Processing chunk 155,000,000 to 156,000,000 (44.7%)\n",
      "Processing chunk 156,000,000 to 157,000,000 (45.0%)\n",
      "Processing chunk 157,000,000 to 158,000,000 (45.3%)\n",
      "Processing chunk 158,000,000 to 159,000,000 (45.6%)\n",
      "Processing chunk 159,000,000 to 160,000,000 (45.9%)\n",
      "Processing chunk 160,000,000 to 161,000,000 (46.2%)\n",
      "Processing chunk 161,000,000 to 162,000,000 (46.4%)\n",
      "Processing chunk 162,000,000 to 163,000,000 (46.7%)\n",
      "Processing chunk 163,000,000 to 164,000,000 (47.0%)\n",
      "Processing chunk 164,000,000 to 165,000,000 (47.3%)\n",
      "Processing chunk 165,000,000 to 166,000,000 (47.6%)\n",
      "Processing chunk 166,000,000 to 167,000,000 (47.9%)\n",
      "Processing chunk 167,000,000 to 168,000,000 (48.2%)\n",
      "Processing chunk 168,000,000 to 169,000,000 (48.4%)\n",
      "Processing chunk 169,000,000 to 170,000,000 (48.7%)\n",
      "Processing chunk 170,000,000 to 171,000,000 (49.0%)\n",
      "Processing chunk 171,000,000 to 172,000,000 (49.3%)\n",
      "Processing chunk 172,000,000 to 173,000,000 (49.6%)\n",
      "Processing chunk 173,000,000 to 174,000,000 (49.9%)\n",
      "Processing chunk 174,000,000 to 175,000,000 (50.2%)\n",
      "Processing chunk 175,000,000 to 176,000,000 (50.5%)\n",
      "Processing chunk 176,000,000 to 177,000,000 (50.7%)\n",
      "Processing chunk 177,000,000 to 178,000,000 (51.0%)\n",
      "Processing chunk 178,000,000 to 179,000,000 (51.3%)\n",
      "Processing chunk 179,000,000 to 180,000,000 (51.6%)\n",
      "Processing chunk 180,000,000 to 181,000,000 (51.9%)\n",
      "Processing chunk 181,000,000 to 182,000,000 (52.2%)\n",
      "Processing chunk 182,000,000 to 183,000,000 (52.5%)\n",
      "Processing chunk 183,000,000 to 184,000,000 (52.7%)\n",
      "Processing chunk 184,000,000 to 185,000,000 (53.0%)\n",
      "Processing chunk 185,000,000 to 186,000,000 (53.3%)\n",
      "Processing chunk 186,000,000 to 187,000,000 (53.6%)\n",
      "Processing chunk 187,000,000 to 188,000,000 (53.9%)\n",
      "Processing chunk 188,000,000 to 189,000,000 (54.2%)\n",
      "Processing chunk 189,000,000 to 190,000,000 (54.5%)\n",
      "Processing chunk 190,000,000 to 191,000,000 (54.8%)\n",
      "Processing chunk 191,000,000 to 192,000,000 (55.0%)\n",
      "Processing chunk 192,000,000 to 193,000,000 (55.3%)\n",
      "Processing chunk 193,000,000 to 194,000,000 (55.6%)\n",
      "Processing chunk 194,000,000 to 195,000,000 (55.9%)\n",
      "Processing chunk 195,000,000 to 196,000,000 (56.2%)\n",
      "Processing chunk 196,000,000 to 197,000,000 (56.5%)\n",
      "Processing chunk 197,000,000 to 198,000,000 (56.8%)\n",
      "Processing chunk 198,000,000 to 199,000,000 (57.0%)\n",
      "Processing chunk 199,000,000 to 200,000,000 (57.3%)\n",
      "Processing chunk 200,000,000 to 201,000,000 (57.6%)\n",
      "Processing chunk 201,000,000 to 202,000,000 (57.9%)\n",
      "Processing chunk 202,000,000 to 203,000,000 (58.2%)\n",
      "Processing chunk 203,000,000 to 204,000,000 (58.5%)\n",
      "Processing chunk 204,000,000 to 205,000,000 (58.8%)\n",
      "Processing chunk 205,000,000 to 206,000,000 (59.1%)\n",
      "Processing chunk 206,000,000 to 207,000,000 (59.3%)\n",
      "Processing chunk 207,000,000 to 208,000,000 (59.6%)\n",
      "Processing chunk 208,000,000 to 209,000,000 (59.9%)\n",
      "Processing chunk 209,000,000 to 210,000,000 (60.2%)\n",
      "Processing chunk 210,000,000 to 211,000,000 (60.5%)\n",
      "Processing chunk 211,000,000 to 212,000,000 (60.8%)\n",
      "Processing chunk 212,000,000 to 213,000,000 (61.1%)\n",
      "Processing chunk 213,000,000 to 214,000,000 (61.3%)\n",
      "Processing chunk 214,000,000 to 215,000,000 (61.6%)\n",
      "Processing chunk 215,000,000 to 216,000,000 (61.9%)\n",
      "Processing chunk 216,000,000 to 217,000,000 (62.2%)\n",
      "Processing chunk 217,000,000 to 218,000,000 (62.5%)\n",
      "Processing chunk 218,000,000 to 219,000,000 (62.8%)\n",
      "Processing chunk 219,000,000 to 220,000,000 (63.1%)\n",
      "Processing chunk 220,000,000 to 221,000,000 (63.4%)\n",
      "Processing chunk 221,000,000 to 222,000,000 (63.6%)\n",
      "Processing chunk 222,000,000 to 223,000,000 (63.9%)\n",
      "Processing chunk 223,000,000 to 224,000,000 (64.2%)\n",
      "Processing chunk 224,000,000 to 225,000,000 (64.5%)\n",
      "Processing chunk 225,000,000 to 226,000,000 (64.8%)\n",
      "Processing chunk 226,000,000 to 227,000,000 (65.1%)\n",
      "Processing chunk 227,000,000 to 228,000,000 (65.4%)\n",
      "Processing chunk 228,000,000 to 229,000,000 (65.6%)\n",
      "Processing chunk 229,000,000 to 230,000,000 (65.9%)\n",
      "Processing chunk 230,000,000 to 231,000,000 (66.2%)\n",
      "Processing chunk 231,000,000 to 232,000,000 (66.5%)\n",
      "Processing chunk 232,000,000 to 233,000,000 (66.8%)\n",
      "Processing chunk 233,000,000 to 234,000,000 (67.1%)\n",
      "Processing chunk 234,000,000 to 235,000,000 (67.4%)\n",
      "Processing chunk 235,000,000 to 236,000,000 (67.7%)\n",
      "Processing chunk 236,000,000 to 237,000,000 (67.9%)\n",
      "Processing chunk 237,000,000 to 238,000,000 (68.2%)\n",
      "Processing chunk 238,000,000 to 239,000,000 (68.5%)\n",
      "Processing chunk 239,000,000 to 240,000,000 (68.8%)\n",
      "Processing chunk 240,000,000 to 241,000,000 (69.1%)\n",
      "Processing chunk 241,000,000 to 242,000,000 (69.4%)\n",
      "Processing chunk 242,000,000 to 243,000,000 (69.7%)\n",
      "Processing chunk 243,000,000 to 244,000,000 (69.9%)\n",
      "Processing chunk 244,000,000 to 245,000,000 (70.2%)\n",
      "Processing chunk 245,000,000 to 246,000,000 (70.5%)\n",
      "Processing chunk 246,000,000 to 247,000,000 (70.8%)\n",
      "Processing chunk 247,000,000 to 248,000,000 (71.1%)\n",
      "Processing chunk 248,000,000 to 249,000,000 (71.4%)\n",
      "Processing chunk 249,000,000 to 250,000,000 (71.7%)\n",
      "Processing chunk 250,000,000 to 251,000,000 (72.0%)\n",
      "Processing chunk 251,000,000 to 252,000,000 (72.2%)\n",
      "Processing chunk 252,000,000 to 253,000,000 (72.5%)\n",
      "Processing chunk 253,000,000 to 254,000,000 (72.8%)\n",
      "Processing chunk 254,000,000 to 255,000,000 (73.1%)\n",
      "Processing chunk 255,000,000 to 256,000,000 (73.4%)\n",
      "Processing chunk 256,000,000 to 257,000,000 (73.7%)\n",
      "Processing chunk 257,000,000 to 258,000,000 (74.0%)\n",
      "Processing chunk 258,000,000 to 259,000,000 (74.2%)\n",
      "Processing chunk 259,000,000 to 260,000,000 (74.5%)\n",
      "Processing chunk 260,000,000 to 261,000,000 (74.8%)\n",
      "Processing chunk 261,000,000 to 262,000,000 (75.1%)\n",
      "Processing chunk 262,000,000 to 263,000,000 (75.4%)\n",
      "Processing chunk 263,000,000 to 264,000,000 (75.7%)\n",
      "Processing chunk 264,000,000 to 265,000,000 (76.0%)\n",
      "Processing chunk 265,000,000 to 266,000,000 (76.3%)\n",
      "Processing chunk 266,000,000 to 267,000,000 (76.5%)\n",
      "Processing chunk 267,000,000 to 268,000,000 (76.8%)\n",
      "Processing chunk 268,000,000 to 269,000,000 (77.1%)\n",
      "Processing chunk 269,000,000 to 270,000,000 (77.4%)\n",
      "Processing chunk 270,000,000 to 271,000,000 (77.7%)\n",
      "Processing chunk 271,000,000 to 272,000,000 (78.0%)\n",
      "Processing chunk 272,000,000 to 273,000,000 (78.3%)\n",
      "Processing chunk 273,000,000 to 274,000,000 (78.5%)\n",
      "Processing chunk 274,000,000 to 275,000,000 (78.8%)\n",
      "Processing chunk 275,000,000 to 276,000,000 (79.1%)\n",
      "Processing chunk 276,000,000 to 277,000,000 (79.4%)\n",
      "Processing chunk 277,000,000 to 278,000,000 (79.7%)\n",
      "Processing chunk 278,000,000 to 279,000,000 (80.0%)\n",
      "Processing chunk 279,000,000 to 280,000,000 (80.3%)\n",
      "Processing chunk 280,000,000 to 281,000,000 (80.6%)\n",
      "Processing chunk 281,000,000 to 282,000,000 (80.8%)\n",
      "Processing chunk 282,000,000 to 283,000,000 (81.1%)\n",
      "Processing chunk 283,000,000 to 284,000,000 (81.4%)\n",
      "Processing chunk 284,000,000 to 285,000,000 (81.7%)\n",
      "Processing chunk 285,000,000 to 286,000,000 (82.0%)\n",
      "Processing chunk 286,000,000 to 287,000,000 (82.3%)\n",
      "Processing chunk 287,000,000 to 288,000,000 (82.6%)\n",
      "Processing chunk 288,000,000 to 289,000,000 (82.8%)\n",
      "Processing chunk 289,000,000 to 290,000,000 (83.1%)\n",
      "Processing chunk 290,000,000 to 291,000,000 (83.4%)\n",
      "Processing chunk 291,000,000 to 292,000,000 (83.7%)\n",
      "Processing chunk 292,000,000 to 293,000,000 (84.0%)\n",
      "Processing chunk 293,000,000 to 294,000,000 (84.3%)\n",
      "Processing chunk 294,000,000 to 295,000,000 (84.6%)\n",
      "Processing chunk 295,000,000 to 296,000,000 (84.9%)\n",
      "Processing chunk 296,000,000 to 297,000,000 (85.1%)\n",
      "Processing chunk 297,000,000 to 298,000,000 (85.4%)\n",
      "Processing chunk 298,000,000 to 299,000,000 (85.7%)\n",
      "Processing chunk 299,000,000 to 300,000,000 (86.0%)\n",
      "Processing chunk 300,000,000 to 301,000,000 (86.3%)\n",
      "Processing chunk 301,000,000 to 302,000,000 (86.6%)\n",
      "Processing chunk 302,000,000 to 303,000,000 (86.9%)\n",
      "Processing chunk 303,000,000 to 304,000,000 (87.1%)\n",
      "Processing chunk 304,000,000 to 305,000,000 (87.4%)\n",
      "Processing chunk 305,000,000 to 306,000,000 (87.7%)\n",
      "Processing chunk 306,000,000 to 307,000,000 (88.0%)\n",
      "Processing chunk 307,000,000 to 308,000,000 (88.3%)\n",
      "Processing chunk 308,000,000 to 309,000,000 (88.6%)\n",
      "Processing chunk 309,000,000 to 310,000,000 (88.9%)\n",
      "Processing chunk 310,000,000 to 311,000,000 (89.2%)\n",
      "Processing chunk 311,000,000 to 312,000,000 (89.4%)\n",
      "Processing chunk 312,000,000 to 313,000,000 (89.7%)\n",
      "Processing chunk 313,000,000 to 314,000,000 (90.0%)\n",
      "Processing chunk 314,000,000 to 315,000,000 (90.3%)\n",
      "Processing chunk 315,000,000 to 316,000,000 (90.6%)\n",
      "Processing chunk 316,000,000 to 317,000,000 (90.9%)\n",
      "Processing chunk 317,000,000 to 318,000,000 (91.2%)\n",
      "Processing chunk 318,000,000 to 319,000,000 (91.4%)\n",
      "Processing chunk 319,000,000 to 320,000,000 (91.7%)\n",
      "Processing chunk 320,000,000 to 321,000,000 (92.0%)\n",
      "Processing chunk 321,000,000 to 322,000,000 (92.3%)\n",
      "Processing chunk 322,000,000 to 323,000,000 (92.6%)\n",
      "Processing chunk 323,000,000 to 324,000,000 (92.9%)\n",
      "Processing chunk 324,000,000 to 325,000,000 (93.2%)\n",
      "Processing chunk 325,000,000 to 326,000,000 (93.5%)\n",
      "Processing chunk 326,000,000 to 327,000,000 (93.7%)\n",
      "Processing chunk 327,000,000 to 328,000,000 (94.0%)\n",
      "Processing chunk 328,000,000 to 329,000,000 (94.3%)\n",
      "Processing chunk 329,000,000 to 330,000,000 (94.6%)\n",
      "Processing chunk 330,000,000 to 331,000,000 (94.9%)\n",
      "Processing chunk 331,000,000 to 332,000,000 (95.2%)\n",
      "Processing chunk 332,000,000 to 333,000,000 (95.5%)\n",
      "Processing chunk 333,000,000 to 334,000,000 (95.7%)\n",
      "Processing chunk 334,000,000 to 335,000,000 (96.0%)\n",
      "Processing chunk 335,000,000 to 336,000,000 (96.3%)\n",
      "Processing chunk 336,000,000 to 337,000,000 (96.6%)\n",
      "Processing chunk 337,000,000 to 338,000,000 (96.9%)\n",
      "Processing chunk 338,000,000 to 339,000,000 (97.2%)\n",
      "Processing chunk 339,000,000 to 340,000,000 (97.5%)\n",
      "Processing chunk 340,000,000 to 341,000,000 (97.8%)\n",
      "Processing chunk 341,000,000 to 342,000,000 (98.0%)\n",
      "Processing chunk 342,000,000 to 343,000,000 (98.3%)\n",
      "Processing chunk 343,000,000 to 344,000,000 (98.6%)\n",
      "Processing chunk 344,000,000 to 345,000,000 (98.9%)\n",
      "Processing chunk 345,000,000 to 346,000,000 (99.2%)\n",
      "Processing chunk 346,000,000 to 347,000,000 (99.5%)\n",
      "Processing chunk 347,000,000 to 348,000,000 (99.8%)\n",
      "Processing chunk 348,000,000 to 348,835,000 (100.0%)\n",
      "Combining chunks...\n",
      "Adjusted 'Time' column by GMT+2 hours\n",
      "Successfully processed large file with chunked reading\n",
      "Successfully loaded data with 174,417,500 samples and 3 channels\n",
      "All channels found in part3: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time columns: ['Time']\n",
      "Signal channels (2): ['Voltage_0', 'Voltage_1']\n",
      "Preserving column 'Time' (not converting)\n",
      "Found 2 signal channels to convert: ['Voltage_0', 'Voltage_1']\n",
      "Found 1 non-signal columns to preserve: ['Time']\n",
      "Converting voltage signals to nanoTesla using factor: 20 nT/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting channels: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage to nanoTesla conversion completed. Converted 2 channels.\n",
      "Signal values are now in nanoTesla (nT) units.\n",
      "Voltage conversion completed for part3\n",
      "Saving part3 data to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part3.parquet...\n",
      "Successfully saved 174,417,500 samples from part3 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part3.parquet\n",
      "DataFrame shape for part3: (174417500, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range for part3: 2021-12-14 22:35:22.120750 to 2021-12-15 06:58:26.370650\n",
      "Duration: 8.38 hours\n",
      "Output file size for part3: 900.6 MB\n",
      "\n",
      "Processing part4 file: ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_12_15\\fruit_9.tdms\n",
      "File size: 8753.4 MB\n",
      "Large file detected. Will apply downsampling factor: 2\n",
      "Reading ..\\..\\..\\Data\\RawData\\Volunteers\\2021\\2021_12_15\\fruit_9.tdms...\n",
      "Total data length: 286,480,000 samples\n",
      "Estimated memory requirement: 6557.0 MB\n",
      "Large file detected, using chunked reading approach...\n",
      "Applying downsampling factor: 2\n",
      "Processing chunk 0 to 1,000,000 (0.3%)\n",
      "Processing chunk 1,000,000 to 2,000,000 (0.7%)\n",
      "Processing chunk 2,000,000 to 3,000,000 (1.0%)\n",
      "Processing chunk 3,000,000 to 4,000,000 (1.4%)\n",
      "Processing chunk 4,000,000 to 5,000,000 (1.7%)\n",
      "Processing chunk 5,000,000 to 6,000,000 (2.1%)\n",
      "Processing chunk 6,000,000 to 7,000,000 (2.4%)\n",
      "Processing chunk 7,000,000 to 8,000,000 (2.8%)\n",
      "Processing chunk 8,000,000 to 9,000,000 (3.1%)\n",
      "Processing chunk 9,000,000 to 10,000,000 (3.5%)\n",
      "Processing chunk 10,000,000 to 11,000,000 (3.8%)\n",
      "Processing chunk 11,000,000 to 12,000,000 (4.2%)\n",
      "Processing chunk 12,000,000 to 13,000,000 (4.5%)\n",
      "Processing chunk 13,000,000 to 14,000,000 (4.9%)\n",
      "Processing chunk 14,000,000 to 15,000,000 (5.2%)\n",
      "Processing chunk 15,000,000 to 16,000,000 (5.6%)\n",
      "Processing chunk 16,000,000 to 17,000,000 (5.9%)\n",
      "Processing chunk 17,000,000 to 18,000,000 (6.3%)\n",
      "Processing chunk 18,000,000 to 19,000,000 (6.6%)\n",
      "Processing chunk 19,000,000 to 20,000,000 (7.0%)\n",
      "Processing chunk 20,000,000 to 21,000,000 (7.3%)\n",
      "Processing chunk 21,000,000 to 22,000,000 (7.7%)\n",
      "Processing chunk 22,000,000 to 23,000,000 (8.0%)\n",
      "Processing chunk 23,000,000 to 24,000,000 (8.4%)\n",
      "Processing chunk 24,000,000 to 25,000,000 (8.7%)\n",
      "Processing chunk 25,000,000 to 26,000,000 (9.1%)\n",
      "Processing chunk 26,000,000 to 27,000,000 (9.4%)\n",
      "Processing chunk 27,000,000 to 28,000,000 (9.8%)\n",
      "Processing chunk 28,000,000 to 29,000,000 (10.1%)\n",
      "Processing chunk 29,000,000 to 30,000,000 (10.5%)\n",
      "Processing chunk 30,000,000 to 31,000,000 (10.8%)\n",
      "Processing chunk 31,000,000 to 32,000,000 (11.2%)\n",
      "Processing chunk 32,000,000 to 33,000,000 (11.5%)\n",
      "Processing chunk 33,000,000 to 34,000,000 (11.9%)\n",
      "Processing chunk 34,000,000 to 35,000,000 (12.2%)\n",
      "Processing chunk 35,000,000 to 36,000,000 (12.6%)\n",
      "Processing chunk 36,000,000 to 37,000,000 (12.9%)\n",
      "Processing chunk 37,000,000 to 38,000,000 (13.3%)\n",
      "Processing chunk 38,000,000 to 39,000,000 (13.6%)\n",
      "Processing chunk 39,000,000 to 40,000,000 (14.0%)\n",
      "Processing chunk 40,000,000 to 41,000,000 (14.3%)\n",
      "Processing chunk 41,000,000 to 42,000,000 (14.7%)\n",
      "Processing chunk 42,000,000 to 43,000,000 (15.0%)\n",
      "Processing chunk 43,000,000 to 44,000,000 (15.4%)\n",
      "Processing chunk 44,000,000 to 45,000,000 (15.7%)\n",
      "Processing chunk 45,000,000 to 46,000,000 (16.1%)\n",
      "Processing chunk 46,000,000 to 47,000,000 (16.4%)\n",
      "Processing chunk 47,000,000 to 48,000,000 (16.8%)\n",
      "Processing chunk 48,000,000 to 49,000,000 (17.1%)\n",
      "Processing chunk 49,000,000 to 50,000,000 (17.5%)\n",
      "Processing chunk 50,000,000 to 51,000,000 (17.8%)\n",
      "Processing chunk 51,000,000 to 52,000,000 (18.2%)\n",
      "Processing chunk 52,000,000 to 53,000,000 (18.5%)\n",
      "Processing chunk 53,000,000 to 54,000,000 (18.8%)\n",
      "Processing chunk 54,000,000 to 55,000,000 (19.2%)\n",
      "Processing chunk 55,000,000 to 56,000,000 (19.5%)\n",
      "Processing chunk 56,000,000 to 57,000,000 (19.9%)\n",
      "Processing chunk 57,000,000 to 58,000,000 (20.2%)\n",
      "Processing chunk 58,000,000 to 59,000,000 (20.6%)\n",
      "Processing chunk 59,000,000 to 60,000,000 (20.9%)\n",
      "Processing chunk 60,000,000 to 61,000,000 (21.3%)\n",
      "Processing chunk 61,000,000 to 62,000,000 (21.6%)\n",
      "Processing chunk 62,000,000 to 63,000,000 (22.0%)\n",
      "Processing chunk 63,000,000 to 64,000,000 (22.3%)\n",
      "Processing chunk 64,000,000 to 65,000,000 (22.7%)\n",
      "Processing chunk 65,000,000 to 66,000,000 (23.0%)\n",
      "Processing chunk 66,000,000 to 67,000,000 (23.4%)\n",
      "Processing chunk 67,000,000 to 68,000,000 (23.7%)\n",
      "Processing chunk 68,000,000 to 69,000,000 (24.1%)\n",
      "Processing chunk 69,000,000 to 70,000,000 (24.4%)\n",
      "Processing chunk 70,000,000 to 71,000,000 (24.8%)\n",
      "Processing chunk 71,000,000 to 72,000,000 (25.1%)\n",
      "Processing chunk 72,000,000 to 73,000,000 (25.5%)\n",
      "Processing chunk 73,000,000 to 74,000,000 (25.8%)\n",
      "Processing chunk 74,000,000 to 75,000,000 (26.2%)\n",
      "Processing chunk 75,000,000 to 76,000,000 (26.5%)\n",
      "Processing chunk 76,000,000 to 77,000,000 (26.9%)\n",
      "Processing chunk 77,000,000 to 78,000,000 (27.2%)\n",
      "Processing chunk 78,000,000 to 79,000,000 (27.6%)\n",
      "Processing chunk 79,000,000 to 80,000,000 (27.9%)\n",
      "Processing chunk 80,000,000 to 81,000,000 (28.3%)\n",
      "Processing chunk 81,000,000 to 82,000,000 (28.6%)\n",
      "Processing chunk 82,000,000 to 83,000,000 (29.0%)\n",
      "Processing chunk 83,000,000 to 84,000,000 (29.3%)\n",
      "Processing chunk 84,000,000 to 85,000,000 (29.7%)\n",
      "Processing chunk 85,000,000 to 86,000,000 (30.0%)\n",
      "Processing chunk 86,000,000 to 87,000,000 (30.4%)\n",
      "Processing chunk 87,000,000 to 88,000,000 (30.7%)\n",
      "Processing chunk 88,000,000 to 89,000,000 (31.1%)\n",
      "Processing chunk 89,000,000 to 90,000,000 (31.4%)\n",
      "Processing chunk 90,000,000 to 91,000,000 (31.8%)\n",
      "Processing chunk 91,000,000 to 92,000,000 (32.1%)\n",
      "Processing chunk 92,000,000 to 93,000,000 (32.5%)\n",
      "Processing chunk 93,000,000 to 94,000,000 (32.8%)\n",
      "Processing chunk 94,000,000 to 95,000,000 (33.2%)\n",
      "Processing chunk 95,000,000 to 96,000,000 (33.5%)\n",
      "Processing chunk 96,000,000 to 97,000,000 (33.9%)\n",
      "Processing chunk 97,000,000 to 98,000,000 (34.2%)\n",
      "Processing chunk 98,000,000 to 99,000,000 (34.6%)\n",
      "Processing chunk 99,000,000 to 100,000,000 (34.9%)\n",
      "Processing chunk 100,000,000 to 101,000,000 (35.3%)\n",
      "Processing chunk 101,000,000 to 102,000,000 (35.6%)\n",
      "Processing chunk 102,000,000 to 103,000,000 (36.0%)\n",
      "Processing chunk 103,000,000 to 104,000,000 (36.3%)\n",
      "Processing chunk 104,000,000 to 105,000,000 (36.7%)\n",
      "Processing chunk 105,000,000 to 106,000,000 (37.0%)\n",
      "Processing chunk 106,000,000 to 107,000,000 (37.3%)\n",
      "Processing chunk 107,000,000 to 108,000,000 (37.7%)\n",
      "Processing chunk 108,000,000 to 109,000,000 (38.0%)\n",
      "Processing chunk 109,000,000 to 110,000,000 (38.4%)\n",
      "Processing chunk 110,000,000 to 111,000,000 (38.7%)\n",
      "Processing chunk 111,000,000 to 112,000,000 (39.1%)\n",
      "Processing chunk 112,000,000 to 113,000,000 (39.4%)\n",
      "Processing chunk 113,000,000 to 114,000,000 (39.8%)\n",
      "Processing chunk 114,000,000 to 115,000,000 (40.1%)\n",
      "Processing chunk 115,000,000 to 116,000,000 (40.5%)\n",
      "Processing chunk 116,000,000 to 117,000,000 (40.8%)\n",
      "Processing chunk 117,000,000 to 118,000,000 (41.2%)\n",
      "Processing chunk 118,000,000 to 119,000,000 (41.5%)\n",
      "Processing chunk 119,000,000 to 120,000,000 (41.9%)\n",
      "Processing chunk 120,000,000 to 121,000,000 (42.2%)\n",
      "Processing chunk 121,000,000 to 122,000,000 (42.6%)\n",
      "Processing chunk 122,000,000 to 123,000,000 (42.9%)\n",
      "Processing chunk 123,000,000 to 124,000,000 (43.3%)\n",
      "Processing chunk 124,000,000 to 125,000,000 (43.6%)\n",
      "Processing chunk 125,000,000 to 126,000,000 (44.0%)\n",
      "Processing chunk 126,000,000 to 127,000,000 (44.3%)\n",
      "Processing chunk 127,000,000 to 128,000,000 (44.7%)\n",
      "Processing chunk 128,000,000 to 129,000,000 (45.0%)\n",
      "Processing chunk 129,000,000 to 130,000,000 (45.4%)\n",
      "Processing chunk 130,000,000 to 131,000,000 (45.7%)\n",
      "Processing chunk 131,000,000 to 132,000,000 (46.1%)\n",
      "Processing chunk 132,000,000 to 133,000,000 (46.4%)\n",
      "Processing chunk 133,000,000 to 134,000,000 (46.8%)\n",
      "Processing chunk 134,000,000 to 135,000,000 (47.1%)\n",
      "Processing chunk 135,000,000 to 136,000,000 (47.5%)\n",
      "Processing chunk 136,000,000 to 137,000,000 (47.8%)\n",
      "Processing chunk 137,000,000 to 138,000,000 (48.2%)\n",
      "Processing chunk 138,000,000 to 139,000,000 (48.5%)\n",
      "Processing chunk 139,000,000 to 140,000,000 (48.9%)\n",
      "Processing chunk 140,000,000 to 141,000,000 (49.2%)\n",
      "Processing chunk 141,000,000 to 142,000,000 (49.6%)\n",
      "Processing chunk 142,000,000 to 143,000,000 (49.9%)\n",
      "Processing chunk 143,000,000 to 144,000,000 (50.3%)\n",
      "Processing chunk 144,000,000 to 145,000,000 (50.6%)\n",
      "Processing chunk 145,000,000 to 146,000,000 (51.0%)\n",
      "Processing chunk 146,000,000 to 147,000,000 (51.3%)\n",
      "Processing chunk 147,000,000 to 148,000,000 (51.7%)\n",
      "Processing chunk 148,000,000 to 149,000,000 (52.0%)\n",
      "Processing chunk 149,000,000 to 150,000,000 (52.4%)\n",
      "Processing chunk 150,000,000 to 151,000,000 (52.7%)\n",
      "Processing chunk 151,000,000 to 152,000,000 (53.1%)\n",
      "Processing chunk 152,000,000 to 153,000,000 (53.4%)\n",
      "Processing chunk 153,000,000 to 154,000,000 (53.8%)\n",
      "Processing chunk 154,000,000 to 155,000,000 (54.1%)\n",
      "Processing chunk 155,000,000 to 156,000,000 (54.5%)\n",
      "Processing chunk 156,000,000 to 157,000,000 (54.8%)\n",
      "Processing chunk 157,000,000 to 158,000,000 (55.2%)\n",
      "Processing chunk 158,000,000 to 159,000,000 (55.5%)\n",
      "Processing chunk 159,000,000 to 160,000,000 (55.9%)\n",
      "Processing chunk 160,000,000 to 161,000,000 (56.2%)\n",
      "Processing chunk 161,000,000 to 162,000,000 (56.5%)\n",
      "Processing chunk 162,000,000 to 163,000,000 (56.9%)\n",
      "Processing chunk 163,000,000 to 164,000,000 (57.2%)\n",
      "Processing chunk 164,000,000 to 165,000,000 (57.6%)\n",
      "Processing chunk 165,000,000 to 166,000,000 (57.9%)\n",
      "Processing chunk 166,000,000 to 167,000,000 (58.3%)\n",
      "Processing chunk 167,000,000 to 168,000,000 (58.6%)\n",
      "Processing chunk 168,000,000 to 169,000,000 (59.0%)\n",
      "Processing chunk 169,000,000 to 170,000,000 (59.3%)\n",
      "Processing chunk 170,000,000 to 171,000,000 (59.7%)\n",
      "Processing chunk 171,000,000 to 172,000,000 (60.0%)\n",
      "Processing chunk 172,000,000 to 173,000,000 (60.4%)\n",
      "Processing chunk 173,000,000 to 174,000,000 (60.7%)\n",
      "Processing chunk 174,000,000 to 175,000,000 (61.1%)\n",
      "Processing chunk 175,000,000 to 176,000,000 (61.4%)\n",
      "Processing chunk 176,000,000 to 177,000,000 (61.8%)\n",
      "Processing chunk 177,000,000 to 178,000,000 (62.1%)\n",
      "Processing chunk 178,000,000 to 179,000,000 (62.5%)\n",
      "Processing chunk 179,000,000 to 180,000,000 (62.8%)\n",
      "Processing chunk 180,000,000 to 181,000,000 (63.2%)\n",
      "Processing chunk 181,000,000 to 182,000,000 (63.5%)\n",
      "Processing chunk 182,000,000 to 183,000,000 (63.9%)\n",
      "Processing chunk 183,000,000 to 184,000,000 (64.2%)\n",
      "Processing chunk 184,000,000 to 185,000,000 (64.6%)\n",
      "Processing chunk 185,000,000 to 186,000,000 (64.9%)\n",
      "Processing chunk 186,000,000 to 187,000,000 (65.3%)\n",
      "Processing chunk 187,000,000 to 188,000,000 (65.6%)\n",
      "Processing chunk 188,000,000 to 189,000,000 (66.0%)\n",
      "Processing chunk 189,000,000 to 190,000,000 (66.3%)\n",
      "Processing chunk 190,000,000 to 191,000,000 (66.7%)\n",
      "Processing chunk 191,000,000 to 192,000,000 (67.0%)\n",
      "Processing chunk 192,000,000 to 193,000,000 (67.4%)\n",
      "Processing chunk 193,000,000 to 194,000,000 (67.7%)\n",
      "Processing chunk 194,000,000 to 195,000,000 (68.1%)\n",
      "Processing chunk 195,000,000 to 196,000,000 (68.4%)\n",
      "Processing chunk 196,000,000 to 197,000,000 (68.8%)\n",
      "Processing chunk 197,000,000 to 198,000,000 (69.1%)\n",
      "Processing chunk 198,000,000 to 199,000,000 (69.5%)\n",
      "Processing chunk 199,000,000 to 200,000,000 (69.8%)\n",
      "Processing chunk 200,000,000 to 201,000,000 (70.2%)\n",
      "Processing chunk 201,000,000 to 202,000,000 (70.5%)\n",
      "Processing chunk 202,000,000 to 203,000,000 (70.9%)\n",
      "Processing chunk 203,000,000 to 204,000,000 (71.2%)\n",
      "Processing chunk 204,000,000 to 205,000,000 (71.6%)\n",
      "Processing chunk 205,000,000 to 206,000,000 (71.9%)\n",
      "Processing chunk 206,000,000 to 207,000,000 (72.3%)\n",
      "Processing chunk 207,000,000 to 208,000,000 (72.6%)\n",
      "Processing chunk 208,000,000 to 209,000,000 (73.0%)\n",
      "Processing chunk 209,000,000 to 210,000,000 (73.3%)\n",
      "Processing chunk 210,000,000 to 211,000,000 (73.7%)\n",
      "Processing chunk 211,000,000 to 212,000,000 (74.0%)\n",
      "Processing chunk 212,000,000 to 213,000,000 (74.4%)\n",
      "Processing chunk 213,000,000 to 214,000,000 (74.7%)\n",
      "Processing chunk 214,000,000 to 215,000,000 (75.0%)\n",
      "Processing chunk 215,000,000 to 216,000,000 (75.4%)\n",
      "Processing chunk 216,000,000 to 217,000,000 (75.7%)\n",
      "Processing chunk 217,000,000 to 218,000,000 (76.1%)\n",
      "Processing chunk 218,000,000 to 219,000,000 (76.4%)\n",
      "Processing chunk 219,000,000 to 220,000,000 (76.8%)\n",
      "Processing chunk 220,000,000 to 221,000,000 (77.1%)\n",
      "Processing chunk 221,000,000 to 222,000,000 (77.5%)\n",
      "Processing chunk 222,000,000 to 223,000,000 (77.8%)\n",
      "Processing chunk 223,000,000 to 224,000,000 (78.2%)\n",
      "Processing chunk 224,000,000 to 225,000,000 (78.5%)\n",
      "Processing chunk 225,000,000 to 226,000,000 (78.9%)\n",
      "Processing chunk 226,000,000 to 227,000,000 (79.2%)\n",
      "Processing chunk 227,000,000 to 228,000,000 (79.6%)\n",
      "Processing chunk 228,000,000 to 229,000,000 (79.9%)\n",
      "Processing chunk 229,000,000 to 230,000,000 (80.3%)\n",
      "Processing chunk 230,000,000 to 231,000,000 (80.6%)\n",
      "Processing chunk 231,000,000 to 232,000,000 (81.0%)\n",
      "Processing chunk 232,000,000 to 233,000,000 (81.3%)\n",
      "Processing chunk 233,000,000 to 234,000,000 (81.7%)\n",
      "Processing chunk 234,000,000 to 235,000,000 (82.0%)\n",
      "Processing chunk 235,000,000 to 236,000,000 (82.4%)\n",
      "Processing chunk 236,000,000 to 237,000,000 (82.7%)\n",
      "Processing chunk 237,000,000 to 238,000,000 (83.1%)\n",
      "Processing chunk 238,000,000 to 239,000,000 (83.4%)\n",
      "Processing chunk 239,000,000 to 240,000,000 (83.8%)\n",
      "Processing chunk 240,000,000 to 241,000,000 (84.1%)\n",
      "Processing chunk 241,000,000 to 242,000,000 (84.5%)\n",
      "Processing chunk 242,000,000 to 243,000,000 (84.8%)\n",
      "Processing chunk 243,000,000 to 244,000,000 (85.2%)\n",
      "Processing chunk 244,000,000 to 245,000,000 (85.5%)\n",
      "Processing chunk 245,000,000 to 246,000,000 (85.9%)\n",
      "Processing chunk 246,000,000 to 247,000,000 (86.2%)\n",
      "Processing chunk 247,000,000 to 248,000,000 (86.6%)\n",
      "Processing chunk 248,000,000 to 249,000,000 (86.9%)\n",
      "Processing chunk 249,000,000 to 250,000,000 (87.3%)\n",
      "Processing chunk 250,000,000 to 251,000,000 (87.6%)\n",
      "Processing chunk 251,000,000 to 252,000,000 (88.0%)\n",
      "Processing chunk 252,000,000 to 253,000,000 (88.3%)\n",
      "Processing chunk 253,000,000 to 254,000,000 (88.7%)\n",
      "Processing chunk 254,000,000 to 255,000,000 (89.0%)\n",
      "Processing chunk 255,000,000 to 256,000,000 (89.4%)\n",
      "Processing chunk 256,000,000 to 257,000,000 (89.7%)\n",
      "Processing chunk 257,000,000 to 258,000,000 (90.1%)\n",
      "Processing chunk 258,000,000 to 259,000,000 (90.4%)\n",
      "Processing chunk 259,000,000 to 260,000,000 (90.8%)\n",
      "Processing chunk 260,000,000 to 261,000,000 (91.1%)\n",
      "Processing chunk 261,000,000 to 262,000,000 (91.5%)\n",
      "Processing chunk 262,000,000 to 263,000,000 (91.8%)\n",
      "Processing chunk 263,000,000 to 264,000,000 (92.2%)\n",
      "Processing chunk 264,000,000 to 265,000,000 (92.5%)\n",
      "Processing chunk 265,000,000 to 266,000,000 (92.9%)\n",
      "Processing chunk 266,000,000 to 267,000,000 (93.2%)\n",
      "Processing chunk 267,000,000 to 268,000,000 (93.5%)\n",
      "Processing chunk 268,000,000 to 269,000,000 (93.9%)\n",
      "Processing chunk 269,000,000 to 270,000,000 (94.2%)\n",
      "Processing chunk 270,000,000 to 271,000,000 (94.6%)\n",
      "Processing chunk 271,000,000 to 272,000,000 (94.9%)\n",
      "Processing chunk 272,000,000 to 273,000,000 (95.3%)\n",
      "Processing chunk 273,000,000 to 274,000,000 (95.6%)\n",
      "Processing chunk 274,000,000 to 275,000,000 (96.0%)\n",
      "Processing chunk 275,000,000 to 276,000,000 (96.3%)\n",
      "Processing chunk 276,000,000 to 277,000,000 (96.7%)\n",
      "Processing chunk 277,000,000 to 278,000,000 (97.0%)\n",
      "Processing chunk 278,000,000 to 279,000,000 (97.4%)\n",
      "Processing chunk 279,000,000 to 280,000,000 (97.7%)\n",
      "Processing chunk 280,000,000 to 281,000,000 (98.1%)\n",
      "Processing chunk 281,000,000 to 282,000,000 (98.4%)\n",
      "Processing chunk 282,000,000 to 283,000,000 (98.8%)\n",
      "Processing chunk 283,000,000 to 284,000,000 (99.1%)\n",
      "Processing chunk 284,000,000 to 285,000,000 (99.5%)\n",
      "Processing chunk 285,000,000 to 286,000,000 (99.8%)\n",
      "Processing chunk 286,000,000 to 286,480,000 (100.0%)\n",
      "Combining chunks...\n",
      "Adjusted 'Time' column by GMT+2 hours\n",
      "Successfully processed large file with chunked reading\n",
      "Successfully loaded data with 143,240,000 samples and 3 channels\n",
      "All channels found in part4: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time columns: ['Time']\n",
      "Signal channels (2): ['Voltage_0', 'Voltage_1']\n",
      "Preserving column 'Time' (not converting)\n",
      "Found 2 signal channels to convert: ['Voltage_0', 'Voltage_1']\n",
      "Found 1 non-signal columns to preserve: ['Time']\n",
      "Converting voltage signals to nanoTesla using factor: 20 nT/V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting channels: 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage to nanoTesla conversion completed. Converted 2 channels.\n",
      "Signal values are now in nanoTesla (nT) units.\n",
      "Voltage conversion completed for part4\n",
      "Saving part4 data to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part4.parquet...\n",
      "Successfully saved 143,240,000 samples from part4 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part4.parquet\n",
      "DataFrame shape for part4: (143240000, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range for part4: 2021-12-15 21:29:29.463300 to 2021-12-16 07:23:24.963100\n",
      "Duration: 9.90 hours\n",
      "Output file size for part4: 1008.3 MB\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "Successfully processed 4 TDMS files:\n",
      "\n",
      "part1:\n",
      "  - File: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part1.parquet\n",
      "  - Shape: (122502500, 3)\n",
      "  - Time range: 2021-11-16 22:24:59.171050 to 2021-11-17 04:49:32.170950\n",
      "  - Duration: 6.41 hours\n",
      "\n",
      "part2:\n",
      "  - File: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part2.parquet\n",
      "  - Shape: (131262500, 3)\n",
      "  - Time range: 2021-12-07 22:19:36.521900 to 2021-12-08 06:51:52.271800\n",
      "  - Duration: 8.54 hours\n",
      "\n",
      "part3:\n",
      "  - File: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part3.parquet\n",
      "  - Shape: (174417500, 3)\n",
      "  - Time range: 2021-12-14 22:35:22.120750 to 2021-12-15 06:58:26.370650\n",
      "  - Duration: 8.38 hours\n",
      "\n",
      "part4:\n",
      "  - File: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part4.parquet\n",
      "  - Shape: (143240000, 3)\n",
      "  - Time range: 2021-12-15 21:29:29.463300 to 2021-12-16 07:23:24.963100\n",
      "  - Duration: 9.90 hours\n",
      "\n",
      "Individual TDMS processing completed!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the anti-aliasing filter function",
   "id": "ff9b5af40bd25829"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T17:12:33.045600Z",
     "start_time": "2025-08-11T17:12:33.031800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_antialiasing_filter(df, lowcut_freq=0.05, highcut_freq=10, sampling_freq=5000, filter_order=6):\n",
    "    \"\"\"\n",
    "    Apply low-pass and high-pass filters separately with improved stability.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with signal data\n",
    "    - lowcut_freq: low cutoff frequency in Hz (default: 0.05Hz)\n",
    "    - highcut_freq: high cutoff frequency in Hz (default: 10Hz)\n",
    "    - sampling_freq: sampling frequency in Hz (default: 5000Hz)\n",
    "    - filter_order: filter order (default: 6)\n",
    "\n",
    "    Returns:\n",
    "    - df_filtered: polars DataFrame with filtered signals\n",
    "    \"\"\"\n",
    "    # Calculate normalized cutoff frequencies\n",
    "    nyq = 0.5 * sampling_freq\n",
    "    high_normal = highcut_freq / nyq\n",
    "\n",
    "    # Design Butterworth low-pass filter\n",
    "    b_low, a_low = butter(filter_order, high_normal, btype='low', analog=False)\n",
    "\n",
    "    # Design high-pass filter with improved stability\n",
    "    apply_highpass = lowcut_freq > 0\n",
    "    if apply_highpass:\n",
    "        low_normal = lowcut_freq / nyq\n",
    "        print(f\"High-pass normalized frequency: {low_normal:.6f}\")\n",
    "\n",
    "        # Use lower filter order for very low frequencies to improve stability\n",
    "        hp_filter_order = min(filter_order, 4) if low_normal < 0.001 else filter_order\n",
    "\n",
    "        # Design high-pass filter with SOS (Second-Order Sections) for better numerical stability\n",
    "        sos_high = butter(hp_filter_order, low_normal, btype='high', analog=False, output='sos')\n",
    "        print(f\"Using filter order {hp_filter_order} for high-pass filter\")\n",
    "\n",
    "    # Get all signal channel names (exclude time column)\n",
    "    signal_column_names = []\n",
    "    for channel_group in signal_channels.values():\n",
    "        signal_column_names.extend(channel_group)\n",
    "\n",
    "    # Apply filter to each signal channel\n",
    "    filtered_data = {}\n",
    "\n",
    "    # Keep the time column unchanged\n",
    "    time_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is not None:\n",
    "        filtered_data[time_col] = df[time_col].to_numpy()\n",
    "        print(f\"Time column '{time_col}' preserved in filtered data\")\n",
    "    else:\n",
    "        print(\"Warning: No time column found in input data\")\n",
    "\n",
    "    filter_description = f\"low-pass ({highcut_freq}Hz)\"\n",
    "    if apply_highpass:\n",
    "        filter_description = f\"high-pass ({lowcut_freq}Hz) + {filter_description}\"\n",
    "\n",
    "    print(f\"Applying {filter_description} filters to channels...\")\n",
    "\n",
    "    for channel in tqdm(signal_column_names, desc=\"Filtering channels\"):\n",
    "        if channel in df.columns:\n",
    "            # Extract signal data\n",
    "            signal = df[channel].to_numpy()\n",
    "\n",
    "            # First apply low-pass filter\n",
    "            filtered_signal = filtfilt(b_low, a_low, signal)\n",
    "\n",
    "            # Then apply high-pass filter if needed using SOS format\n",
    "            if apply_highpass:\n",
    "                from scipy.signal import sosfiltfilt\n",
    "                filtered_signal = sosfiltfilt(sos_high, filtered_signal)\n",
    "\n",
    "            # Check for NaN values and report\n",
    "            if np.any(np.isnan(filtered_signal)):\n",
    "                print(f\"Warning: NaN values detected in {channel} after filtering\")\n",
    "                # Option: replace NaN with interpolated values or skip this channel\n",
    "                nan_count = np.sum(np.isnan(filtered_signal))\n",
    "                print(f\"  {nan_count} NaN values out of {len(filtered_signal)} samples\")\n",
    "\n",
    "                # Simple NaN handling: replace with median of non-NaN values\n",
    "                if nan_count < len(filtered_signal) * 0.1:  # Less than 10% NaN\n",
    "                    median_val = np.nanmedian(filtered_signal)\n",
    "                    filtered_signal = np.where(np.isnan(filtered_signal), median_val, filtered_signal)\n",
    "                    print(f\"  Replaced NaN values with median: {median_val:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  Too many NaN values ({nan_count}/{len(filtered_signal)}), skipping channel\")\n",
    "                    continue\n",
    "\n",
    "            # Store filtered signal\n",
    "            filtered_data[channel] = filtered_signal\n",
    "        else:\n",
    "            print(f\"Warning: Channel '{channel}' not found in data\")\n",
    "\n",
    "    # Convert back to polars DataFrame\n",
    "    df_filtered = pl.DataFrame(filtered_data)\n",
    "\n",
    "    print(f\"Filtering completed successfully. Processed {len(signal_column_names)} channels.\")\n",
    "    return df_filtered\n"
   ],
   "id": "ed6713389248c313",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the downsampling function",
   "id": "615fdcdbc9462d94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T17:12:39.802105Z",
     "start_time": "2025-08-11T17:12:39.785889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def downsample_data(df, original_fs=5000, target_fs=25, fix_saturated=True):\n",
    "    \"\"\"\n",
    "    Downsample the (filtered) data using averaging.\n",
    "\n",
    "    Parameters:\n",
    "    - df: polars DataFrame with filtered signal data\n",
    "    - original_fs: original sampling frequency in Hz (default: 5000Hz)\n",
    "    - target_fs: target sampling frequency in Hz (default: 25Hz)\n",
    "    - fix_saturated: bool, if True removes saturated values before averaging (default: True)\n",
    "\n",
    "    Returns:\n",
    "    - df_downsampled: polars DataFrame with downsampled signals\n",
    "    \"\"\"\n",
    "    # Calculate downsampling factor\n",
    "    downsample_factor = original_fs // target_fs\n",
    "    print(f\"Downsampling from {original_fs}Hz to {target_fs}Hz (factor: {downsample_factor})\")\n",
    "\n",
    "    if fix_saturated:\n",
    "        print(\"Saturation removal enabled - will exclude saturated values from averaging\")\n",
    "\n",
    "    # Get signal column names (exclude time column)\n",
    "    signal_column_names = []\n",
    "    for channel_group in signal_channels.values():\n",
    "        signal_column_names.extend(channel_group)\n",
    "\n",
    "    # Calculate number of complete windows\n",
    "    n_samples = df.shape[0]\n",
    "    n_windows = n_samples // downsample_factor\n",
    "    print(f\"Processing {n_samples} samples into {n_windows} downsampled points\")\n",
    "\n",
    "    # Initialize dictionary for downsampled data\n",
    "    downsampled_data = {}\n",
    "\n",
    "    # Downsample time column if present - check for both 'time' and 'Time'\n",
    "    time_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'time':\n",
    "            time_col = col\n",
    "            break\n",
    "\n",
    "    if time_col is not None:\n",
    "        time_data = df[time_col].to_numpy()\n",
    "        # Take every nth sample for time (or average if needed)\n",
    "        downsampled_time = time_data[::downsample_factor][:n_windows]\n",
    "        downsampled_data[time_col] = downsampled_time\n",
    "        print(f\"Time column '{time_col}' downsampled\")\n",
    "    else:\n",
    "        print(\"Warning: No time column found for downsampling\")\n",
    "\n",
    "    # Define saturation thresholds based on physical constants\n",
    "    # Use the predefined sensor saturation threshold\n",
    "    saturation_threshold_nt = SENSOR_SATURATION  # Saturation threshold for the sensor\n",
    "    print(f\"Saturation threshold: ±{saturation_threshold_nt} nT\")\n",
    "\n",
    "    # Track saturation statistics\n",
    "    total_saturated_samples = 0\n",
    "    saturated_windows = 0\n",
    "\n",
    "    # Downsample each signal channel using averaging\n",
    "    print(\"Downsampling channels using averaging...\")\n",
    "    for channel in tqdm(signal_column_names, desc=\"Downsampling channels\"):\n",
    "        if channel in df.columns:\n",
    "            # Extract signal data\n",
    "            signal = df[channel].to_numpy()\n",
    "\n",
    "            # Reshape for averaging (trim to complete windows)\n",
    "            signal_windowed = signal[:n_windows * downsample_factor].reshape(n_windows, downsample_factor)\n",
    "\n",
    "            if fix_saturated:\n",
    "                # Apply saturation filtering before averaging\n",
    "                downsampled_signal = []\n",
    "                saturation_flags = []  # Track which windows had saturation issues\n",
    "                channel_saturated_samples = 0\n",
    "                channel_saturated_windows = 0\n",
    "\n",
    "                for window in signal_windowed:\n",
    "                    # Identify saturated samples (beyond threshold)\n",
    "                    # Use > instead of >= to avoid edge case issues\n",
    "                    saturated_mask = np.abs(window) > saturation_threshold_nt\n",
    "                    saturated_count = np.sum(saturated_mask)\n",
    "\n",
    "                    if saturated_count > 0:\n",
    "                        channel_saturated_samples += saturated_count\n",
    "                        channel_saturated_windows += 1\n",
    "\n",
    "                        # Remove saturated values from averaging\n",
    "                        valid_samples = window[~saturated_mask]\n",
    "\n",
    "                        if len(valid_samples) > 0:\n",
    "                            # Average only non-saturated samples\n",
    "                            window_avg = np.mean(valid_samples)\n",
    "                            saturation_flags.append(1)  # Partially saturated\n",
    "                        else:\n",
    "                            # If all samples are saturated, mark as invalid\n",
    "                            # Use NaN to indicate unreliable data\n",
    "                            window_avg = np.nan\n",
    "                            saturation_flags.append(2)  # Fully saturated (unreliable)\n",
    "\n",
    "                        downsampled_signal.append(window_avg)\n",
    "                    else:\n",
    "                        # No saturation, use normal averaging\n",
    "                        downsampled_signal.append(np.mean(window))\n",
    "                        saturation_flags.append(0)  # No saturation\n",
    "\n",
    "                downsampled_signal = np.array(downsampled_signal)\n",
    "\n",
    "                # Note: Saturation flags are tracked for statistics but not stored in output data\n",
    "\n",
    "                # Update statistics\n",
    "                total_saturated_samples += channel_saturated_samples\n",
    "                saturated_windows += channel_saturated_windows\n",
    "\n",
    "                if channel_saturated_samples > 0:\n",
    "                    saturation_percentage = (channel_saturated_samples / (n_windows * downsample_factor)) * 100\n",
    "                    print(f\"  {channel}: {channel_saturated_samples} saturated samples ({saturation_percentage:.2f}%) in {channel_saturated_windows} windows\")\n",
    "            else:\n",
    "                # Standard averaging without saturation handling\n",
    "                downsampled_signal = np.mean(signal_windowed, axis=1)\n",
    "\n",
    "            # Store downsampled signal\n",
    "            downsampled_data[channel] = downsampled_signal\n",
    "        else:\n",
    "            print(f\"Warning: Channel '{channel}' not found in filtered data\")\n",
    "\n",
    "    # Print saturation summary\n",
    "    if fix_saturated and total_saturated_samples > 0:\n",
    "        total_samples = n_windows * downsample_factor * len(signal_column_names)\n",
    "        overall_saturation_percentage = (total_saturated_samples / total_samples) * 100\n",
    "        print(f\"\\nSaturation Summary:\")\n",
    "        print(f\"  Total saturated samples: {total_saturated_samples}\")\n",
    "        print(f\"  Total windows with saturation: {saturated_windows}\")\n",
    "        print(f\"  Overall saturation rate: {overall_saturation_percentage:.2f}%\")\n",
    "\n",
    "    # Convert to polars DataFrame\n",
    "    df_downsampled = pl.DataFrame(downsampled_data)\n",
    "\n",
    "    print(f\"Downsampling completed. New shape: {df_downsampled.shape}\")\n",
    "    print(f\"Effective sampling rate: {target_fs}Hz\")\n",
    "\n",
    "    return df_downsampled"
   ],
   "id": "1e1816aa6b4b512e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Process signal data by periods\n",
    "\n",
    "- Load the signal file\n",
    "- Rename the voltage channels to Hand1 and Hand2\n",
    "  - Apply band-pass filter (0.05 Hz to 10 Hz)\n",
    "  - Downsample from 5000 Hz to 25 Hz\n",
    "  - Save as separate parquet file"
   ],
   "id": "286f8f2a97a6ad57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and process the signal data from the saved parquet files",
   "id": "9d71457796ea8c92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:30:13.778663Z",
     "start_time": "2025-08-11T16:27:01.920139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the saved parquet file\n",
    "signal_file = signals_dir / \"volunteer.parquet\"\n",
    "print(f\"\\nLoading signal file: {signal_file}\")\n",
    "if not signal_file.exists():\n",
    "    print(f\"Signal file {signal_file} does not exist. Cannot load data.\")\n",
    "else:\n",
    "    try:\n",
    "        signal_df = pl.read_parquet(str(signal_file))\n",
    "        print(f\"Successfully loaded signal DataFrame with shape: {signal_df.shape}\")\n",
    "        print(f\"Columns: {signal_df.columns}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading signal file: {e}\")\n",
    "\n",
    "# Rename channels from Voltage_1 and Voltage_2 to Hand1 and Hand2\n",
    "if signal_df is not None:\n",
    "    print(\"Renaming channels to Hand1 and Hand2...\")\n",
    "    signal_df = signal_df.rename({\n",
    "        'Voltage_0': 'Hand1',\n",
    "        'Voltage_1': 'Hand2'\n",
    "    })\n",
    "    print(\"Renaming completed.\")\n",
    "\n",
    "# Split the DataFrame into periods based on the specified dates\n",
    "if signal_df is not None:\n",
    "    print(\"Splitting data into specified periods...\")\n",
    "    periods = {\n",
    "        '2021-11-16_17': (pl.datetime(2021, 11, 16), pl.datetime(2021, 11, 17)),\n",
    "        '2021-12-07_08': (pl.datetime(2021, 12, 7), pl.datetime(2021, 12, 8)),\n",
    "        '2021-12-14_15': (pl.datetime(2021, 12, 14), pl.datetime(2021, 12, 15)),\n",
    "        '2021-12-15_16': (pl.datetime(2021, 12, 15), pl.datetime(2021, 12, 16))\n",
    "    }\n",
    "    for period_name, (start_date, end_date) in periods.items():\n",
    "        print(f\"Processing period: {period_name} from {start_date} to {end_date}\")\n",
    "\n",
    "        # Filter the DataFrame for the current period\n",
    "        period_df = signal_df.filter(\n",
    "            (pl.col('Time') >= start_date) & (pl.col('Time') <= end_date)\n",
    "        )\n",
    "\n",
    "        if period_df.is_empty():\n",
    "            print(f\"No data found for period {period_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Apply the anti-aliasing filter to all signal channels\n",
    "        print(f\"Applying band-pass filter to period {period_name}...\")\n",
    "        period_df_filtered = apply_antialiasing_filter(\n",
    "            period_df,\n",
    "            lowcut_freq=LOWCUT_FREQ,\n",
    "            highcut_freq=HIGHCUT_FREQ,\n",
    "            sampling_freq=SAMPLING_FREQUENCY,\n",
    "            filter_order=FILTER_ORDER\n",
    "        )\n",
    "        print(f\"Filtered data shape for {period_name}: {period_df_filtered.shape}\")\n",
    "\n",
    "        # Apply downsampling from 5000Hz to 25Hz\n",
    "        print(f\"Downsampling period {period_name} to 25Hz...\")\n",
    "        period_df_downsampled = downsample_data(\n",
    "            period_df_filtered,\n",
    "            original_fs=SAMPLING_FREQUENCY,\n",
    "            target_fs=25,\n",
    "            fix_saturated=True\n",
    "        )\n",
    "        print(f\"Downsampled data shape for {period_name}: {period_df_downsampled.shape}\")\n",
    "\n",
    "        # Save the filtered and downsampled DataFrame to a new parquet file\n",
    "        output_period_file = signals_dir / f\"volunteer_{period_name}.parquet\"\n",
    "        try:\n",
    "            period_df_downsampled.write_parquet(str(output_period_file))\n",
    "            print(f\"Successfully saved filtered and downsampled period {period_name} to {output_period_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving period {period_name}: {e}\")\n"
   ],
   "id": "dfb1e60c34ae3602",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading signal file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer.parquet\n",
      "Successfully loaded signal DataFrame with shape: (571422500, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Renaming channels to Hand1 and Hand2...\n",
      "Renaming completed.\n",
      "Splitting data into specified periods...\n",
      "Processing period: 2021-11-16_17 from 2021-11-16 00:00:00.alias(\"datetime\") to 2021-11-17 00:00:00.alias(\"datetime\")\n",
      "Applying band-pass filter to period 2021-11-16_17...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:06<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n",
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for 2021-11-16_17: (46163290, 3)\n",
      "Downsampling period 2021-11-16_17 to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 46163290 samples into 230816 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:07<00:12,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 77478 saturated samples (0.17%) in 416 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:14<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 75863 saturated samples (0.16%) in 411 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 153341\n",
      "  Total windows with saturation: 827\n",
      "  Overall saturation rate: 0.04%\n",
      "Downsampling completed. New shape: (230816, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for 2021-11-16_17: (230816, 3)\n",
      "Successfully saved filtered and downsampled period 2021-11-16_17 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_2021-11-16_17.parquet\n",
      "Processing period: 2021-12-07_08 from 2021-12-07 00:00:00.alias(\"datetime\") to 2021-12-08 00:00:00.alias(\"datetime\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying band-pass filter to period 2021-12-07_08...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:05<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n",
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for 2021-12-07_08: (43012500, 3)\n",
      "Downsampling period 2021-12-07_08 to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 43012500 samples into 215062 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:06<00:11,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 121716 saturated samples (0.28%) in 645 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:13<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 102761 saturated samples (0.24%) in 541 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 224477\n",
      "  Total windows with saturation: 1186\n",
      "  Overall saturation rate: 0.07%\n",
      "Downsampling completed. New shape: (215062, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for 2021-12-07_08: (215062, 3)\n",
      "Successfully saved filtered and downsampled period 2021-12-07_08 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_2021-12-07_08.parquet\n",
      "Processing period: 2021-12-14_15 from 2021-12-14 00:00:00.alias(\"datetime\") to 2021-12-15 00:00:00.alias(\"datetime\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying band-pass filter to period 2021-12-14_15...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:04<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n",
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for 2021-12-14_15: (46351293, 3)\n",
      "Downsampling period 2021-12-14_15 to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 46351293 samples into 231756 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:06<00:11,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 617331 saturated samples (1.33%) in 3286 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:13<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 614167 saturated samples (1.33%) in 3268 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 1231498\n",
      "  Total windows with saturation: 6554\n",
      "  Overall saturation rate: 0.33%\n",
      "Downsampling completed. New shape: (231756, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for 2021-12-14_15: (231756, 3)\n",
      "Successfully saved filtered and downsampled period 2021-12-14_15 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_2021-12-14_15.parquet\n",
      "Processing period: 2021-12-15_16 from 2021-12-15 00:00:00.alias(\"datetime\") to 2021-12-16 00:00:00.alias(\"datetime\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying band-pass filter to period 2021-12-15_16...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:44<00:00,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for 2021-12-15_16: (173196391, 3)\n",
      "Downsampling period 2021-12-15_16 to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 173196391 samples into 865981 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:27<00:45,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 628530 saturated samples (0.36%) in 3475 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:54<00:00,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 149350 saturated samples (0.09%) in 817 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 777880\n",
      "  Total windows with saturation: 4292\n",
      "  Overall saturation rate: 0.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling completed. New shape: (865981, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for 2021-12-15_16: (865981, 3)\n",
      "Successfully saved filtered and downsampled period 2021-12-15_16 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_2021-12-15_16.parquet\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gc.collect()",
   "id": "4c474834dd105214"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load and process individual parquet files (volunteer_part1 through part4)\n",
    "\n",
    "For each part:\n",
    "- Load the signal data\n",
    "- Rename the voltage channels to Hand1 and Hand2\n",
    "- Apply band-pass filtering (0.05 Hz to 10 Hz)\n",
    "- Downsample from 5000 Hz to 25 Hz\n",
    "- Save as separate parquet file with \"_downsampled_25Hz\" suffix"
   ],
   "id": "ce7b060d4dd5d37c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T17:31:26.039149Z",
     "start_time": "2025-08-11T17:26:09.614441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and process individual parquet files (volunteer_part1 through part4)\n",
    "processed_downsampled_files = {}\n",
    "\n",
    "# Define the individual parquet files to process\n",
    "individual_files = {\n",
    "    'part1': signals_dir / \"volunteer_part1.parquet\",\n",
    "    'part2': signals_dir / \"volunteer_part2.parquet\",\n",
    "    'part3': signals_dir / \"volunteer_part3.parquet\",\n",
    "    'part4': signals_dir / \"volunteer_part4.parquet\"\n",
    "}\n",
    "\n",
    "for part_name, file_path in individual_files.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING {part_name.upper()}: {file_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not file_path.exists():\n",
    "        print(f\"File {file_path} does not exist. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load the parquet file\n",
    "        print(f\"Loading {part_name} data...\")\n",
    "        signal_df = pl.read_parquet(str(file_path))\n",
    "        print(f\"Successfully loaded {part_name} with shape: {signal_df.shape}\")\n",
    "        print(f\"Columns: {signal_df.columns}\")\n",
    "\n",
    "        # Display time range for this part\n",
    "        if 'Time' in signal_df.columns:\n",
    "            time_min = signal_df['Time'].min()\n",
    "            time_max = signal_df['Time'].max()\n",
    "            duration_hours = (time_max - time_min).total_seconds() / 3600\n",
    "            print(f\"Time range: {time_min} to {time_max}\")\n",
    "            print(f\"Duration: {duration_hours:.2f} hours\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {part_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Rename voltage channels to Hand1 and Hand2\n",
    "    print(f\"Renaming voltage channels in {part_name}...\")\n",
    "    original_columns = signal_df.columns\n",
    "\n",
    "    # Check what voltage columns exist and rename them\n",
    "    rename_dict = {}\n",
    "    if 'Voltage_0' in signal_df.columns:\n",
    "        rename_dict['Voltage_0'] = 'Hand1'\n",
    "    if 'Voltage_1' in signal_df.columns:\n",
    "        rename_dict['Voltage_1'] = 'Hand2'\n",
    "\n",
    "    if rename_dict:\n",
    "        signal_df = signal_df.rename(rename_dict)\n",
    "        print(f\"Renamed channels: {rename_dict}\")\n",
    "    else:\n",
    "        print(\"No voltage channels found to rename\")\n",
    "\n",
    "    print(f\"Updated columns: {signal_df.columns}\")\n",
    "\n",
    "    # Apply band-pass filter (0.05 Hz to 10 Hz)\n",
    "    print(f\"Applying band-pass filter to {part_name}...\")\n",
    "    try:\n",
    "        signal_df_filtered = apply_antialiasing_filter(\n",
    "            signal_df,\n",
    "            lowcut_freq=LOWCUT_FREQ,\n",
    "            highcut_freq=HIGHCUT_FREQ,\n",
    "            sampling_freq=SAMPLING_FREQUENCY,\n",
    "            filter_order=FILTER_ORDER\n",
    "        )\n",
    "        print(f\"Filtered data shape for {part_name}: {signal_df_filtered.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying filter to {part_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Downsample from 5000 Hz to 25 Hz\n",
    "    print(f\"Downsampling {part_name} from 5000Hz to 25Hz...\")\n",
    "    try:\n",
    "        signal_df_downsampled = downsample_data(\n",
    "            signal_df_filtered,\n",
    "            original_fs=SAMPLING_FREQUENCY,\n",
    "            target_fs=25,\n",
    "            fix_saturated=True\n",
    "        )\n",
    "        print(f\"Downsampled data shape for {part_name}: {signal_df_downsampled.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downsampling {part_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Save as separate parquet file with \"_downsampled_25hz\" ending\n",
    "    output_file = signals_dir / f\"volunteer_{part_name}_downsampled_25hz.parquet\"\n",
    "    try:\n",
    "        signal_df_downsampled.write_parquet(str(output_file))\n",
    "        print(f\"Successfully saved downsampled {part_name} to {output_file}\")\n",
    "\n",
    "        # Calculate output file size\n",
    "        if output_file.exists():\n",
    "            output_size_mb = output_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Output file size: {output_size_mb:.1f} MB\")\n",
    "\n",
    "        # Track successful processing\n",
    "        processed_downsampled_files[part_name] = {\n",
    "            'original_file': file_path,\n",
    "            'downsampled_file': output_file,\n",
    "            'original_shape': signal_df.shape,\n",
    "            'downsampled_shape': signal_df_downsampled.shape,\n",
    "            'time_range': (time_min, time_max) if 'Time' in signal_df.columns else None,\n",
    "            'duration_hours': duration_hours if 'Time' in signal_df.columns else None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving downsampled {part_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    finally:\n",
    "        # Clean up memory\n",
    "        del signal_df, signal_df_filtered, signal_df_downsampled\n",
    "        gc.collect()\n",
    "\n",
    "# Print summary of processed files\n",
    "if processed_downsampled_files:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"DOWNSAMPLING PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Successfully processed {len(processed_downsampled_files)} files:\")\n",
    "\n",
    "    for part_name, info in processed_downsampled_files.items():\n",
    "        print(f\"\\n{part_name.upper()}:\")\n",
    "        print(f\"  - Original file: {info['original_file']}\")\n",
    "        print(f\"  - Downsampled file: {info['downsampled_file']}\")\n",
    "        print(f\"  - Shape change: {info['original_shape']} → {info['downsampled_shape']}\")\n",
    "        if info['time_range']:\n",
    "            print(f\"  - Time range: {info['time_range'][0]} to {info['time_range'][1]}\")\n",
    "            print(f\"  - Duration: {info['duration_hours']:.2f} hours\")\n",
    "\n",
    "        # Calculate compression ratio\n",
    "        orig_samples = info['original_shape'][0]\n",
    "        down_samples = info['downsampled_shape'][0]\n",
    "        compression_ratio = orig_samples / down_samples if down_samples > 0 else 0\n",
    "        print(f\"  - Compression ratio: {compression_ratio:.1f}:1\")\n",
    "else:\n",
    "    print(\"No files were successfully processed and downsampled.\")\n",
    "\n",
    "print(f\"\\nDownsampling completed! All processed files are saved with '_downsampled_25hz' suffix.\")\n"
   ],
   "id": "4501e133d92ea9f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING PART1: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part1.parquet\n",
      "============================================================\n",
      "Loading part1 data...\n",
      "Successfully loaded part1 with shape: (122502500, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range: 2021-11-16 22:24:59.171050 to 2021-11-17 04:49:32.170950\n",
      "Duration: 6.41 hours\n",
      "Renaming voltage channels in part1...\n",
      "Renamed channels: {'Voltage_0': 'Hand1', 'Voltage_1': 'Hand2'}\n",
      "Updated columns: ['Time', 'Hand1', 'Hand2']\n",
      "Applying band-pass filter to part1...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:16<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n",
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for part1: (122502500, 3)\n",
      "Downsampling part1 from 5000Hz to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 122502500 samples into 612512 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:22<00:38,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 213953 saturated samples (0.17%) in 1156 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:45<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 195512 saturated samples (0.16%) in 1066 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 409465\n",
      "  Total windows with saturation: 2222\n",
      "  Overall saturation rate: 0.04%\n",
      "Downsampling completed. New shape: (612512, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for part1: (612512, 3)\n",
      "Successfully saved downsampled part1 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part1_downsampled_25Hz.parquet\n",
      "Output file size: 10.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING PART2: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part2.parquet\n",
      "============================================================\n",
      "Loading part2 data...\n",
      "Successfully loaded part2 with shape: (131262500, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range: 2021-12-07 22:19:36.521900 to 2021-12-08 06:51:52.271800\n",
      "Duration: 8.54 hours\n",
      "Renaming voltage channels in part2...\n",
      "Renamed channels: {'Voltage_0': 'Hand1', 'Voltage_1': 'Hand2'}\n",
      "Updated columns: ['Time', 'Hand1', 'Hand2']\n",
      "Applying band-pass filter to part2...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:16<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n",
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for part2: (131262500, 3)\n",
      "Downsampling part2 from 5000Hz to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 131262500 samples into 656312 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:24<00:40,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 328322 saturated samples (0.25%) in 1747 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:46<00:00,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 297234 saturated samples (0.23%) in 1585 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 625556\n",
      "  Total windows with saturation: 3332\n",
      "  Overall saturation rate: 0.06%\n",
      "Downsampling completed. New shape: (656312, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for part2: (656312, 3)\n",
      "Successfully saved downsampled part2 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part2_downsampled_25Hz.parquet\n",
      "Output file size: 11.1 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING PART3: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part3.parquet\n",
      "============================================================\n",
      "Loading part3 data...\n",
      "Successfully loaded part3 with shape: (174417500, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range: 2021-12-14 22:35:22.120750 to 2021-12-15 06:58:26.370650\n",
      "Duration: 8.38 hours\n",
      "Renaming voltage channels in part3...\n",
      "Renamed channels: {'Voltage_0': 'Hand1', 'Voltage_1': 'Hand2'}\n",
      "Updated columns: ['Time', 'Hand1', 'Hand2']\n",
      "Applying band-pass filter to part3...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:20<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n",
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for part3: (174417500, 3)\n",
      "Downsampling part3 from 5000Hz to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 174417500 samples into 872087 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:29<00:49,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 774734 saturated samples (0.44%) in 4146 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [01:02<00:00,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 755043 saturated samples (0.43%) in 4039 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 1529777\n",
      "  Total windows with saturation: 8185\n",
      "  Overall saturation rate: 0.11%\n",
      "Downsampling completed. New shape: (872087, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for part3: (872087, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved downsampled part3 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part3_downsampled_25Hz.parquet\n",
      "Output file size: 14.6 MB\n",
      "\n",
      "============================================================\n",
      "PROCESSING PART4: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part4.parquet\n",
      "============================================================\n",
      "Loading part4 data...\n",
      "Successfully loaded part4 with shape: (143240000, 3)\n",
      "Columns: ['Time', 'Voltage_0', 'Voltage_1']\n",
      "Time range: 2021-12-15 21:29:29.463300 to 2021-12-16 07:23:24.963100\n",
      "Duration: 9.90 hours\n",
      "Renaming voltage channels in part4...\n",
      "Renamed channels: {'Voltage_0': 'Hand1', 'Voltage_1': 'Hand2'}\n",
      "Updated columns: ['Time', 'Hand1', 'Hand2']\n",
      "Applying band-pass filter to part4...\n",
      "High-pass normalized frequency: 0.000020\n",
      "Using filter order 4 for high-pass filter\n",
      "Time column 'Time' preserved in filtered data\n",
      "Applying high-pass (0.05Hz) + low-pass (10Hz) filters to channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in data\n",
      "Warning: Channel 'Head_right' not found in data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering channels: 100%|██████████| 8/8 [00:17<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Liver1' not found in data\n",
      "Warning: Channel 'Liver2' not found in data\n",
      "Warning: Channel 'Background1' not found in data\n",
      "Warning: Channel 'Background2' not found in data\n",
      "Filtering completed successfully. Processed 8 channels.\n",
      "Filtered data shape for part4: (143240000, 3)\n",
      "Downsampling part4 from 5000Hz to 25Hz...\n",
      "Downsampling from 5000Hz to 25Hz (factor: 200)\n",
      "Saturation removal enabled - will exclude saturated values from averaging\n",
      "Processing 143240000 samples into 716200 downsampled points\n",
      "Time column 'Time' downsampled\n",
      "Saturation threshold: ±250 nT\n",
      "Downsampling channels using averaging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Channel 'Head_left' not found in filtered data\n",
      "Warning: Channel 'Head_right' not found in filtered data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels:  38%|███▊      | 3/8 [00:30<00:51, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand1: 721774 saturated samples (0.50%) in 4023 windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling channels: 100%|██████████| 8/8 [00:54<00:00,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hand2: 7202 saturated samples (0.01%) in 51 windows\n",
      "Warning: Channel 'Liver1' not found in filtered data\n",
      "Warning: Channel 'Liver2' not found in filtered data\n",
      "Warning: Channel 'Background1' not found in filtered data\n",
      "Warning: Channel 'Background2' not found in filtered data\n",
      "\n",
      "Saturation Summary:\n",
      "  Total saturated samples: 728976\n",
      "  Total windows with saturation: 4074\n",
      "  Overall saturation rate: 0.06%\n",
      "Downsampling completed. New shape: (716200, 3)\n",
      "Effective sampling rate: 25Hz\n",
      "Downsampled data shape for part4: (716200, 3)\n",
      "Successfully saved downsampled part4 to ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part4_downsampled_25Hz.parquet\n",
      "Output file size: 12.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DOWNSAMPLING PROCESSING SUMMARY\n",
      "================================================================================\n",
      "Successfully processed 4 files:\n",
      "\n",
      "PART1:\n",
      "  - Original file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part1.parquet\n",
      "  - Downsampled file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part1_downsampled_25Hz.parquet\n",
      "  - Shape change: (122502500, 3) → (612512, 3)\n",
      "  - Time range: 2021-11-16 22:24:59.171050 to 2021-11-17 04:49:32.170950\n",
      "  - Duration: 6.41 hours\n",
      "  - Compression ratio: 200.0:1\n",
      "\n",
      "PART2:\n",
      "  - Original file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part2.parquet\n",
      "  - Downsampled file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part2_downsampled_25Hz.parquet\n",
      "  - Shape change: (131262500, 3) → (656312, 3)\n",
      "  - Time range: 2021-12-07 22:19:36.521900 to 2021-12-08 06:51:52.271800\n",
      "  - Duration: 8.54 hours\n",
      "  - Compression ratio: 200.0:1\n",
      "\n",
      "PART3:\n",
      "  - Original file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part3.parquet\n",
      "  - Downsampled file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part3_downsampled_25Hz.parquet\n",
      "  - Shape change: (174417500, 3) → (872087, 3)\n",
      "  - Time range: 2021-12-14 22:35:22.120750 to 2021-12-15 06:58:26.370650\n",
      "  - Duration: 8.38 hours\n",
      "  - Compression ratio: 200.0:1\n",
      "\n",
      "PART4:\n",
      "  - Original file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part4.parquet\n",
      "  - Downsampled file: ..\\..\\..\\Data\\ProcessedData\\Signal_Files\\volunteer_part4_downsampled_25Hz.parquet\n",
      "  - Shape change: (143240000, 3) → (716200, 3)\n",
      "  - Time range: 2021-12-15 21:29:29.463300 to 2021-12-16 07:23:24.963100\n",
      "  - Duration: 9.90 hours\n",
      "  - Compression ratio: 200.0:1\n",
      "\n",
      "Downsampling completed! All processed files are saved with '_downsampled_25Hz' suffix.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d4acff5f6dff7806"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
